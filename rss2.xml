<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>YOHAI</title>
    <link>https://l-yohai.github.io/</link>
    
    <atom:link href="https://l-yohai.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description></description>
    <pubDate>Wed, 13 Oct 2021 21:25:26 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>PStage MRC 7강 - Linking MRC and Retrieval</title>
      <link>https://l-yohai.github.io/PStage-MRC-7%EA%B0%95-Linking-MRC-and-Retrieval/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-7%EA%B0%95-Linking-MRC-and-Retrieval/</guid>
      <pubDate>Wed, 13 Oct 2021 21:24:55 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Introduction-to-Open-domain-Question-Answering-ODQA&quot;&gt;&lt;a href=&quot;#Introduction-to-Open-domain-Question-Answering-ODQA&quot; class=&quot;headerlink&quot; title=&quot;Introduction to Open-domain Question Answering (ODQA)&quot;&gt;&lt;/a&gt;Introduction to Open-domain Question Answering (ODQA)&lt;/h2&gt;&lt;p&gt;MRC는 지문이 주어진 상황에서 질의응답을 하는 과제이다. 이에 반해 ODQA는 비슷한 형태이지만, 지문 부분이 주어지는 것이 아니라 위키 혹은 웹 전체가 주어지게 되기 때문에 매우 많은 지문을 봐야하는 Large Scale 과제가 된다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Introduction-to-Open-domain-Question-Answering-ODQA"><a href="#Introduction-to-Open-domain-Question-Answering-ODQA" class="headerlink" title="Introduction to Open-domain Question Answering (ODQA)"></a>Introduction to Open-domain Question Answering (ODQA)</h2><p>MRC는 지문이 주어진 상황에서 질의응답을 하는 과제이다. 이에 반해 ODQA는 비슷한 형태이지만, 지문 부분이 주어지는 것이 아니라 위키 혹은 웹 전체가 주어지게 되기 때문에 매우 많은 지문을 봐야하는 Large Scale 과제가 된다.</p><p>ODQA라는 문제는 꽤 예전부터 다루었던 문제이며, short answer with support의 형태를 목표로 했었다.</p><p>1) Question processing</p><p>   질문으로부터 키워드를 선택하여 답변의 타입을 선택할 수 있도록 했다.</p><p>2) Passage Retrieval</p><p>   기존 IR 방법을 활용하여 연관된 document를 뽑고, passage 단위로 자른 후 선별한다.</p><p>3) Answer processing</p><p>   주어진 질문과 passage들 내에서 답을 선택하는 휴리스틱한 분류문제로 진행했다.</p><p>ODQA는 꽤 역사가 길며, 지금까지는 위와 같은 방식으로 진행되고 있었는데, 2011년 IBM Watson을 통해 발전하게 된다.</p><h2 id="Retriever-Reader-Approach"><a href="#Retriever-Reader-Approach" class="headerlink" title="Retriever-Reader Approach"></a>Retriever-Reader Approach</h2><p>ODQA에서 가장 많이 쓰이는 approach로, DB에서 관련있는 문서를 검색하는 Retriever과 검색된 문서에서 질문에 해당하는 답을 찾아내는 Reader로 이루어진다.</p><h3 id="학습-단계"><a href="#학습-단계" class="headerlink" title="학습 단계"></a>학습 단계</h3><p>Retriever</p><ul><li>TF-IDF, BM25 -&gt; 학습없음</li><li>Dense -&gt; 학습필요</li></ul><p>Reader</p><ul><li>SQuAD와 같은 MRC 데이터셋으로 학습</li><li>학습 데이터를 추가하기 위해서 Distant supervision 활용</li></ul><h3 id="Distant-supervision"><a href="#Distant-supervision" class="headerlink" title="Distant supervision"></a>Distant supervision</h3><p>질문-답변만 있는 데이터셋에서 MRc 학습 데이터 만드는 방법. Supporting document가 필요함.</p><ol><li>위키피디아에서 Retriever를 이용하여 관련성 높은 문서를 검색한다.</li><li>너무 짧거나 긴 문서, 질문의 고유명사를 포함하지 않는 등 부적합한 문서를 제거한다.</li><li>answer가 exact match로 들어있지 않은 문서를 제거한다.</li><li>남은 문서 중에 질문과 (사용 단어 기준) 연관성이 가장 높은 단락을 supporting evidence로 사용한다.</li></ol><h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><ul><li>Retriever가 질문과 가장 관련성이 높은 k개의 문서 출력</li><li>Reader는 k개의 문서를 읽고 답변 예측</li><li>Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용</li></ul><h2 id="Issues-and-Recent-Approaches"><a href="#Issues-and-Recent-Approaches" class="headerlink" title="Issues and Recent Approaches"></a>Issues and Recent Approaches</h2><h3 id="Different-granularities-of-text-at-indexing-time"><a href="#Different-granularities-of-text-at-indexing-time" class="headerlink" title="Different granularities of text at indexing time"></a>Different granularities of text at indexing time</h3><p>위키피디아에서 각 passage의 단위를 문서, 단락, 또는 문장으로 정의할지 정해야 함</p><p>Retriever 단계에서 몇 개(top-k)의 문서를 넘길지 정해야 함</p><p>Granularity에 따라 k가 다를 수밖에 없음.</p><h3 id="Single-passage-training-vs-Multi-passage-training"><a href="#Single-passage-training-vs-Multi-passage-training" class="headerlink" title="Single-passage training vs. Multi-passage training"></a>Single-passage training vs. Multi-passage training</h3><p>k개의 passage들을 따로따로 보는 것이 아니라 retrieval이 가져온 문서 전체를 하나의 passage로 취급하여 reader모델이 그 안에서 answer span 하나를 찾도록 하는 것임. 다만 문서가 너무 길어지므로 GPU에 더 많은 메로리를 할당해야하고, 처리해야하는 연산량이 많아진다.</p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-7%EA%B0%95-Linking-MRC-and-Retrieval/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PStage MRC 6강 - Scaling up with FAISS</title>
      <link>https://l-yohai.github.io/PStage-MRC-6%EA%B0%95-Scaling-up-with-FAISS/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-6%EA%B0%95-Scaling-up-with-FAISS/</guid>
      <pubDate>Wed, 13 Oct 2021 21:24:41 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Passage-Retrieval-and-Similarity-Search&quot;&gt;&lt;a href=&quot;#Passage-Retrieval-and-Similarity-Search&quot; class=&quot;headerlink&quot; title=&quot;Passage Retrieval and Similarity Search&quot;&gt;&lt;/a&gt;Passage Retrieval and Similarity Search&lt;/h2&gt;&lt;p&gt;저번 강의에서 임베딩을 통해 리트리벌을 진행하려고 할 때 질문과 지문 쪽 각각 인코더가 존재했다는 것을 배웠다. 질문은 그때그때 임베딩을 하며, 지문은 미리 임베딩을 해놓지만 새로운 것이 들어올 때 추가하여 적절한 임베딩을 반환하게 된다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Passage-Retrieval-and-Similarity-Search"><a href="#Passage-Retrieval-and-Similarity-Search" class="headerlink" title="Passage Retrieval and Similarity Search"></a>Passage Retrieval and Similarity Search</h2><p>저번 강의에서 임베딩을 통해 리트리벌을 진행하려고 할 때 질문과 지문 쪽 각각 인코더가 존재했다는 것을 배웠다. 질문은 그때그때 임베딩을 하며, 지문은 미리 임베딩을 해놓지만 새로운 것이 들어올 때 추가하여 적절한 임베딩을 반환하게 된다.</p><p>Nearest Neighbor Search에서는 Passage 개수가 늘어날 수록 Top-k 때 Dot product 연산이 부담스러워질 수 있는데, 이러한 과정에서 Similarity Search를 아는 것이 중요하다.</p><h3 id="MIPS-Maximum-Inner-Product-Search"><a href="#MIPS-Maximum-Inner-Product-Search" class="headerlink" title="MIPS(Maximum Inner Product Search)"></a>MIPS(Maximum Inner Product Search)</h3><p>기본적으로 Nearest Neighbor Search보다, Inner product Search가 더 많이 사용되고 있다. Nearest Neighbor와 같은 L2 유클리디언 거리를 계산하는 것보다 Dot Product의 Maximum을 찾는 문제로 돌리는 것이 더 수월하기 때문이다. 가장 가까운 벡터를 찾겠다는 것은 Maximum Inner Product를 찾겠다는 것으로 이해하면 된다. 하지만 개념 정리나 상상을 할 때는 Nearest Neighbor Search를 떠올리는 것이 더 쉽긴 한다.</p><p>MIPS는 주어진 질문(query) 벡터 q에 대해 Passage 벡터 v들 중 가장 질문과 관련된 벡터를 찾는 문제이며, 관련성은 내적이 가장 큰 값으로 찾는다.</p><p>하지만 실제 검색해야할 데이터는 위키피디아에만 5백만개이며, 수십억, 수십조까지 커질 수 있기 때문에 사실상 모든 문서 임베딩을 Bruteforce로 찾는 것은 불가능하다.</p><h3 id="Tradeoffs-of-similarity-search"><a href="#Tradeoffs-of-similarity-search" class="headerlink" title="Tradeoffs of similarity search"></a>Tradeoffs of similarity search</h3><p>1) Search Speed - 쿼리당 유사한 벡터 k개를 찾는 시간. 많은 벡터를 가지고 있을수록 당연히 시간이 오래걸린다.</p><p>   <strong><em>Pruning</em></strong> 을 사용하면 속도 개선 가능</p><p>2) Memory Usage - 벡터를 어디에서 가져올 것인지. RAM에 올려놓으면 가장 빠르지만 RAM은 비싸다.</p><p>   <strong><em>Compression</em></strong> 으로 메모리 압축 가능</p><p>3) Accuracy - Bruteforce 검색과 얼마나 유사한지. 속도를 증가시키려면 정확도를 어느정도 희생해야 한다.</p><p>   <strong><em>Exhaustive Search</em></strong>로 정확도 개선 가능</p><h2 id="Approximating-Similarity-Search"><a href="#Approximating-Similarity-Search" class="headerlink" title="Approximating Similarity Search"></a>Approximating Similarity Search</h2><h3 id="Compression-Scalar-Quantization-SQ"><a href="#Compression-Scalar-Quantization-SQ" class="headerlink" title="Compression - Scalar Quantization (SQ)"></a>Compression - Scalar Quantization (SQ)</h3><p>Vector를 압축하여 어느정도의 정보 손실을 감안하면서 메모리 사용량을 낮추는 방법이다. 보통 32bit floating point를 사용하는데, 8bit의 unsigned integer로 압축하여 사용한다.</p><h3 id="Pruning-Inverted-File-IVF"><a href="#Pruning-Inverted-File-IVF" class="headerlink" title="Pruning - Inverted File (IVF)"></a>Pruning - Inverted File (IVF)</h3><p>클러스터링 기법으로, 일정 벡터들을 군집화하여 Search Space를 줄인 후 검색속도를 개선하는 방법이다. 군집화 이후 쿼리가 들어왔을 때 쿼리와 가장 비슷한 군집만을 방문하도록 하는 것이다. 군집화 방법으로는 k-means clustering을 가장 많이 사용한다.</p><p>Inverted file (IVF) 라고 불리는 이유는 각 클러스터에 있는 포인트들을 인덱스로 가지고 있기 때문이다. 각 클러스터의 아이디와 클러스터 벡터들이 연결되어 있는 형태로 데이터 구조가 형성된다.</p><h2 id="Introduction-to-FAISS"><a href="#Introduction-to-FAISS" class="headerlink" title="Introduction to FAISS"></a>Introduction to FAISS</h2><p>FAISS는 large scale 데이터에 대해서 효율적으로 유사도 검색과 군집화를 도와주는 페이스북의 오픈소스 라이브러리이다. </p><p>FAISS를 사용하기 위해서는 클러스터링된 벡터들을 확보해야 한다. 클러스터를 랜덤하게 지정하면 비효율적이기 때문에 적절한 군집화를 위해 데이터셋을 활용해야 하며, Quantization을 할 때도 scale할 비율을 계산해야 하기 때문에 학습데이터를 활용해야 한다.</p><p>따라서 Train과 Adding 단계가 필요하다. </p><h2 id="Scaling-up-with-FAISS"><a href="#Scaling-up-with-FAISS" class="headerlink" title="Scaling up with FAISS"></a>Scaling up with FAISS</h2><p>IVF 인덱스 만들기 (SQ 방식)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">nlist = <span class="number">100</span> <span class="comment"># 클러스터 개수</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist) <span class="comment"># Inverted File 만들기</span></span><br><span class="line">index.train(xb) <span class="comment"># 클러스터 학습</span></span><br><span class="line"></span><br><span class="line">index.add(xb) <span class="comment"># 클러스터에 벡터 추가</span></span><br><span class="line">D, I = index.search(xq, k) <span class="comment"># 검색</span></span><br></pre></td></tr></table></figure><p>IVF 인덱스 만들기 (PQ 방식)</p><ul><li>벡터 압축 기법으로, 전체 벡터를 저장하지 않고 압축된 벡터만을 저장하여 메모리 사용량을 줄일 수 있음.</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nlist = <span class="number">100</span> <span class="comment"># 클러스터 개수</span></span><br><span class="line">m = <span class="number">8</span> <span class="comment"># subquantizer 개수</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, <span class="number">8</span>) <span class="comment"># 각각의 sub-vector가 8 bits로 인코딩됨</span></span><br><span class="line"></span><br><span class="line">index.train(xb) <span class="comment"># 클러스터 학습</span></span><br><span class="line">index.add(xb) <span class="comment"># 클러스터에 벡터 추가</span></span><br><span class="line">D, I = index.search(xq, k) <span class="comment"># 검색</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-6%EA%B0%95-Scaling-up-with-FAISS/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PStage MRC 4-5강 - Passage Retrieval - Sparse Embedding, Dense Embedding</title>
      <link>https://l-yohai.github.io/PStage-MRC-4-5%EA%B0%95-Passage-Retrieval-Sparse-Embedding-Dense-Embedding/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-4-5%EA%B0%95-Passage-Retrieval-Sparse-Embedding-Dense-Embedding/</guid>
      <pubDate>Wed, 13 Oct 2021 19:59:34 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Introduction-to-Passage-Retrieval&quot;&gt;&lt;a href=&quot;#Introduction-to-Passage-Retrieval&quot; class=&quot;headerlink&quot; title=&quot;Introduction to Passage Retrieval&quot;&gt;&lt;/a&gt;Introduction to Passage Retrieval&lt;/h2&gt;&lt;h3 id=&quot;Passage-Retrieval&quot;&gt;&lt;a href=&quot;#Passage-Retrieval&quot; class=&quot;headerlink&quot; title=&quot;Passage Retrieval&quot;&gt;&lt;/a&gt;Passage Retrieval&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Introduction-to-Passage-Retrieval"><a href="#Introduction-to-Passage-Retrieval" class="headerlink" title="Introduction to Passage Retrieval"></a>Introduction to Passage Retrieval</h2><h3 id="Passage-Retrieval"><a href="#Passage-Retrieval" class="headerlink" title="Passage Retrieval"></a>Passage Retrieval</h3><p>Passage Retrieval은 질문에 맞는 문서를 찾는 것을 의미한다. 데이터베이스에 따라서 형태가 달라질 수는 있지만, 일반적으로 웹 상에 존재하는 모든 문서를 대상으로 한다. 예를들어 토트넘이라는 쿼리에 대해서 토트넘의 역사, 우승기록, 손흥민 등에 대한 문서를 가져오는 시스템을 의미한다.</p><h3 id="Passage-Retrieval-with-MRC"><a href="#Passage-Retrieval-with-MRC" class="headerlink" title="Passage Retrieval with MRC"></a>Passage Retrieval with MRC</h3><p>Passage Retrieval을 MRC와 연결지어 생각했을 때, Open-domain Question Answering이 가능해진다. Passage Retrieval과 MRC를 이어서 2-Stage로 만들어버리는 것을 의미하는데, 쿼리가 들어왔을 때 데이터베이스에서 적절한 문서를 가져온 후, MRC 모델을 통해 가져온 문서에서 답을 찾는 프로세스이다.</p><p>이 때 중요한 것은 정답이 있을 만한 문서를 적절하게 찾아서 가져오는 과정이며, Query와 Passage를 미리 임베딩한 뒤 유사도로 랭킹을 매겨놓고, 유사도가 가장 높은 Passage를 선택한다.</p><h2 id="Passage-Embedding-and-Sparse-Embedding"><a href="#Passage-Embedding-and-Sparse-Embedding" class="headerlink" title="Passage Embedding and Sparse Embedding"></a>Passage Embedding and Sparse Embedding</h2><h3 id="Passage-Embedding-Space"><a href="#Passage-Embedding-Space" class="headerlink" title="Passage Embedding Space"></a>Passage Embedding Space</h3><p>Passage Embedding의 벡터 공간을 의미하며, 벡터화된 Passage를 이용하여 Passage 간 유사도도 계산할 수 있다.</p><h3 id="Sparse-Embedding"><a href="#Sparse-Embedding" class="headerlink" title="Sparse Embedding"></a>Sparse Embedding</h3><p>0이 아닌 숫자가 상당히 적게 분포되어 있는 벡터를 Sparse Vector라고 하는데, 보통 원핫임베딩의 경우 Vector의 차원이 굉장히 커지게 된다. 따라서 Sparse한 Embedding을 구성하는 것이 유리한데, 그 방법으로는</p><ol><li>BoW(Bag of Words)를 구성 -&gt; n-gram (하지만 n이 커질수록 제곱으로 vocab 크기가 커진다는 단점이 있음.)<ul><li>unigram: it, was, the, best, of, times</li><li>bigram: it was, was the, the best, best of, of times</li></ul></li><li>Term Value를 결정하는 방법<ul><li>Term이 document에 등장하는지 (binary)</li><li>Term이 몇 번 등장하는지 (term frequency) 등 (TF-IDF)</li></ul></li></ol><p>즉 vocab의 크기에 따라서 Embedding 차원이 결정되는데, 등장하는 단어가 많을수록, n-gram에서 n이 커질수록 차원이 커지게 된다.</p><p>이러한 BoW 방법론은 Term overlap을 정확하게 잡아내야 할 때 굉장히 유용하지만, 의미(semantic)가 비슷한데 다른 단어인 경우에는 아예 비교가 불가능하다</p><h2 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h2><h3 id="TF-IDF란"><a href="#TF-IDF란" class="headerlink" title="TF-IDF란?"></a>TF-IDF란?</h3><p>Term Frequency - Inverse Document Frequency 란 단어의 등장빈도 뿐만 아니라 단어가 제공하는 정보의 양을 곱하여 계산하며, 어떤 지문에선 자주 등장하지만 통합 문서에서는 많이 등장하지 않았으면 그 지문에서 많이 등장한 단어들에 점수를 더 주게 된다.</p><h3 id="Term-Frequency-TF"><a href="#Term-Frequency-TF" class="headerlink" title="Term Frequency (TF)"></a>Term Frequency (TF)</h3><p>해당 문서 내 단어의 등장 빈도를 의미한다. TF를 계산하는 것은 매우 간단한데, 특정 단어가 해당 문서에 몇 번 등장하는지를 계산하게 된다. 이후에 Normalize를 한 이후 비율로 변환하여 사용하게 된다.</p><h3 id="Inverse-Document-Frequency-IDF"><a href="#Inverse-Document-Frequency-IDF" class="headerlink" title="Inverse Document Frequency (IDF)"></a>Inverse Document Frequency (IDF)</h3><p>단어가 제공하는 정보의 양으로, 특정 Term이 등장한 Document 개수를 총 Document 개수로 나눈 이후에 log를 씌워서 계산한다.</p><h3 id="Combine-TF-amp-IDF"><a href="#Combine-TF-amp-IDF" class="headerlink" title="Combine TF &amp; IDF"></a>Combine TF &amp; IDF</h3><p>TF와 IDF를 곱하여 최종 점수를 계산하며, ‘a’나 ‘the’ 같은 매우 자주 나오는 관사들은 TF는 높지만 IDF가 0에 가깝기 때문에 매우 낮은 점수를 기록할 것이며, 자주 등장하지 않는 고유명사 들에는 IDF가 커지면서 전체적인 TF-IDF 값이 증가할 것이다.</p><h3 id="TF-IDF를-이용해-유사도-구하기"><a href="#TF-IDF를-이용해-유사도-구하기" class="headerlink" title="TF-IDF를 이용해 유사도 구하기"></a>TF-IDF를 이용해 유사도 구하기</h3><p>TF-IDF에서는 cosine 유사도를 사용하며, 결국엔 cosine distance로 볼 수 있다.</p><h3 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h3><p>TF-IDF보다 많이 쓰이는 BoW와 유사한 sparse embedding으로, TF-IDF에서 문서의 길이까지 고려하여 점수를 매긴다. 검색엔진이나 추천 시스템에서 굉장히 많이 사용되는 방법론이다.</p><h2 id="Introduction-to-Dense-Embedding"><a href="#Introduction-to-Dense-Embedding" class="headerlink" title="Introduction to Dense Embedding"></a>Introduction to Dense Embedding</h2><h3 id="Passage-Embedding"><a href="#Passage-Embedding" class="headerlink" title="Passage Embedding"></a>Passage Embedding</h3><p>Passage Embedding은 구절(passage)을 벡터로 변환한 것으로, TF-IDF와 같은 Sparse Embedding의 경우에는 벡터의 크기는 매우 크지만 실제로 0이 아닌 숫자가 상당히 적게 존재한다. 또한 이러한 BoW 방법론 같은 경우에는 특정 단어가 있어야 0이 아닌 값이 생기기 때문에 실제로 대부분의 값들이 0으로 존재한다.</p><h3 id="Limitations-of-sparse-embedding"><a href="#Limitations-of-sparse-embedding" class="headerlink" title="Limitations of sparse embedding"></a>Limitations of sparse embedding</h3><p>Sparse Embedding의 차원이 큰 것은 compressed format으로 해결할 순 있지만… 가장 큰 단점은 유사성을 고려하지 못한다는 것이다.</p><h3 id="Dense-Embedding"><a href="#Dense-Embedding" class="headerlink" title="Dense Embedding"></a>Dense Embedding</h3><p>Sparse Embedding의 한계를 해결하기 위해 Dense Embedding이 등장하고 최근에는 훨씬 많이 사용되고 있다. Sparse embedding에 비해 더 작은 차원의 고밀도 벡터로 이루어져있고, 대부분의 요소가 0이 아닌 값들을 가지게 된다. 또한 각 차원이 특정 Term에 대응되지 않는다는 특징이 있다.</p><h3 id="Retrieval-Sparse-vs-Dense"><a href="#Retrieval-Sparse-vs-Dense" class="headerlink" title="Retrieval: Sparse vs. Dense"></a>Retrieval: Sparse vs. Dense</h3><p>Sparse Embedding은 단어의 존재 유무를 알아맞추기 상당히 유용하지만 의미를 해석하기가 어렵고, 따라서 Dense 처럼 의미가 같더라도 다른 단어로 표현된 것을 탐지할 수 있는 임베딩을 사용하게 된다.</p><p>하지만 중요한 Term들이 정확히 일치해야 하는 경우에 Sparse 임베딩의 성능이 확실히 뛰어나기 때문에 Sparse 혼자만으로는 할 수 있는 것이 적기 때문에 Sparse와 Dense를 동시에 사용하거나, Dense만으로 구성하는 경우가 많아졌다.</p><h2 id="Training-Dense-Encoder"><a href="#Training-Dense-Encoder" class="headerlink" title="Training Dense Encoder"></a>Training Dense Encoder</h2><p>BERT와 같은 Pre-trained Language Model(PLM)이면 Dense Encoder가 될 수 있으며, 그 외 다양한 Neural Net 구조도 가능하다. Passage를 Embedding 하게 될 때는 CLS 토큰의 output을 사용하게 된다. 이후엔 Question Encoder와 Passage Encoder 에서 각각 나온 Dense Embedding을 dot product 함으로써 유사도를 구하게 된다. 학습 시에는 두 인코더를 Fine Tuning하게 된다.</p><h3 id="Dense-Encoder-학습-목표와-학습-데이터"><a href="#Dense-Encoder-학습-목표와-학습-데이터" class="headerlink" title="Dense Encoder 학습 목표와 학습 데이터"></a>Dense Encoder 학습 목표와 학습 데이터</h3><ul><li><p>학습목표: 연관된 Question과 Passage Dense Embedding 간의 거리를 좁히는 것 (내적값을 높이는 것)으로 higher한 similarity를 찾는 것을 목표로 한다.</p></li><li><p>Challenge: 그렇다면, 연관된 Question과 Passage를 어떻게 찾을 것인가? 이것은 기존 MRC 데이터셋을 활용하여 해결할 수 있다.</p><p>-&gt; 연관된 Question과 Passage 간의 dense embedding 거리를 좁히며 (positive), 연관되지 않은 것들에 대해서는 거리를 멀게 하는 방식으로 학습을 진행한다. (negative)</p></li><li><p>Negative Sampling</p><ul><li>Corpus 내에서 랜덤하게 뽑으며, 좀 더 헷갈리는 negative 샘플들 (TF-IDF 스코어는 높지만, 답을 포함하지 않는 것들)을 뽑아낸다.</li></ul></li><li><p>Objective function</p><ul><li>Positive passage에 대한 Negative Log Likelihood Loss를 사용한다.</li></ul></li><li><p>성능을 측정하는 방법으로는 Tok-k retrieval accuracy: retrieve 된 passage 중에 답을 포함하는 passage 비율로 측정한다.</p></li></ul><h2 id="Passage-Retrieval-with-Dense-Encoder"><a href="#Passage-Retrieval-with-Dense-Encoder" class="headerlink" title="Passage Retrieval with Dense Encoder"></a>Passage Retrieval with Dense Encoder</h2><h3 id="From-dense-encoding-to-retrieval"><a href="#From-dense-encoding-to-retrieval" class="headerlink" title="From dense encoding to retrieval"></a>From dense encoding to retrieval</h3><p>Passage와 Query를 각각 임베딩한 후, 쿼리로부터 가까운 순서대로 Pasage의 순서를 매긴다.</p><h3 id="From-retrieval-to-odqa"><a href="#From-retrieval-to-odqa" class="headerlink" title="From retrieval to odqa"></a>From retrieval to odqa</h3><p>Retriever를 통해 찾아낸 Passage를 활용하여, MRC 모델로 답을 찾는다.</p><h3 id="Dense-Encoding을-개선하는-방법"><a href="#Dense-Encoding을-개선하는-방법" class="headerlink" title="Dense Encoding을 개선하는 방법"></a>Dense Encoding을 개선하는 방법</h3><ul><li>학습방법 개선 (DPR)</li><li>인코더 모델 개선 (BERT보다 크고 정확한 Pretrained 모델)</li><li>데이터 개선 (더 많은 데이터와 전처리 등)</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-4-5%EA%B0%95-Passage-Retrieval-Sparse-Embedding-Dense-Embedding/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PStage MRC 3강 - Generation-based MRC</title>
      <link>https://l-yohai.github.io/PStage-MRC-3%EA%B0%95-Generation-based-MRC/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-3%EA%B0%95-Generation-based-MRC/</guid>
      <pubDate>Wed, 13 Oct 2021 02:50:23 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Generation-based-MRC&quot;&gt;&lt;a href=&quot;#Generation-based-MRC&quot; class=&quot;headerlink&quot; title=&quot;Generation-based MRC&quot;&gt;&lt;/a&gt;Generation-based MRC&lt;/h2&gt;&lt;h3 id=&quot;Generation-based-MRC-문제-정의&quot;&gt;&lt;a href=&quot;#Generation-based-MRC-문제-정의&quot; class=&quot;headerlink&quot; title=&quot;Generation-based MRC 문제 정의&quot;&gt;&lt;/a&gt;Generation-based MRC 문제 정의&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Generation-based-MRC"><a href="#Generation-based-MRC" class="headerlink" title="Generation-based MRC"></a>Generation-based MRC</h2><h3 id="Generation-based-MRC-문제-정의"><a href="#Generation-based-MRC-문제-정의" class="headerlink" title="Generation-based MRC 문제 정의"></a>Generation-based MRC 문제 정의</h3><p>Extraction-based mrc와 다르게 주어진 질문의 답이 지문 내에 존재하지 않을 수도 있기 때문에 기본적으로 답변을 ‘생성’하는 문제로 분류한다.</p><p>input은 Extraction-based mrc와 동일하지만, generation-based mrc에서는 fine-tuning 시 정답 text까지 생성하는 seq2seq 모델로 분류할 수 있다.</p><h3 id="Generation-based-MRC-vs-Extraction-base-MRC"><a href="#Generation-based-MRC-vs-Extraction-base-MRC" class="headerlink" title="Generation-based MRC vs. Extraction-base MRC"></a>Generation-based MRC vs. Extraction-base MRC</h3><ol><li>MRC 모델 구조<ul><li>Seq-to-Seq PLM 구조 (Generation) vs. PLM + Classifier 구조 (Extraction)</li></ul></li><li>Loss 계산을 위한 답의 형태 / Prediction의 형태<ul><li>Free-form text 형태 (Generation) vs. 지문 내 답의 위치 (Extraction)</li><li>Extraction-based MRC는 f1 계산을 위해 text로의 별도의 전환 과정이 필요하다.</li></ul></li></ol><h2 id="Pre-processing"><a href="#Pre-processing" class="headerlink" title="Pre-processing"></a>Pre-processing</h2><h3 id="Tokenize"><a href="#Tokenize" class="headerlink" title="Tokenize"></a>Tokenize</h3><p>Extraction과 같이 토큰화를 진행한 뒤 input_ids(또는 input_token_ids)를 생성한다.</p><p>Generation 모델에서는 PAD 토큰은 사용되지만, CLS와 SEP 토큰의 경우 사용할 수는 있지만 보통 자연어를 이용한 텍스트 포맷으로 대신하는 경우가 많다. ex) CLS -&gt; Question, SEP -&gt; Context</p><h3 id="입력표현"><a href="#입력표현" class="headerlink" title="입력표현"></a>입력표현</h3><p>Extraction 모델과 달리 어텐션 마스크만 사용한다. 대부분 decoder가 사용된 모델에서는 입력시퀀스를 구분하지 않기 때문이다.</p><h3 id="출력표현"><a href="#출력표현" class="headerlink" title="출력표현"></a>출력표현</h3><p>토큰의 시작과 끝 위치를 맞추는 것이 아닌 정답 텍스트를 표현하기만 하면 된다.</p><p>모델의 출력을 선형 레이어에 넣고, sequence_length 내 각 위치마다 들어가야할 단어를 하나씩 선택하면서 전체 길이만큼 반복하며 시퀀스를 생성한다.</p><h2 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h2><h3 id="BART"><a href="#BART" class="headerlink" title="BART"></a>BART</h3><ul><li>seq2seq 문제의 pretraining을 위한 denoising autoencoder를 사용한다.</li><li>인코더는 BERT와 같은 bi-directional 구조이며, 디코더는 GPT와 같은 uni-directional(autoregressive)이다.</li><li>텍스트에 노이즈를 주고 원래 텍스트를 복구하는 문제를 푸는 것으로 pretraining을 진행한다.</li></ul><h2 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h2><h3 id="Decoding"><a href="#Decoding" class="headerlink" title="Decoding"></a>Decoding</h3><p>보통 스페셜 토큰을 사용하여 문장의 시작점부터 search를 진행한다.</p><ul><li>Greedy Search<ul><li>빠르지만 처음에 선택한 답이 잘못된 선택일 수 있다.</li></ul></li><li>Exhaustive Search<ul><li>모든 가능성을 고려하기 때문에 vocab이나 문장 길이가 조금만 길어도 불가능에 가까워진다.</li></ul></li><li>Beam Search<ul><li>Exhaustive Search를 하되 각 타임스텝마다 가장 가능성이 높은 Top-k만 유지한다.</li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-3%EA%B0%95-Generation-based-MRC/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PStage MRC 2강 - Extraction-based MRC</title>
      <link>https://l-yohai.github.io/PStage-MRC-2%EA%B0%95-Extraction-based-MRC/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-2%EA%B0%95-Extraction-based-MRC/</guid>
      <pubDate>Tue, 12 Oct 2021 02:41:29 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Extraction-based-MRC&quot;&gt;&lt;a href=&quot;#Extraction-based-MRC&quot; class=&quot;headerlink&quot; title=&quot;Extraction-based MRC&quot;&gt;&lt;/a&gt;Extraction-based MRC&lt;/h2&gt;&lt;h3 id=&quot;Extraction-based-MRC의-문제-정의&quot;&gt;&lt;a href=&quot;#Extraction-based-MRC의-문제-정의&quot; class=&quot;headerlink&quot; title=&quot;Extraction-based MRC의 문제 정의&quot;&gt;&lt;/a&gt;Extraction-based MRC의 문제 정의&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Extraction-based-MRC"><a href="#Extraction-based-MRC" class="headerlink" title="Extraction-based MRC"></a>Extraction-based MRC</h2><h3 id="Extraction-based-MRC의-문제-정의"><a href="#Extraction-based-MRC의-문제-정의" class="headerlink" title="Extraction-based MRC의 문제 정의"></a>Extraction-based MRC의 문제 정의</h3><p>질문의 답변이 항상 주어진 지문 내에 span으로 존재한다고 가정한다.</p><p>평가방법은 크게 EM과 f1이 존재한다. f1은 조금 더 soft한 metric으로 보통 EM보다 점수가 높게 형성된다. EM score는 정답과 조금이라도 다르면 0점을 부여하고, F1은 정답과의 overlap 비율을 계산하여 부분점수를 부여한다.</p><p>모델의 input은 Context, Question 두 개의 벡터가 임베딩의 형태로 들어가게 되며 start index와 end index이 존재하는 벡터가 output이 된다.</p><h2 id="Pre-processing"><a href="#Pre-processing" class="headerlink" title="Pre-processing"></a>Pre-processing</h2><h3 id="Tokenization"><a href="#Tokenization" class="headerlink" title="Tokenization"></a>Tokenization</h3><p>텍스트를 작은 단위(token)로 나누어야 한다.</p><p>띄어쓰기, 형태소, subword 등 여러 단위의 토큰 기준이 사용되는데, 최근엔 OOV(out of vocab) 문제를 해결해주고 정보학적 이점을 가진 BPE가 주로 사용되며, WordPiece Tokenizer 역시 BPE 방법론 중 하나이다.</p><p>input으로 들어갈 때 SEP 라는 special 토큰으로 context와 질문을 구분하게 된다.</p><h3 id="Attention-Mask"><a href="#Attention-Mask" class="headerlink" title="Attention Mask"></a>Attention Mask</h3><p>입력 시퀀스 중에서 attention 연산 시 무시할 토큰을 표시한다. 보통 0은 무시, 1은 연산에 포함하며, PAD와 같은 의미를 가지지 않은 special token을 무시하기 위해 사용한다.</p><h3 id="Token-Type-IDS"><a href="#Token-Type-IDS" class="headerlink" title="Token Type IDS"></a>Token Type IDS</h3><p>입력이 두 개 이상의 시퀀스(ex; 질문 &amp; 지문)로 구성되어 있을 때 각각에게 ID를 부여하여 모델이 구분하여 해석하도록 유도하게 된다. MRC에선 질문이 항상 첫 번째 문장으로 구성되기 때문에 질문을 0으로, 지문을 1로 구성하고 padding 값들에 0을 부여하여 지문 내에서만 정답을 찾을 수 있도록 한다.</p><h3 id="모델-출력값"><a href="#모델-출력값" class="headerlink" title="모델 출력값"></a>모델 출력값</h3><p>정답은 문서 내에 존재하는 연속된 단어 토큰(span)이므로, 시작 인덱스와 종료 인덱스를 알면 정답을 맞출 수 있다. Extraction-based에선 답안을 생성하기 보다, 시작위치와 끝위치를 예측하도록 학습하여 Token Classification Task로 치환하여 학습을 진행한다.</p><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>Question + Paragraph 가 SEP 토큰으로 구분되어 있는 워드 임베딩이 input으로 들어온 후 Start/End Span을 output으로 내보내야 한다. 이 output span은 각 토큰이 답의 시작 토큰일 확률, 각 토큰이 답의 끝 토큰일 확률로 구성되며, 실제 답의 start/end position과 CELoss를 취하게 된다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">logits = self.qa_outputs(sequence_output)</span><br><span class="line">start_logits, end_logits = logits.split(<span class="number">1</span>, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss_fc = CrossEntropyLoss(ignore_index=ignored_index)</span><br><span class="line">start_loss = loss_fct(start_logits, start_position)</span><br><span class="line">end_loss = loss_fct(end_logits, end_position)</span><br><span class="line">total_loss = (start_loss + end_loss) / <span class="number">2</span></span><br></pre></td></tr></table></figure><h2 id="Post-processing"><a href="#Post-processing" class="headerlink" title="Post-processing"></a>Post-processing</h2><h3 id="불가능한-답-제거"><a href="#불가능한-답-제거" class="headerlink" title="불가능한 답 제거"></a>불가능한 답 제거</h3><p>아래와 같은 경우에는 candidate list에서 후보를 제거해야한다.</p><ul><li>End position이 start position보다 앞에 있는 경우</li><li>예측한 위치가 context를 벗어난 경우 (ex - question 위치에서 답이 등장한 경우)</li><li>미리 설정한 max_answer_length보다 길이가 긴 경우</li></ul><h3 id="최적의-답-찾기"><a href="#최적의-답-찾기" class="headerlink" title="최적의 답 찾기"></a>최적의 답 찾기</h3><ol><li>Start/end position prediction 에서 score(logits)가 가장 높은 n개를 찾는다.</li><li>불가능한 start/end 조합을 제거한다.</li><li>가능한 조합들을 score의 합이 큰 순서대로 정렬한다.</li><li>Score가 가장 큰 조합을 최종 예측으로 선정한다.</li><li>Top-k가 필요한 경우 차례대로 내보낸다.</li></ol>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-2%EA%B0%95-Extraction-based-MRC/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>PStage MRC 1강 - MRC Intro</title>
      <link>https://l-yohai.github.io/PStage-MRC-1%EA%B0%95-MRC-Intro/</link>
      <guid>https://l-yohai.github.io/PStage-MRC-1%EA%B0%95-MRC-Intro/</guid>
      <pubDate>Tue, 12 Oct 2021 00:58:58 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Introduction-to-MRC&quot;&gt;&lt;a href=&quot;#Introduction-to-MRC&quot; class=&quot;headerlink&quot; title=&quot;Introduction to MRC&quot;&gt;&lt;/a&gt;Introduction to MRC&lt;/h2&gt;&lt;p&gt;Machine Reading Comprehension (기계독해)는 주어진 지문(Context)를 이해하고, 주어진 질의(Query/Question)의 답변을 추론하는 문제이다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Introduction-to-MRC"><a href="#Introduction-to-MRC" class="headerlink" title="Introduction to MRC"></a>Introduction to MRC</h2><p>Machine Reading Comprehension (기계독해)는 주어진 지문(Context)를 이해하고, 주어진 질의(Query/Question)의 답변을 추론하는 문제이다.</p><p>예를들어 서울특별시에 대한 지문에서 <code>서울의 GDP는 세계 몇 위야?</code> 라는 질문을 했을 때, <code>세계 4위</code> 라는 답변을 하는 과제이다.</p><p>주로 질문(쿼리)가 들어오면 문서에서 답을 낼 수 있는 시스템(구글, 인공지능 스피커)에서 사용되며, 관련 문서를 찾고 -&gt; 정답을 찾는 방식으로 구성된다.</p><p>MRC의 종류들은 아래와 같다.</p><p>1) Extractive Answer Datasets</p><p>   질의에 대한 답이 항상 주어진 지문의 segment(or span)으로 존재하는 경우. 이런 방식을 채택하는 데이터셋으로는 SQuAD, KorQuAD, NewsQA 등이 있다.</p><p>2) Descriptive/Narrative Answer Datasets</p><p>   답이 지문 내에 존재하지 않고, 질의를 보고 생성된 sentence(or free-form)의 형태. MS MARCO, Narrative QA 등의 데이터셋이 이러한 방식을 채택한다.</p><p>3) Multiple-choice Datasets</p><p>   최근에는 잘 사용되지 않지만, 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태임.</p><p>MRC Datasets들이 언제부터 생성되었는지 확정적으로 정해지진 않았지만, MCTest(2013)을 시초로 보고, 해당 형태는 수능 객관식문제와 비슷한 형태를 띄고 있다. 이후에 점점 지문 내에 답이 존재하지 않더라도 검색을 통해 관련된 답변을 찾아올 수 있는 형태로 데이터셋이 구축되고 있다.</p><h3 id="MRC의-Challenges"><a href="#MRC의-Challenges" class="headerlink" title="MRC의 Challenges"></a>MRC의 Challenges</h3><p>MRC를 하면서 어려운 과제들을 직면할 수 있다. 첫 번째로, 단어들의 구성이 유사하지 않지만 동일한 의미의 문장을 이해해야 하는 경우이다. Question에 있는 단어들이 Phrase에 대체로 비슷하게 존재하면 굉장히 쉽게 답을 찾을 수 있지만, 비슷한 단어가 없는 등의 경우엔 굉장히 답을 고르기 어려울 수 있다. 또한 지시대명사(it, the, him, her 등)를 찾는 것이 굉장히 어려운 과제이다. 두 번째는 주어진 지문에서 질문에 대한 답을 찾을 수 없는 경우가 있으며, 마지막으로 Multi-hop reasoning(여러 개의 document에서 질의에 대한 supporting fact를 찾아서 답을 찾아야 하는 경우)이다.</p><h3 id="MRC의-평가방법"><a href="#MRC의-평가방법" class="headerlink" title="MRC의 평가방법"></a>MRC의 평가방법</h3><p>MRC에서는 Exact Match / F1 score, ROUGE-L / BLEU 로 평가를 진행할 수 있다.</p><ul><li>EM: 예측한 답과 ground-truth이 정확히 일치하는 샘플의 비율</li><li>F1 score: 예측한 답과 ground-truth 사이의 token overlap을 F1으로 계산</li><li>ROUGE-L Score: 예측한 값과 ground-truth 사이의 overlap recall</li><li>BLEU (Bilingual Evaluation Understudy): 예측한 답과 ground-truth 사이의 precision</li></ul><h2 id="Unicode-amp-Tokenization"><a href="#Unicode-amp-Tokenization" class="headerlink" title="Unicode &amp; Tokenization"></a>Unicode &amp; Tokenization</h2><h3 id="유니코드"><a href="#유니코드" class="headerlink" title="유니코드"></a>유니코드</h3><p>Unicode는 전 세계의 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋으로, 각 문자마다 숫자 하나에 매핑한다. </p><p>유니코드와 조금 다른 방식으로 인코딩 &amp; UTF-8이 있는데, 인코딩이란 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것이며, UTF-8은 현재 가장 많이 쓰는 인코딩 방식으로 문자 타입에 따라 다른 길이의 바이트를 할당하는 방식이다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ord: 문자를 유니코드 code point로 변환</span></span><br><span class="line"><span class="built_in">ord</span>(<span class="string">&#x27;A&#x27;</span>)</span><br><span class="line">&gt; <span class="number">65</span></span><br><span class="line"><span class="built_in">ord</span>(<span class="string">&#x27;가&#x27;</span>)</span><br><span class="line">&gt; <span class="number">44032</span></span><br><span class="line"><span class="built_in">hex</span>(<span class="built_in">ord</span>(<span class="string">&#x27;A&#x27;</span>))</span><br><span class="line">&gt; <span class="string">&#x27;0x41&#x27;</span></span><br><span class="line"><span class="built_in">hex</span>(<span class="built_in">ord</span>(<span class="string">&#x27;가&#x27;</span>))</span><br><span class="line">&gt; <span class="string">&#x27;0xac00&#x27;</span></span><br><span class="line"><span class="comment"># chr: Code point를 문자로 변환</span></span><br><span class="line"><span class="built_in">chr</span>(<span class="number">44032</span>)</span><br><span class="line">&gt; <span class="string">&#x27;가&#x27;</span></span><br><span class="line"><span class="built_in">chr</span>(<span class="number">65</span>)</span><br><span class="line">&gt; <span class="string">&#x27;A&#x27;</span></span><br><span class="line"><span class="built_in">chr</span>(<span class="number">0xAC00</span>)</span><br><span class="line">&gt; <span class="string">&#x27;가&#x27;</span></span><br><span class="line"><span class="built_in">chr</span>(<span class="number">0x41</span>)</span><br><span class="line">&gt; <span class="string">&#x27;A&#x27;</span></span><br></pre></td></tr></table></figure><p>한국어는 유니코드에서 한자 다음으로 많은 코드를 차지하고 있다. 한글은 초성, 중성, 종성을 분리할 수 있기 때문에 각 성 하나하나가 유니코드에 매핑되어 있으나, 초성/중성/종성이 전부 결합되어있는 완성형 글자 하나하나도 모두 유니코드에 매핑되어 있기 때문에 가짓수가 굉장히 많아지기 때문이다.</p><h3 id="토크나이징"><a href="#토크나이징" class="headerlink" title="토크나이징"></a>토크나이징</h3><p>가장 쉽게 토크나이징을 할 수 있는 방법은 띄어쓰기 기준으로 split하는 것이나, 효과가 좋지 않아서 주로 subword 토크나이징 방식을 사용한다. 자주 쓰이는 글자 조합은 한 단위로 취급하고, 자주 쓰이지 않는 조합은 subword로 쪼갠다.</p><blockquote><p>BERT 토크나이저에서 “##”은 디코딩할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 뜻한다.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.tokenize(<span class="string">&#x27;아버지 가방에 들어가신다&#x27;</span>)</span><br><span class="line">&gt; [<span class="string">&#x27;아버지&#x27;</span>, <span class="string">&#x27;가&#x27;</span>, <span class="string">&#x27;##방&#x27;</span>, <span class="string">&#x27;##에&#x27;</span>, <span class="string">&#x27;들어&#x27;</span>, <span class="string">&#x27;##가&#x27;</span>, <span class="string">&#x27;##신&#x27;</span>, <span class="string">&#x27;##다&#x27;</span>]</span><br></pre></td></tr></table></figure><p>그 다음으로 볼 것은 BPE(Byte-Pair Encoding) 토크나이저다. 데이터 압축용으로 제안된 알고리즘이며, NLP에서 활발하게 사용되고 있다. BPE 알고리즘은 아래의 방식을 따른다.</p><ol><li>가장 자주 나오는 글자 단위 Bigram(or Byte pair)를 다른 글자로 치환한다.</li><li>치환된 글자를 저장해둔다.</li><li>1~2번을 반복한다.</li></ol><h2 id="Looking-into-the-Dataset"><a href="#Looking-into-the-Dataset" class="headerlink" title="Looking into the Dataset"></a>Looking into the Dataset</h2><h3 id="KorQuAD-살펴보기"><a href="#KorQuAD-살펴보기" class="headerlink" title="KorQuAD 살펴보기"></a>KorQuAD 살펴보기</h3><p>LG CNS에서 언어지능 연구를 위해 공개한 질의응답/기계독해 데이터셋으로 한국어 위키피디아 1,550개의 문서에 대한 10,649건의 하위 문서와 크라우드 소싱으로 제작한 63,592개의 질의응답 쌍으로 구성되어 있음. (Train 60,407 / Dev 5,774 / Test 3,898)</p><p>현재 KorQuAD v1.0과 v2.0이 공개되어 있다. 2.0은 보다 긴 분량의 문서가 포함되어 있고, 단순 자연어 문장 뿐 아니라 복잡한 표와 리스트 등을 포함하는 HTML 형태로 표현되어 있어서 문서 전체 구조에 대한 이해가 필요하다.</p><p>KorQuAD는 SQuAD v1.0의 데이터 수집 방식을 벤치마크하여 표준성을 확보하였다. 대상 문서를 수집한 뒤 전처리를 거친 후 질문/답변 약 7만쌍을 생성하였고, 양질의 질의응답 쌍을 작업자들이 생성할 수 있도록 상세한 가이드라인을 제시하였으며 2차 태깅과정에서 사람이 직접 답해보면서 Human Performance를 측정하였다.</p><p>KorQuAD는 huggingface datasets 라이브러리로 편하게 불러올 수 있으며, huggingface datasets 라이브러리를 사용하면 memory-mapped, cached 되어 있는 데이터셋을 불러올 수 있어서 메모리 공간 부족이나 전처리 과정 반복의 번거로움을 피할 수 있다.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line">dataset = load_dataset(<span class="string">&#x27;squad_kor_v1&#x27;</span>, split=<span class="string">&#x27;train&#x27;</span>) <span class="comment"># squad_kor_v2</span></span><br></pre></td></tr></table></figure><p>KorQuAD 의 답변 유형은 대상, 인물, 시간, 장소` 등 쉽게 특정할 수 있는 것이 대부분을 차지하고 있으며, 영문 표준 데이터와 특성이 유사하다.</p><p><br></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Study/">Study</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/">NLP</category>
      
      <category domain="https://l-yohai.github.io/categories/Study/NLP/MRC/">MRC</category>
      
      
      
      <comments>https://l-yohai.github.io/PStage-MRC-1%EA%B0%95-MRC-Intro/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech 9주차 회고 (2021-09-27 ~ 2021-10-01)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-9%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-27-2021-10-01/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-9%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-27-2021-10-01/</guid>
      <pubDate>Sun, 03 Oct 2021 13:31:47 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;P-Stage-강의-커리큘럼&quot;&gt;&lt;a href=&quot;#P-Stage-강의-커리큘럼&quot; class=&quot;headerlink&quot; title=&quot;P Stage 강의 커리큘럼&quot;&gt;&lt;/a&gt;P Stage 강의 커리큘럼&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;인공지능과 자연어 처리&lt;/li&gt;
&lt;li&gt;자연어의 전처리&lt;/li&gt;
&lt;li&gt;BERT 언어모델 소개&lt;/li&gt;
&lt;li&gt;한국어 BERT 언어 모델 학습&lt;/li&gt;
&lt;li&gt;BERT 기반 단일 문장 분류 모델 학습&lt;/li&gt;
&lt;li&gt;BERT 기반 두 문장 관계 분류 모델 학습&lt;/li&gt;
&lt;li&gt;BERT 언어모델 기반의 문장 토큰 분류&lt;/li&gt;
&lt;/ol&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="P-Stage-강의-커리큘럼"><a href="#P-Stage-강의-커리큘럼" class="headerlink" title="P Stage 강의 커리큘럼"></a>P Stage 강의 커리큘럼</h2><ol><li>인공지능과 자연어 처리</li><li>자연어의 전처리</li><li>BERT 언어모델 소개</li><li>한국어 BERT 언어 모델 학습</li><li>BERT 기반 단일 문장 분류 모델 학습</li><li>BERT 기반 두 문장 관계 분류 모델 학습</li><li>BERT 언어모델 기반의 문장 토큰 분류</li></ol><h2 id="대회관련"><a href="#대회관련" class="headerlink" title="대회관련"></a>대회관련</h2><h3 id="협업전략-툴"><a href="#협업전략-툴" class="headerlink" title="협업전략, 툴"></a>협업전략, 툴</h3><p><strong>협업툴</strong></p><p>협업툴은 Notion을 메인으로 사용한다. 여러 툴에서 파편적인 기능들만을 선별적으로 사용할 수도 있지만, 데드라인이 주어진 대회라는 플랫폼 하에서는 채널이 다양할 수록 혼란만 가중되는 것 같다. Notion에서 여러 유용한 Template을 하나하나 가져와서 꽤 괜찮은 Dashboard를 만들고, 코드관리를 제외한 모든 것을 Notion으로 해결하고 있다. 기록의 중요성에 공감을 많이 하여 모인 팀 답게, Notion을 적극적으로 사용하고 있고 그로인해 체계적인 실험과 작업관리가 가능해진 것 같다.</p><p>또한 Wandb를 팀단위로 만들어서 각자 한 실험에 대한 log를 시각화하여 볼 수 있게 되었다. good practice인듯!!</p><p><strong>협업전략</strong></p><p>실험전략은 개인이 하고싶은 것 위주로 플래닝을 진행하며, 작은 모델을 Base로 하여 빠른 실험이 가능하도록 했다. 또한 실험을 통한 성능비교를 위해 hyperparameter나 seed등 변수가 될 수 있는 모든 것들을 fix하여 진행한다. 코드의 경우 본인의 실험에 따라 각자의 코드가 전부 달라지며, 그것을 하나로 통합하면서 진행하는 것은 대회에서는 굉장히 비효율적인 방식인 것 같다. 한 번 진행해보고 PR이나 Merge에 병목이 생기는 것을 경험한 뒤로, 언제든 그때그때의 실험으로 Rollback이 가능하도록 branch를 여러개로 나누되, 합치진 않기로 결정했다.</p><h3 id="NLP-Augmentation"><a href="#NLP-Augmentation" class="headerlink" title="NLP Augmentation"></a>NLP Augmentation</h3><p>NLP에서 Text Augmentation 방법은 크게 텍스트의 일부를 변형하여 데이터를 증강하는 방법과 생성모델을 사용하여 새로운 텍스트를 생성하여 데이터를 증강하는 방법이 있다. 그 중에서 가장 손쉽게 접근할 수 있는 방법은 <a href="https://github.com/toriving/KoEDA">KoEDA</a> 라이브러리를 사용하는 것이었다.</p><p>KoEDA는 <code>EDA</code>와 <code>AEDA</code> 논문에서 소개된 방식을 한국어 Wordnet 으로 Porting하여 공개한 오픈소스 라이브러리이다.</p><ul><li><code>EDA</code>의 경우 SR, RI, RS, RD 네 가지 Operation을 제공하며, 한 문장에 대해서 몇 개의 문장을 만들건지에 따라 $\alpha$ 값에 조정이 필요하며, 4문장 이하는 $p=0.1$, 4문장 초과는 $p=0.05$ 정도의 확률값으로 데이터를 변형하는게 가장 성능이 좋았다고 저술되어있다. 하지만 텍스트 데이터의 특성상, 위치를 바꾸거나 일부 단어를 제거하는 것은 결국 본 문장의 의미를 손실시키는 행위이기 때문에 <code>AEDA</code> 방법론이 등장하게 된다.</li><li><code>AEDA</code>는 문장을 손실시키지 않게 하기 위해 Special character를 문장 곳곳에 배치하는 방법론으로, 역시 많은 특수문자가 들어가게 되면 성능이 떨어지기 때문에 적절한 확률값을 찾는 것이 중요하다.</li></ul><p>KoEDA의 Snippet<br><figure class="highlight python"><figcaption><span>eda.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">augmenter = EDA(</span><br><span class="line">              morpheme_analyzer: <span class="built_in">str</span> = <span class="literal">None</span>,  <span class="comment"># Default = &quot;Okt&quot;</span></span><br><span class="line">              alpha_sr: <span class="built_in">float</span> = <span class="number">0.1</span>,</span><br><span class="line">              alpha_ri: <span class="built_in">float</span> = <span class="number">0.1</span>,</span><br><span class="line">              alpha_rs: <span class="built_in">float</span> = <span class="number">0.1</span>,</span><br><span class="line">              prob_rd: <span class="built_in">float</span> = <span class="number">0.1</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">result = augmenter(</span><br><span class="line">            data: <span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">str</span>], <span class="built_in">str</span>], </span><br><span class="line">            p: <span class="type">List</span>[<span class="built_in">float</span>] = <span class="literal">None</span>,  <span class="comment"># Default = (0.1, 0.1, 0.1, 0.1)</span></span><br><span class="line">            repetition: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">          )</span><br></pre></td></tr></table></figure></p><figure class="highlight python"><figcaption><span>aeda.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">augmenter = AEDA(</span><br><span class="line">              morpheme_analyzer: <span class="built_in">str</span> = <span class="literal">None</span>,  <span class="comment"># Default = &quot;Okt&quot;</span></span><br><span class="line">              punc_ratio: <span class="built_in">float</span> = <span class="number">0.3</span>,</span><br><span class="line">              punctuations: <span class="type">List</span>[<span class="built_in">str</span>] = <span class="literal">None</span>  <span class="comment"># default = (&#x27;.&#x27;, &#x27;,&#x27;, &#x27;!&#x27;, &#x27;?&#x27;, &#x27;;&#x27;, &#x27;:&#x27;)</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">result = augmenter(</span><br><span class="line">            data: <span class="type">Union</span>[<span class="type">List</span>[<span class="built_in">str</span>], <span class="built_in">str</span>], </span><br><span class="line">            p: <span class="built_in">float</span> = <span class="literal">None</span>,  <span class="comment"># Default = 0.3 </span></span><br><span class="line">            repetition: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">          )</span><br></pre></td></tr></table></figure><ul><li>하지만 <code>EDA</code>, <code>AEDA</code> 모두 KLUE-RE Task에선 별 효과가 없는 것 같다..</li></ul><p>다음 방법은 생성모델을 활용한 방법으로 <a href="https://arxiv.org/abs/1812.06705">Conditional BERT Contextual Augmentation</a> 논문에 소개되었다. 기존 BERT에서는 <code>token embedding + segment embedding + positional embedding</code> 으로 representation을 구성하지만, conditional BERT의 경우 <code>token embedding + label embedding + positional embedding</code> 으로 representation을 구성하고, label을 부착한 상태로 데이터셋을 MLM task로 pretraining한다. 이후에 mask token을 replace하는 것과 마찬가지로 label에 대하여 token replace를 수행한다.</p><ul><li>논문을 자세히 읽어보진 않았지만, 충분히 시도해볼만한 가치가 있는 것 같긴 하다. 현재 augmentation에 대한 효과를 전혀 보지 못했기 때문에 이번 실험이 끝나면 해봐야지…</li></ul><h3 id="Huggingface"><a href="#Huggingface" class="headerlink" title="Huggingface"></a>Huggingface</h3><p>살면서 huggingface 공식문서를 가장 많이 읽은 한 주였다. Trainer와 Tokenizer는 거의 다 읽어는 봤는데, 기능이 너무 많아서 두고두고 찾아보면서 개발해야할 것 같다.</p><p>그 중에서 인상깊은 기능들은 아래와 같다.</p><h3 id="hyperpameter-search-of-Trainer-class-Huggingface-transformers"><a href="#hyperpameter-search-of-Trainer-class-Huggingface-transformers" class="headerlink" title="hyperpameter_search of Trainer class (Huggingface, transformers)"></a>hyperpameter_search of Trainer class (Huggingface, transformers)</h3><p>KLUE RE(Relation Extraction) 과제를 하면서 모델을 finetuning 해보다가 hyperparameter가 생각보다 성능에 많은 영향을 준다는 것을 알게 되었다. 그에 따라 huggingface transformers의 Trainer 클래스에서 제공하는 <code>hyperparameter_search</code> 를 사용하여 최적의 hyperparameter를 찾아보는 실험을 하게 되면서 사용법을 알 필요가 있었다.</p><p><img src="/image/hp_search.png" alt=""></p><p>trainer의 hyperparameter_search method를 보시면, 하이퍼파라미터를 찾을 때 <code>optuna</code> , <code>Ray Tune</code>, <code>SigOpt</code> 세 친구들을 통해 hyperparameter를 탐색할 수 있다. huggingface의 trainer가 train 시 wandb와 연동되는 메커니즘과 비슷하다고 이해하면 될듯.</p><p><strong>Snippet</strong></p><figure class="highlight python"><figcaption><span>hp_search.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> load your tokenizer &amp; dataset</span></span><br><span class="line"><span class="comment"># tokenizer = ...</span></span><br><span class="line"><span class="comment"># dataset = ...</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> change your pretrained model path</span></span><br><span class="line">config = AutoConfig.from_pretrained(<span class="string">&quot;YOUR_MODEL_PATH&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_init</span>():</span></span><br><span class="line">    <span class="keyword">return</span> AutoModelForSequenceClassification.from_pretrained(</span><br><span class="line">        model_path, config=config)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> fill it your training arguments</span></span><br><span class="line">training_args = TrainingArguments(...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> fill it your trainer arguments</span></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model_init=model_init,</span><br><span class="line">    args=training_args,</span><br><span class="line">    ...</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> optuna</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optuna_hp_space</span>(<span class="params">trial</span>):</span></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;learning_rate&quot;</span>: trial.suggest_float(<span class="string">&quot;learning_rate&quot;</span>, <span class="number">5e-6</span>, <span class="number">5e-4</span>, log=<span class="literal">True</span>),</span><br><span class="line">        <span class="string">&quot;num_train_epochs&quot;</span>: trial.suggest_int(<span class="string">&quot;num_train_epochs&quot;</span>, <span class="number">1</span>, <span class="number">5</span>),</span><br><span class="line">        <span class="string">&quot;seed&quot;</span>: trial.suggest_int(<span class="string">&quot;seed&quot;</span>, <span class="number">1</span>, <span class="number">42</span>),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> ray tune</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ray_hp_space</span>():</span></span><br><span class="line">    <span class="keyword">from</span> ray <span class="keyword">import</span> tune</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&quot;learning_rate&quot;</span>: tune.loguniform(<span class="number">5e-6</span>, <span class="number">5e-4</span>),</span><br><span class="line">        <span class="string">&quot;num_train_epochs&quot;</span>: tune.choice(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">6</span>)),</span><br><span class="line">        <span class="string">&quot;seed&quot;</span>: tune.choice(<span class="built_in">range</span>(<span class="number">1</span>, <span class="number">42</span>)),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">trainer.hyperparameter_search(</span><br><span class="line">    direction=<span class="string">&quot;maximize&quot;</span>, <span class="comment"># <span class="doctag">NOTE:</span> or direction=&quot;minimize&quot;</span></span><br><span class="line">    hp_space=ray_hp_space, <span class="comment"># <span class="doctag">NOTE:</span> if you wanna use optuna, change it to optuna_hp_space</span></span><br><span class="line">    backend=<span class="string">&quot;ray&quot;</span>, <span class="comment"># <span class="doctag">NOTE:</span> if you wanna use optuna, remove this argument</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>사용법은 생각보다 간단하다. <code>hp_space</code> 함수에서 원하는 하이퍼파라미터와 그 하이퍼파라미터의 범위를 key, value 형태로 return 해주면 해당 dictionary를 토대로 실험이 진행된다. 위에서는 3개의 하이퍼파라미터만 작성되어 있지만 warmup_steps, weight_decay, per_device_train_batch_size 등을 동일한 방식으로 추가할 수 있고, training argument에 들어가는 대부분의 hyperparameter들을 탐색할 수 있는 것 같다.</p><p>하지만 위에서 사용하는 <code>optuna</code> 혹은 <code>raytune</code>이 hyperparameter를 탐색하는 알고리즘도 다르고, CLIReporter, Pruner, Scheduler 또한 전부 다르기 때문에 해당 라이브러리에 대한 이해가 필요한 것 같다.</p><blockquote><p>backend (<code>str</code> or <code>HPSearchBackend</code>, optional) – The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending on which one is installed. If all are installed, will default to optuna.</p></blockquote><ul><li>default 는 optuna 로 설정되어 있고, 자신의 환경에 설치되어 있는 걸 우선적으로 사용하도록 되어있다.</li></ul><p>하이퍼파라미터를 search하는 direction을 maximize 또는 minimize로 설정할 수 있는데, 이 때 사용자의 입맛대로 목적함수를 작성하여 사용할 수도 있다.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def my_objective(metrics):</span><br><span class="line">    # Your elaborate computation here</span><br><span class="line">    return result_to_optimize</span><br><span class="line"></span><br><span class="line">trainer.hyperparameter_search(</span><br><span class="line">    direction=&quot;maximize&quot;,</span><br><span class="line">    compute_objective=my_objective</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>custom object function의 경우 공식문서나 discussion 참고할 것.</p><ul><li>Ray <a href="https://docs.ray.io/en/latest/tune/index.html">https://docs.ray.io/en/latest/tune/index.html</a></li><li>optuna <a href="https://optuna.org/#code_examples">https://optuna.org/#code_examples</a></li><li><a href="https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10">https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10</a></li></ul><p><img src="/image/hp_search_wandb.png" alt="진행중인 모습"><br><img src="/image/sweep.png" alt="Wandb sweep"></p><h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><p>다음은 Focal Loss function인데, 이번 대회에서 효과를 톡톡히 보았다. Focal Loss란 <code>RetinaNet</code> 논문에서 처음 제안되었는데, Class Imabalance를 해결하기 위한 목적함수이다. 쉽게 말하면 예측하기 쉬운 값에 대해서는 0에 가까운 loss값을 부여하고, 예측하기 어려운 negative sample에 대해서는 기존보다 높은 loss값을 부여하여 weighted scale의 형태로 표현할 수 있게 만든 손실함수이다.</p><p>이것 역시 나중에 꼭 본 논문을 읽어보도록 하자….</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://www.youtube.com/watch?v=4DzKM0vgG1Y">[딥러닝 기계 번역] Seq2Seq: Sequence to Sequence Learning with Neural Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습)</a></li><li><a href="https://www.youtube.com/watch?v=7UA21vg4kKE">Paper Reivew - FastText: Enriching Word Vectors with Subword Information</a></li><li><a href="https://keep-steady.tistory.com/37">한국어 자연어처리 1편_서브워드 구축(Subword Tokenizer, Mecab, huggingface VS SentencePiece)</a></li><li><a href="https://huggingface.co/transformers/glossary.html#attention-mask">huggingface transformers의 attention mask</a></li><li><a href="https://gist.github.com/lovit/259bc1d236d78a77f044d638d0df300c">huggingface tokenizer로 konlpy 사용하기</a></li><li><a href="https://docs.likejazz.com/bert/">BERT 톺아보기</a></li><li><a href="https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=5oESe8djApQw">LM Training from scratch</a></li><li><a href="https://monologg.kr/2020/04/27/wordpiece-vocab/">나만의 BERT Wordpiece Vocab 만들기</a></li><li><a href="https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0">나만의 언어모델 만들기 - Wordpiece Tokenizer 만들기</a></li><li><a href="https://jiho-ml.com/weekly-nlp-28/">BERT만 잘 써먹어도 최고가 될 수 있다</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-9%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-27-2021-10-01/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech 7주차 회고 (2021-09-13 ~ 2021-09-17)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-7%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-13-2021-09-17/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-7%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-13-2021-09-17/</guid>
      <pubDate>Fri, 17 Sep 2021 13:35:00 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;강의내용&quot;&gt;&lt;a href=&quot;#강의내용&quot; class=&quot;headerlink&quot; title=&quot;강의내용&quot;&gt;&lt;/a&gt;강의내용&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="강의내용"><a href="#강의내용" class="headerlink" title="강의내용"></a>강의내용</h2><ul><li>강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.</li></ul><h3 id="학습현황"><a href="#학습현황" class="headerlink" title="학습현황"></a>학습현황</h3><ul><li>21.09.13 월 (09:30 ~ 20:15)</li><li>21.09.14 화 (09:20 ~ 20:50)</li><li>21.09.15 수 (09:40 ~ 23:59)</li><li>21.09.16 목 (09:58 ~ 23:59)</li><li>21.09.17 금 (10:00 ~ 22:50)</li></ul><h3 id="진도"><a href="#진도" class="headerlink" title="진도"></a>진도</h3><ul><li>Transformer - Self-supervised Pre-training Models - Advanced Self-supervised Pre-training Models</li></ul><h3 id="이번-주의-개선할-사항-및-자기성찰"><a href="#이번-주의-개선할-사항-및-자기성찰" class="headerlink" title="이번 주의 개선할 사항 및 자기성찰"></a>이번 주의 개선할 사항 및 자기성찰</h3><ul><li>멘탈 트레이닝을 잘 해야겠다.</li><li>부캠을 떠나서 그냥 이번 주에는 감정적인 대응도 많이 하고, 어떤 일의 결과들도 쉽게 승복하지 못했던 것 같다.</li></ul><hr><h2 id="과제-수행과정과-결과물"><a href="#과제-수행과정과-결과물" class="headerlink" title="과제 수행과정과 결과물"></a>과제 수행과정과 결과물</h2><ul><li>2개의 선택과제</li></ul><div class="table-container"><table><thead><tr><th>분류</th><th>해결 여부 or 결과</th></tr></thead><tbody><tr><td>선택과제 2 - NMT Training with Fairseq</td><td>O</td></tr><tr><td>선택과제 3 - Byte Pair Encoding</td><td>O</td></tr></tbody></table></div><h3 id="Facts-사실-객관"><a href="#Facts-사실-객관" class="headerlink" title="Facts (사실, 객관)"></a>Facts (사실, 객관)</h3><ul><li>Fairseq의 사용방법을 배울 수 있었다.</li><li>Byte Pair Encoding 과정을 논문을 보면서 이해할 수 있었다.</li></ul><h3 id="Feelings-느낌-주관"><a href="#Feelings-느낌-주관" class="headerlink" title="Feelings (느낌, 주관)"></a>Feelings (느낌, 주관)</h3><ul><li>과제를 왜 이렇게 냈을까라는 생각을 참 많이 했다.</li><li>억지스러운 테스트케이스와 어줍잖은 Fairseq 사용은 참 많이 아쉬웠다.</li></ul><h3 id="Affimation-자기선언"><a href="#Affimation-자기선언" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>공부해야할 양이 정말 많았는데, 당장에 하는 것들보다는 앞서 해야할 것들에 우선순위를 부여하고 여유롭게 실행해보자.</li></ul><hr><h2 id="피어세션"><a href="#피어세션" class="headerlink" title="피어세션"></a>피어세션</h2><p>나름 질의응답에도 많이 참여하려고 하고, 저번주보다는 조금 친밀한 느낌을 받으면서 세션을 진행할 수 있었다.</p><h3 id="멘토링"><a href="#멘토링" class="headerlink" title="멘토링"></a>멘토링</h3><ul><li>NLP에서의 EDA를 소개해주셨는데, 꽤 유용한 것들을 알려주셔서 많은 도움을 받을 수 있었다.</li></ul><hr><h2 id="학습회고"><a href="#학습회고" class="headerlink" title="학습회고"></a>학습회고</h2><h3 id="Affimation-자기선언-1"><a href="#Affimation-자기선언-1" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>그동안 게을리하던 논문읽기를 그래도 좀 한 것 같다. 대략적인 내용들을 파악한 뒤 읽으니까 생각보다 빠르게 읽을 수 있었던 것 같다. 그래도 아직 꼼꼼하게 하나하나의 의미를 전부 따져가면서 읽는 것은 좀 어렵지만, 점점 익숙해지면 더 나아질 것 같다.</li></ul><hr><h2 id="총평"><a href="#총평" class="headerlink" title="총평"></a>총평</h2><ul><li>새로운 팀을 구했는데, 방향성도 일치하고 한분한분 너무 실력있고 좋은 분들이라 기대가 많이 된다.</li><li>여러 학습을 진행했는데 정작 정리가 된 것은 하나도 없다. 반성하자.</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-7%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-13-2021-09-17/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>VSCode Terminal Customizing</title>
      <link>https://l-yohai.github.io/VSCode-Terminal-Customizing/</link>
      <guid>https://l-yohai.github.io/VSCode-Terminal-Customizing/</guid>
      <pubDate>Thu, 16 Sep 2021 07:22:57 GMT</pubDate>
      
      <description>&lt;p&gt;서버에서 vscode 터미널 많이들 사용하고 계시죠?!! 캠퍼여러분들께 도움이되는 터미널 커스텀에 대해서 소개해드릴까 합니다!&lt;/p&gt;
&lt;p&gt;vscode의 default terminal은 bash로, 커스터마이징의 끝은 순정이라고는 하지만.. zsh의 강력함을 한 번 맛보면 bash로 돌아가기가 정말 힘듭니다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>서버에서 vscode 터미널 많이들 사용하고 계시죠?!! 캠퍼여러분들께 도움이되는 터미널 커스텀에 대해서 소개해드릴까 합니다!</p><p>vscode의 default terminal은 bash로, 커스터마이징의 끝은 순정이라고는 하지만.. zsh의 강력함을 한 번 맛보면 bash로 돌아가기가 정말 힘듭니다.</p><p><br></p><p><img src="/image/image-20210916163359082.png" alt="굉장히 칙칙한 bash"></p><p><br></p><p>그러면.. zsh는 뭐가다른데?! 이미지 한장으로 보여드릴게요. 결과는 아래와 같습니다.</p><p><img src="/image/image-20210916145624492.png" alt="Powerlevel10k"></p><p><br></p><p>딱봐도 엄청나죠..!!!? 위의 것들은 oh-my-zsh과 powerlevel10k, 그리고 zsh-autosuggestions 과 zsh-syntax-highlighting 이라는 플러그인을 적용한 모습입니다. 여러분들도 이대로만 따라하시면 위와같은 스마트한 기능과 멋을 가진 매력적인 터미널을 얻을 수 있습니다!</p><p>자 그렇다면 지금부터 본격적인 세팅 들어갑니다. 레쓰기릿!!</p><ul><li>Linux 환경을 전제로 둔 방법입니다. MacOS나 Window사용자분들은 주의해주세요!</li></ul><hr><p>목차</p><p><a href="#Prev-Installation">1. Prev Installation</a><br><a href="#Install-oh-my-zsh">2. install oh-my-zsh</a><br><a href="#Install-zsh-plugins">3. install zsh plugins</a><br><a href="#Install-Powerlevel10k-amp-Config-p10k">4. install powerlevel10k &amp; config p10k</a><br><a href="#Color-Customizing">5. color customizing</a></p><hr><h2 id="Prev-Installation"><a href="#Prev-Installation" class="headerlink" title="Prev Installation"></a>Prev Installation</h2><p><code>curl</code> 만 설치되어있다면 모든 것을 할 수 있습니다. <code>curl</code> 이 깔려있지 않다면, 터미널을 열고</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get install curl</span><br></pre></td></tr></table></figure><p>위의 명령어를 타이핑하여 curl 설치를 먼저 진행해주세요!</p><p><img src="/image/image-20210916152202454.png" alt="curl install"></p><p>저는 이미 깔려있기 때문에, 처음 설치하는 분들은 다른 메세지가 출력될 거에요! 아무튼 저렇게 쫘르르르르륵 나오고 설치가 완료되었다는 메세지가 나타나면 성공!</p><p><br></p><h2 id="Install-oh-my-zsh"><a href="#Install-oh-my-zsh" class="headerlink" title="Install oh-my-zsh"></a>Install oh-my-zsh</h2><p>위에서 설치한 curl을 가지고 <code>oh-my-zsh</code> 를 설치하는 단계입니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot;</span><br></pre></td></tr></table></figure><p><a href="https://github.com/ohmyzsh/ohmyzsh">oh-my-zsh</a> 는 오픈소스 프로젝트로, zsh을 자유롭게 customizing 할 수 있는 프레임워크입니다.</p><p><img src="/image/image.png" alt="oh-my-zsh install"></p><p>위와 같은 메세지가 나타나면 성공!</p><p>자, 그럼 저희는 한 번의 복붙으로 oh-my-zsh에서 지원하는 여러 테마를 사용할 수 있게 되었어요!! <code>agnoster</code> 와 같은 이쁜 테마도 있긴 하지만, 저희는 조금 더 강력하고 자유로운 커스텀이 쉬운 <a href="https://github.com/romkatv/powerlevel10k">powerlevel10k</a> 라는 녀석을 사용해보도록 할거에요!</p><p>잠깐, 그전에!! 먼저 유용한 zsh plugins 부터 설치하고 갈게요!</p><p><br></p><h2 id="Install-zsh-plugins"><a href="#Install-zsh-plugins" class="headerlink" title="Install zsh plugins"></a>Install zsh plugins</h2><p>Vim에 대한 유명한 일화를 아시나요..? (대충 수십년간 빔을 사용한 개발자에게 왜 빔만 쓰냐고 물어봤더니, ‘나가는 법을 몰라서’라고 대답하는 짤) Vim을 메인으로 사용하는 개발자들은 대부분 굉장한 자부심은 물론, Customizing에 대한 엄청난 실력을 가지고 있는데요! 그래서 그분들이 shell 기반에서 편리하게 사용할 수 있는 플러그인들을 엄청나게 많이 만들어놓았다는 사실…!!</p><p>그 중에 두 가지 유명한 플러그인을 소개해드릴까 합니다!</p><p><br></p><h3 id="Auto-Suggestions"><a href="#Auto-Suggestions" class="headerlink" title="Auto Suggestions"></a>Auto Suggestions</h3><p>zsh-autosuggestions는 이전에 한번이라도 타이핑했던 command line이 있으면, 흐릿한 글씨로 이전에 타이핑했던 command를 제안해주는 기능입니다.</p><p align="center">  <img src= "/image/image-20210916154649761.png"></p><p>설치방법은 아래와 같습니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/zsh-users/zsh-autosuggestions $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure><p>설치가 완료되었다면, <code>~/.oh-my-zsh/custom/plugins</code> 라는 경로에 <code>zsh-autosuggestions</code> 라는 폴더가 생겨있을거에요! 이것을 zsh 에 반영을 시켜주어야 합니다.</p><p>oh-my-zsh를 설치하면 자동으로 root 디렉토리에 <code>.zshrc</code> 라는 파일이 생기는데요, vscode를 root 디렉토리로 여신 다음, <code>.zshrc</code> 파일을 클릭하여 에디터로 띄워줍니다.</p><p>쭈우우욱 내리다보면 </p><p><img src="/image/image-20210916155134754.png" alt="zshrc"></p><p><code>plugins=(git)</code> 이라고 되어있는 라인을 찾으실 수 있을텐데요! 이 부분을 위와같이 <code>plugins=(git zsh-autosuggestions)</code> 로 바꿔주기만 하면 끝입니다!</p><p>바로 이어서 syntax highlighting도 설치해볼까요?!</p><p><br></p><h3 id="Syntax-Highlight"><a href="#Syntax-Highlight" class="headerlink" title="Syntax Highlight"></a>Syntax Highlight</h3><p><br></p><p align="center">  <img src="/image/image-2021099999.png"></p><p>syntax highligh 기능은 적절한 리눅스 command라던지, 이전에 설정했던 올바른 alias가 typing 되었다면 색을 바꿔주는 녀석을 말합니다! default 색상은 올바른 command면 녹색, 틀린 command면 빨간색으로 표시가 돼요!</p><p>설치방법은 아래와 같습니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/zsh-users/zsh-syntax-highlighting.git $&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure><p>이 친구도 크게 다를게 없습니다. auto suggestions 처럼 위와 같이 git clone을 이용하여 플러그인을 받아줍니다.</p><p>설치가 되었다면 역시나 <code>~/.oh-my-zsh/custom/plugins</code> 라는 경로에 <code>zsh-syntax-highlighting</code> 이라는 폴더가 생겨있을거에요. 위와 똑같이 <code>zshrc</code> 에서 <code>plugins=(git zsh-autosuggestions)</code>를 찾은다음 <code>plugins=(git zsh-autosuggestions zsh-syntax-highlighting)</code> 으로 수정해주시면 됩니다.</p><p><img src="/image/image-20210916155825654.png" alt="zshrc"></p><p>이러면 이제 설치가 끝난 것이나 다름없는데요, 최종적으로 zsh에 반영하기 위해서는</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure><p>위처럼 source command를 이용하여 수정한 zshrc의 내용을 zsh에 반영시켜주어야 합니다!</p><p>source 까지 끝났다면 <code>ls</code> 나 <code>pwd</code> 같은 명령어를 두세번씩 쳐보세요. 어때요, 조금 쓸만해졌죠?!</p><p>자 아직 끝이 아닙니다!! 거의 다왔어요. 조금만 힘내세요!</p><p><br></p><h2 id="Install-Powerlevel10k-amp-Config-p10k"><a href="#Install-Powerlevel10k-amp-Config-p10k" class="headerlink" title="Install Powerlevel10k &amp; Config p10k"></a>Install Powerlevel10k &amp; Config p10k</h2><p>oh-my-zsh에서 powerlevel10k를 설치하는 명령어는 아래와 같습니다.</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth=1 https://github.com/romkatv/powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom&#125;/themes/powerlevel10k</span><br></pre></td></tr></table></figure><p>터미널에서 위의 명령어를 친 후, 설치가 되었다면 또다시 <code>.zshrc</code> 파일을 수정해줘야겠죠?!</p><p><img src="/image/image-20210916160753301.png" alt="zshrc"></p><p><code>ZSH_THEME=robbyrussell</code> 부분을 찾아서 위의 사진처럼 <code>ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot;</code> 로 변경해주시면 끝이에요!</p><p>그다음에</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~/.zshrc</span><br></pre></td></tr></table></figure><p>source를 통해 zshrc를 zsh에 반영해주면..!!?</p><p><img src="/image/image-20210916161006548.png" alt="p10k config"></p><p>짜잔..!!! 설정마법사가 등장했어요. 이제 위에서 나오는 질문들에 대해서 주어진 보기와 같은 알파벳들을 타이핑하면 해당 설정들이 반영된 나만의 터미널이 완성이 된답니다~~~!!!</p><p><img src="/image/image-20210916145624492-1776271.png" alt="Powerlevel10k"></p><p>저 같은 경우엔 <code>one line, compact, 12hour format ..</code> 등등과 같은 옵션을 선택하여 위와 같은 결과를 얻게 되었답니다!! :)</p><p><br></p><h2 id="Color-Customizing"><a href="#Color-Customizing" class="headerlink" title="Color Customizing"></a>Color Customizing</h2><p>마지막으로 VSCode에서의 터미널 Color Customizing에 대한 것입니다! 저같은 경우는 <code>mojave</code> 라는 vscode 테마를 사용하고 있어서 따로 터미널 색을 지정해주진 않았지만, 기본 default theme를 사용중이신 분들께 유용한 링크를 하나 소개해드리려고 해요!</p><p>바로, <a href="https://glitchbone.github.io/vscode-base16-term/#/">vscode base16 term</a> 이라는 곳인데요, 여기에서 하나씩 클릭해보면서 원하는 색 조합을 선택한 뒤,</p><p><img src="/image/image-20210916161543861.png" alt="base16"></p><p>Copy to clipboard 를 클릭하시면 vscode color setting에 대응하는 json 형태의 key, value가 클립보드에 복사가돼요!</p><p>이것을 vscode의 <code>settings.json</code> 파일의 맨 아래에다가 복사를 해주면 끝입니다!!</p><p><img src="/image/image-20210916161832232.png" alt="setting.json"></p><p>참고로 settings.json 의 위치는 사용자에 따라서 다를 수 있어요! Vscode에서 <code>ctrl ,</code> 를 입력하신 뒤 설정창에서 setting 을 검색하시면</p><p><img src="/image/image-20210916161950304.png" alt="settings.json"></p><p>위와 같이 <code>settings.json에서 편집</code> 이라는 링크가 나오는데, 이것을 누르시면 자동으로 연결된 settings.json 파일이 생성되거나, 이미 있다면 기존 settings.json 파일이 열립니다!</p><p>그럼 이상으로 terminal customizing 을 마무리하겠습니다!! 긴 글 읽어주셔서 감사하고, 이쁜 터미널로 즐거운 코딩 하시길 바라겠습니다! :)</p><p><br><br><br></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Programming/">Programming</category>
      
      <category domain="https://l-yohai.github.io/categories/Programming/Tips/">Tips</category>
      
      
      
      <comments>https://l-yohai.github.io/VSCode-Terminal-Customizing/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech 6주차 회고 (2021-09-06 ~ 2021-09-10)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-6%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-06-2021-09-10/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-6%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-06-2021-09-10/</guid>
      <pubDate>Fri, 10 Sep 2021 13:01:53 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;강의내용&quot;&gt;&lt;a href=&quot;#강의내용&quot; class=&quot;headerlink&quot; title=&quot;강의내용&quot;&gt;&lt;/a&gt;강의내용&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="강의내용"><a href="#강의내용" class="headerlink" title="강의내용"></a>강의내용</h2><ul><li>강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.</li></ul><h3 id="학습현황"><a href="#학습현황" class="headerlink" title="학습현황"></a>학습현황</h3><ul><li>21.09.06 월 (05:30 ~ 20:15)</li><li>21.09.07 화 (05:20 ~ 20:50)</li><li>21.09.08 수 (06:00 ~ 22:00)</li><li>21.09.09 목 (08:28 ~ 23:59)</li><li>21.09.10 금 (00:00 ~ 03:00, 09:58 ~ 21:50)</li></ul><h3 id="진도"><a href="#진도" class="headerlink" title="진도"></a>진도</h3><ul><li>Intro to NLP, Bag of Words - Word Embedding - Recurrent Neural Network - LSTM and GRU - Sequence to Sequence - Beam Search and BLEU score</li></ul><h3 id="이번-주의-개선할-사항-및-자기성찰"><a href="#이번-주의-개선할-사항-및-자기성찰" class="headerlink" title="이번 주의 개선할 사항 및 자기성찰"></a>이번 주의 개선할 사항 및 자기성찰</h3><ul><li>이력서는 미리미리</li><li>자소서는 미리미리</li></ul><hr><h2 id="과제-수행과정과-결과물"><a href="#과제-수행과정과-결과물" class="headerlink" title="과제 수행과정과 결과물"></a>과제 수행과정과 결과물</h2><ul><li>4개의 필수과제와 1개의 선택과제</li></ul><div class="table-container"><table><thead><tr><th>분류</th><th>해결 여부 or 결과</th></tr></thead><tbody><tr><td>필수과제 1 - Data Preprocessing</td><td>O</td></tr><tr><td>필수과제 2 - RNN-based Language model</td><td>O</td></tr><tr><td>필수과제 3 - Subword-level Language model</td><td>O</td></tr><tr><td>필수과제 4 - Preprocessing for NMT model</td><td>O</td></tr><tr><td>선택과제 1 - BERT Fine-tuning with transformers</td><td>O</td></tr></tbody></table></div><h3 id="Facts-사실-객관"><a href="#Facts-사실-객관" class="headerlink" title="Facts (사실, 객관)"></a>Facts (사실, 객관)</h3><ul><li>대략적으로만 알던 RNN, LSTM, GRU를 깊게 살펴볼 수 있었음.</li><li>수식을 통해 어떤 식의 shifting 이 일어나는지 등 수식의 의미를 조금 더 곱씹어볼 수 있었음</li></ul><h3 id="Feelings-느낌-주관"><a href="#Feelings-느낌-주관" class="headerlink" title="Feelings (느낌, 주관)"></a>Feelings (느낌, 주관)</h3><ul><li>전반적으로 나름 익숙한 것들이었기에 강의에서 완전 새롭게 배운다 하는 것은 없었지만, 그게 아니었다면 강의가 너무 어려웠을 것 같다.</li><li>사실 흘려들은 내용들도 많기 때문에 다시 한 번 들을 수 있을 때 복습해보는 것이 좋을 것 같다.</li></ul><h3 id="Affimation-자기선언"><a href="#Affimation-자기선언" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>사실 이번주는 이력서쓰느라 너무 바빴기 때문에 학습에 많은 몰입을 할 수 없었던 것 같다. 다음주에는 다음 주 내용을 공부하는 것도 좋지만 이번 주 내용을 꼭 복습해봐야할 것 같다.</li></ul><hr><h2 id="피어세션"><a href="#피어세션" class="headerlink" title="피어세션"></a>피어세션</h2><p>새로운 조라서 그런지 적응할 시간이 필요하다. 물론 이력서에 자소서를 준비하느라 완전 몰입하여 참여하지 못한 것에 대한 아쉬움이 크게 남는다.</p><h3 id="질의응답"><a href="#질의응답" class="headerlink" title="질의응답"></a>질의응답</h3><ul><li>팀에 선생님이 한 분 계셔서 든든하다.</li><li>퀄리티 좋은 답변을 위해 나도 이것저것 찾아보기 때문에 하나의 질문을 커버하는 것에도 시간이 좀 걸린다. 미리 많은 것을 궁금해하고 준비하여 좀 빠르게 여러 질문들에 대응이 가능해졌으면 좋겠다.</li></ul><h3 id="멘토링"><a href="#멘토링" class="headerlink" title="멘토링"></a>멘토링</h3><ul><li>첫 멘토링이라서 자기소개 정도만 하고 수다하는 시간을 가졌다.</li><li>딱 주마다 한 번씩, 한 시간씩만 보게 되는 것 같은데 멘토님께 조금 더 많은 도움을 얻었으면 좋겠다.</li><li>예닮님 보고싶다.</li></ul><hr><h2 id="학습회고"><a href="#학습회고" class="headerlink" title="학습회고"></a>학습회고</h2><h3 id="Affimation-자기선언-1"><a href="#Affimation-자기선언-1" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>이번주 너무 공부하지 못한 시간이 많았는데 다음주부터는 정말 공부 열심히 하자 ㅠㅠ</li></ul><hr><h2 id="총평"><a href="#총평" class="headerlink" title="총평"></a>총평</h2><ul><li>많은 시간을 학습에 투자하지 못했기 때문에 회고할 내용도 크게 없는 것 같다.. 다음주부터는 진짜 빡세게 할 것!!</li><li>실습이나 과제도 무난무난했고 특별히 어려운 것은 없었던 것 같다.</li><li>하지만 강의가 너무 어려웠다. 제대로 공부하고 싶다. 앎에 있어서 허전한 느낌을 받고 싶지 않다.</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-6%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-06-2021-09-10/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>AI Competition의 협업을 위한 플래닝가이드</title>
      <link>https://l-yohai.github.io/AI-Competition%EC%9D%98-%ED%98%91%EC%97%85%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/</link>
      <guid>https://l-yohai.github.io/AI-Competition%EC%9D%98-%ED%98%91%EC%97%85%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/</guid>
      <pubDate>Sat, 04 Sep 2021 19:34:22 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;🍡-떡볶이조-팀원들-🔥&quot;&gt;&lt;a href=&quot;#🍡-떡볶이조-팀원들-🔥&quot; class=&quot;headerlink&quot; title=&quot;🍡 떡볶이조 팀원들 🔥&quot;&gt;&lt;/a&gt;🍡 떡볶이조 팀원들 🔥&lt;/h2&gt;&lt;div class=&quot;table-container&quot;&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&quot;text-align:center&quot;&gt;김다영&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;김아경&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;문하겸&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;박지민&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;이요한&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;전준영&lt;/th&gt;
&lt;th style=&quot;text-align:center&quot;&gt;정민지&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/keemdy&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/68893924?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/EP000&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/70522267?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/ddobokki&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/44228269?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/ddeokbboki-good&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/82632580?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/l-yohai&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/49181231?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/20180707jun&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/50571795?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;td style=&quot;text-align:center&quot;&gt;&lt;a href=&quot;https://github.com/minji-o-j&quot;&gt;&lt;img src=&quot;https://avatars.githubusercontent.com/u/45448731?v=4&quot; alt=&quot;Avatar&quot;&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="🍡-떡볶이조-팀원들-🔥"><a href="#🍡-떡볶이조-팀원들-🔥" class="headerlink" title="🍡 떡볶이조 팀원들 🔥"></a>🍡 떡볶이조 팀원들 🔥</h2><div class="table-container"><table><thead><tr><th style="text-align:center">김다영</th><th style="text-align:center">김아경</th><th style="text-align:center">문하겸</th><th style="text-align:center">박지민</th><th style="text-align:center">이요한</th><th style="text-align:center">전준영</th><th style="text-align:center">정민지</th></tr></thead><tbody><tr><td style="text-align:center"><a href="https://github.com/keemdy"><img src="https://avatars.githubusercontent.com/u/68893924?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/EP000"><img src="https://avatars.githubusercontent.com/u/70522267?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/ddobokki"><img src="https://avatars.githubusercontent.com/u/44228269?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/ddeokbboki-good"><img src="https://avatars.githubusercontent.com/u/82632580?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/l-yohai"><img src="https://avatars.githubusercontent.com/u/49181231?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/20180707jun"><img src="https://avatars.githubusercontent.com/u/50571795?v=4" alt="Avatar"></a></td><td style="text-align:center"><a href="https://github.com/minji-o-j"><img src="https://avatars.githubusercontent.com/u/45448731?v=4" alt="Avatar"></a></td></tr></tbody></table></div><hr><p><img src="/image/image-20210903191905704.png" alt=""></p><p>7명의 팀원이 2주동안 잠도 거의 안자가면서 올인했던 P Stage의 ‘마스크 착용 분류 대회’에 대한 플래닝 가이드입니다. 저희는 각자 실력도 다 다르고 협업문화도 잘 모르는 상태로 대회에 임하였습니다. 그러다 보니 뭘 해야할지 모르는 순간들도 너무 많았고, 여러 시행착오를 겪었기 때문에 다음 기수분들 혹은 인공지능 대회 프로세스 중 협업관리를 어떻게 하는 것이 좋을지 저희의 경험을 조금 나누어볼까 합니다.</p><p>당연히 처음엔 너무 막막했어요. 이게 잘 될까? 싶었던 대회였는데, 계속해서 시간과 에너지를 쏟으니 별로 경험이 없는 사람들 7명이서 38개 조 중 13등을 하게 되었습니다. 물론 순위권에 들었으면 좋았겠지만, 대회는 처음인 사람들끼리는 나름 성공적인 결과였다고 생각합니다. 이러한 저희 조의 이야기들이 어떤 형태로든 도움이 되고, 좋은 레퍼런스가 되기를 바랍니다.</p><hr><h2 id="🦆-대회-전략-수립-및-협업환경-구축"><a href="#🦆-대회-전략-수립-및-협업환경-구축" class="headerlink" title="🦆 대회 전략 수립 및 협업환경 구축"></a>🦆 대회 전략 수립 및 협업환경 구축</h2><p><br></p><p align="center"><em>이번생에 협업은 처음이라..</em></p><p><br></p><blockquote><ol><li>협업을 위한 적절한 채널을 설정하고, 최소한의 Git에 익숙해지자.</li><li>대회의 목표를 수립하고, 핵심 가치를 팀원들과 공유하자.</li></ol></blockquote><p><br></p><h3 id="🔥-대회-프로세스-소개-및-목표-설정"><a href="#🔥-대회-프로세스-소개-및-목표-설정" class="headerlink" title="🔥 대회 프로세스 소개 및 목표 설정"></a>🔥 대회 프로세스 소개 및 목표 설정</h3><p>인공지능 대회는 간단하게 주어진 데이터셋으로 모델을 훈련시키고 성능만 올리면 되는 단순한 프로세스로 이루어져 있습니다. 하지만 세부적으로 보면 <code>데이터 정제 및 전처리와 Labeling -&gt; 실험을 위한 베이스라인 코드 작성 -&gt; 모델링과 실험 -&gt; 제출</code> 의 프로세스를 가지고 있고, 보통 데이터셋 구축과 베이스라인 코드가 주어지는 경우가 많습니다.</p><p>이번 P Stage의 경우 지난 U Stage를 통한 학습이 끝나자마자 시작되었기 때문에 이러한 프로세스를 익히는 차원에서 데이터만 먼저 공개되고 베이스라인은 1주일이 지나서야 공개가 되는 형태였습니다. 따라서 빠른 실험을 위해서는 데이터셋 구축 작업과 베이스라인이 절실했고 그것을 하기 위해서는 팀원들과의 협업 방식에 대한 상의를 하고 대회 전략을 수립해야 했습니다.</p><p>지금 생각해보면 무모했으나 저희 조의 목표는 <strong>무조건 1등!</strong> 이었고 그에 따라 2주간의 대회 기간 중 1주차는 베이스라인코드 작성과 데이터 전처리에 집중하고, 2주차에는 성능향상을 위한 실험들을 전략으로 잡았습니다.</p><p><br></p><h3 id="🤔-협업을-위한-채널-정하기"><a href="#🤔-협업을-위한-채널-정하기" class="headerlink" title="🤔 협업을 위한 채널 정하기"></a>🤔 협업을 위한 채널 정하기</h3><p>목표가 잡혔으니 이제는 협업을 어떻게 할 건지 결정해야 했습니다. </p><p>사전에 사용하던 채널로는 화상회의를 위한 줌, 공식적인 이야기를 위한 슬랙 채널, 그리고 코드관리를 위한 깃허브의 팀 Repo가 있었습니다. 하지만 이것만으로는 프로젝트 관리나 일정관리를 하기가 애매했어서, 새로운 채널을 만들 필요가 있었습니다. 해당 후보로는 Github Project의 대시보드, Jira, Notion 등이 있었는데 최종적으로는 Github Project를 사용하기로 결정했습니다. 칸반보드 기반의 Dashboard를 생성하면 TODO를 시각적으로 관리하기가 용이하고, Milestone이나 Label, Assignee 등 여러 장치들을 직관적으로 사용할 수 있기 때문입니다. 그리고 다른 템플릿 역시 다들 처음이지만, 사전학습 비용이 Github Project가 가장 적을 것 같았고, 채널이 다양해지면 다양할수록 팀원들의 혼란이 가중될 것 같다는 이유 또한 있었습니다.</p><p align="center">  <img src = "/image/image-20210903171740158.png"></p><p>저희는 위의 사진처럼, Github Project의 Kanban Dashboard를 사용했고, 매일 아침 9시 30분마다 당일 작업할 TODO 리스트를 업데이트하는 식으로 진행했습니다. 그리고 각각의 이름을 가진 라벨을 만들어서 누가 해당 TODO를 작업중인지 표시하도록 했습니다.</p><p>이렇게 서로의 할일과 작업 결과를 공유할 수 있는 채널을 만들어놓고 어떻게 사용할지에 대한 Flow를 미리 정해놨다는 것 자체가 저희같은 협업 초보들에게는 큰 성과이자 시작점이었습니다. 채널마다 역할과 책임을 명확히하고, 해당 채널을 어떻게 사용할건지 팀원들과의 상의가 지속해서 이루어질수록 더욱 긴밀한 협업이 진행될 수 있음을 알게 되었습니다.</p><p><br></p><h3 id="💻-코드관리와-Git-실패-🤮"><a href="#💻-코드관리와-Git-실패-🤮" class="headerlink" title="💻 코드관리와 Git (실패 🤮)"></a>💻 <del>코드관리와 Git</del> (실패 🤮)</h3><p>협업을 위한 채널 설정이 끝나고 마지막으로 코드관리를 어떻게 할 것인지 정해야했습니다. 1주차는 개인제출이 가능하다곤 하지만 결국에는 팀프로젝트이기 때문에 모두가 같은 베이스라인 코드를 가지고 중복된 실험을 하지 않고, 더 좋은 성능을 가진 코드가 나왔을 때 바로바로 버전업을 할 수 있도록 Git Flow를 따라서 협업을 진행하기로 결정했었습니다.</p><p>하지만 막상 대회 시작 후 베이스라인 코드가 너무 어렵고, 다른 할 것들이 많아서 정신이 없었으며 그리고 협업이 처음인 저희는 Git 자체가 너무나도 어려웠기 때문에 결국에는 Git Flow를 활용한 코드관리에 실패하였습니다.</p><p align="center">  <img src="/image/image-20210903173613783.png"></p><p>하지만 Git을 활용하지 않으면서 점점 대회의 끝이 다가올수록 서로의 코드가 점점 달라졌습니다. 결국에는 어떤 한 명이 Best Score를 달성했을 때 그 방법을 공유받고 다른 사람들이 그 모델을 같은 방식으로 학습시켰을 때 다른 모두가 재현할 수 없다는 문제가 생겼습니다. 그래서 지금 생각으로는 다시 대회 초반부터 진행하게 된다면 <strong>무조건 Git을 활용해서 버전관리에 신경쓰면서 작업을 하고 싶습니다.</strong> 성능을 올린 결과를 계속해서 베이스라인을 삼고 해당 코드에서 다른 방법들을 실험하는 쪽으로요. 다른 동료분이 성능을 올렸다고 했을 때 “저거 어떻게 했지..”라는 좌절스러운 생각도 많이 했거든요.. ㅎㅎ; 그렇게 한두명씩 뒤쳐지게 되면 추후에는 더 많은 실험을 할 수 없게 돼요.</p><p>또한 대회에서는 성능 개선이 우선시 되기 때문에 시간을 오래 투자해야하는 Git flow까지는 아니더라도 간단한 flow를 사전에 정하는 것이 좋다고 생각합니다. 실험하기도 바쁜데 커밋 메세지 신경쓰고 PR 템플릿 찾아보고 있으면 빡치잖아요. 🤮</p><p><br></p><h3 id="❤️‍🔥-핵심가치를-공유했다면-어땠을까"><a href="#❤️‍🔥-핵심가치를-공유했다면-어땠을까" class="headerlink" title="❤️‍🔥 핵심가치를 공유했다면 어땠을까?"></a>❤️‍🔥 핵심가치를 공유했다면 어땠을까?</h3><p>협업에 있어서 가장 중요한 것 중 하나는 핵심 가치를 공유하는 것입니다. 많은 실험이 이루어지고 코드 규모가 커질수록 정말 정신없어지는 상태에서 수많은 의사결정을 해야합니다. 그러면 자연스럽게 의사결정 비용이 높아질 수 밖에 없고, 논의를 위한 시간이 길어지면 길어질수록 피로도는 높아지고, 일관되지 않은 코드를 짜고, 공유해야 할 것을 공유하지 못하는 등 어떻게든 본래의 목적을 달성할 순 있어도 협력이란 관점에서 정말 좋지 않은 일이 일어날 수 있겠죠.</p><p>처음에 정했던 목적 달성을 위해 조금 귀찮았어도 여러 가치중에서 우선순위를 정했으면 어땠을까 하는 아쉬움이 남습니다. 사실 대회 끝물로 갈 수록 여러 의사결정이 필요할 때 결론을 짓지 못하고 흐지부지 끝나게 되는 등 첫 주 차때의 열정이 점점 사라지고 있었거든요. 감정과 시간의 소모를 줄이는 일, 그것을 해결하려고 하지 않았던 것이 아쉬움으로 남는 것 같습니다.</p><p>구성원의 만족, 빠른 실험, 가독성, 일관성, 단순성, 통제가능성, 학습가능성, 안정성 등 협업에 필요한 가치들을 리스트업하고 그것들의 우선순위를 매겼으면 더욱 행복한 협업이 되었을 수도 있겠다는 생각이 듭니다.</p><p><br></p><hr><p><br></p><h2 id="🔎-EDA와-Custom-Dataset-구축"><a href="#🔎-EDA와-Custom-Dataset-구축" class="headerlink" title="🔎 EDA와 Custom Dataset 구축"></a>🔎 EDA와 Custom Dataset 구축</h2><p><br></p><p align="center">  <em>데이터, 그게 뭔데?</em></p><p><br></p><blockquote><p>EDA의 중요성을 잊지 말고, 다양한 실험을 위해 Dataset을 다양하게 구성해보자.</p></blockquote><p><br></p><h3 id="🌄-EDA와-데이터-정제"><a href="#🌄-EDA와-데이터-정제" class="headerlink" title="🌄 EDA와 데이터 정제"></a>🌄 EDA와 데이터 정제</h3><p>대회를 시작했다면 가장 먼저 해야할 중요한 일을 하나만 꼽으라고 한다면 그것은 바로 <em>무조건</em> 데이터의 형태를 파악하는 것입니다. 데이터의 분포는 어떻게 되는지, 데이터에 노이즈는 없는지, 잘못 Labeling 된 데이터는 없는지 등을 판단하고, 그렇게 판단한 결과를 토대로 데이터를 정제해야 원활한 학습이 이루어질 수 있기 때문입니다.</p><p>저희는 데이터를 지속적으로 관찰하고 분석하는 다양한 전략을 구상하였습니다.</p><p><strong>1. 라벨 별 데이터가 어떻게 분포되어있는지 판단하자.</strong></p><p align="center">  <img src="/image/image-20210903184436458.png"></p><p>저희가 EDA 과정에서 가장 중요하게 생각한 것은 데이터가 Imbalancing 하지 않은지 확인하는 것이었습니다. 어느 한 쪽으로 데이터가 쏠려있으면 모델은 데이터가 적은 라벨을 쉽게 맞추지 못할 것이기 때문입니다.</p><p><strong>2. Evaluation Dataset의 분포는 어떻게 되어있는지 확인하자.</strong></p><p align="center">  <img src="/image/image-20210903184759275.png"></p><p>대회 1주차에는 하루에 개인별 10회씩 제출이 가능했습니다. 저희는 대회 초반엔 분명히 제출을 하지 못할 것이라 예상하고, 제출할 때 0부터 17까지 모든 라벨로만 Submission을 구성하여 제출했습니다. 이렇게 각 라벨별로 제출해보고 얻은 Accuracy 정보를 토대로 Evaluation 셋에 대한 정답 분포를 시각화할 수 있었습니다.</p><p>해당 방법은 이번 대회가 서버에 코드를 올려서 실행시키는 방식이 아닌, Eval Dataset을 주고 정답지만 제출하라는 방식의 대회였기 때문에 가능한 것이었습니다. 아래에서 또 이야기하겠지만, 이번 대회의 데이터셋으로는 과적합이 너무 쉽게 일어나고 Validation 데이터셋을 아무리 잘 나누어도 Validataion 관련 Score들을 신뢰할 수 없었습니다. 따라서 제출할 때 내가 도출한 정답이 어느정도의 Score일지 예상할 수가 없었습니다. 하지만 해당 방식으로 정답 분포를 알아낸 뒤 제출 전에 만든 정답지와 오차를 비교해보면 그 결과를 비슷하게 예측할 수 있었기 때문에 모델을 선택하고 제출할 정답을 고를 때 굉장히 좋은 판단 근거가 되었습니다.</p><p><strong>3. 모든 데이터를 찍어보자.</strong></p><p>제공받은 데이터는 사람마다 7장의 사진이 있고, 총 2700명의 사람이 있었습니다. 그렇게 총 18900개의 학습 데이터가 있었는데 저희는 팀원들끼리 분량을 나누어 위와 같이 모든 데이터를 확인하였습니다. 이 때 <code>Labeling이 잘못된 것은 없는지, 데이터는 어떻게 생겼는지, 학습 중에 노이즈가 있을만한 요소들이 있는지</code> 확인했습니다. 이 과정에서 남&lt;-&gt;녀가 뒤바뀌거나 마스크를 착용하지 않았는데 마스크를 착용하고 있다고 파일명이 되어있는 데이터들을 탐지할 수 있었습니다. 또한 안경을 썼거나, 마스크를 눈에다 착용하는 등 학습에 방해가 될 수 있다거나 기이한 데이터들을 확인했습니다. 이렇게 모든 데이터를 확인하면서 학습 전략을 어떻게 취하거나 데이터 정제는 어떻게 하는 것이 좋을지 등 여러 실험 아이디어들을 얻을 수 있었습니다.</p><p>이와 더불어 실제 학습을 거쳐 모델을 생성한 뒤에 진행한 시각화로는 Confusion Matrix와 Wandb, Tensorboard 활용 등이 있는데 이것은 아래에서 이어서 말씀드리겠습니다.</p><p><br></p><h3 id="🔧-Custom-Dataset-구축하기"><a href="#🔧-Custom-Dataset-구축하기" class="headerlink" title="🔧 Custom Dataset 구축하기"></a>🔧 Custom Dataset 구축하기</h3><p>데이터를 확인했으니 Dataset을 구축하는 일을 진행해야 했습니다. 거창한 것이 아니라 데이터가 있는 경로의 폴더구조라던지, 이미지 파일의 확장자 등을 판단하고 데이터를 쉽게 불러올 수 있도록 csv파일을 만드는 것까지로 결정했습니다.</p><p>이 때 중요하게 생각했던 요소들은 Labeling을 어떻게 해야 할 지와 Validation 데이터셋을 만들어야 할 지에 대한 것이었습니다. 해당 내용을 가지고 많은 상의를 했으며 결론적으로는</p><ul><li>통합라벨, Mask 착용여부, 성별, 나이를 포함하여 4개의 라벨을 갖도록 하자.</li><li>사람을 기준으로 Stratify 하게 나누어서 10%, 15%, 20% 비율로 split된 Validation Dataset을 만들자.</li></ul><p>위와 같은 결과를 얻게 되었습니다.</p><p>저희가 분류해야 하는 라벨 개수는 총 18개입니다. 단일 모델로 18개의 라벨을 분류하라고 하면 성능이 굉장히 안좋게 나올 가능성이 크죠. 따라서 저희는 Mask 착용여부와 성별, 나이를 각각 분류하는 세 개의 모델을 학습시킨다음에 결과를 합치자는 쪽으로 전략이 기울었고, 그에따라 통합라벨을 포함한 세 개의 라벨을 추가한 csv 파일을 만들게 되었습니다.</p><p>그리고 Validation Dataset의 경우 동료 캠퍼분이신 서광채님의 토론글을 참고하여 사람을 기준으로 Stratify 한 데이터셋을 구축하였습니다. 빠르게 베이스라인을 만들고 먼저 실험을 진행하셨던 분들의 이야기를 들어보니 과적합 이슈와 Validation Score를 신뢰할 수 없다는 이야기를 듣게 되었습니다. 그렇기 때문에 Validation에는 처음보는 사람들이 들어갈 수 있도록 데이터셋을 분리하는 것이 좋겠다고 판단했으며 추후에 최종 제출을 할 때에도 판단의 도움이 될 것이라고 생각했습니다.</p><p>하지만 생각해볼 것은 어떤 특정 라벨에 대해서는 데이터가 굉장히 적게 분포되어 있는데, <strong>적게 분포된 데이터가 Validation Set에 포함되어 있을 가능성</strong>에 대한 것입니다. 실제로 위의 방법을 사용했을 때 Validation Score는 조금이나마 신뢰할 수 있게 되었지만, 가뜩이나 적은 데이터가 학습에 사용되지 않았고, 대회가 끝나고 나서야 알게 되었습니다. 그것을 사전에 파악하지 못한 것이 조금의 성능향상을 방해했다고 생각합니다.</p><p>즉, Dataset 을 어떻게 구축하냐도 성능에 많은 영향을 미치기 때문에, 사전에 많은 상의를 통해서 여러 데이터셋을 구축하고 이후에는 많은 실험을 통해 택1을 하는 것이 가장 좋은 전략인 것 같습니다.</p><p><br></p><hr><p><br></p><h2 id="🎮-Baseline-구축과-적용"><a href="#🎮-Baseline-구축과-적용" class="headerlink" title="🎮 Baseline 구축과 적용"></a>🎮 Baseline 구축과 적용</h2><p><br></p><p align="center">  <em>이 코드 어떻게 실행시키나요?</em></p><p><br></p><blockquote><ol><li>팀원들의 실력을 고려하여 베이스라인으로 삼을 템플릿 혹은 코드를 사전에 정해보자.</li><li>각자의 환경을 고려하여 베이스라인을 작성하자. (데이터 경로나 패키지 버전 등을 꼭 신경쓸 것!)</li></ol></blockquote><p><br></p><p>저희는 U Stage에서 배웠던 <a href="https://github.com/victoresque/pytorch-template">pytorch-template</a> 을 이용하여 베이스라인 코드를 작성해보기로 사전에 협의를 했습니다. 왜냐하면 모듈화가 너무 잘 되어있고, 좋은 코드로 실험을 해야 좋은 모델이 나올것이라는 나름의 기대감이 있었기 때문입니다. (ㅎㅎ;)</p><p>하지만 결과적으로 성공이라고만은 말할 수 없을 것 같은데요, 왜냐하면 깃으로만도 머리아파 죽겠는데 여기에 어려운 코드까지 써야한다?? 네. 베이스라인 코드를 작성하고 이해하는데 너무 많은 시간이 걸렸습니다. 대회 기준 4일차에 베이스라인 코드가 완성되었고, 다음날에는 기존에 제공하기로 되어있던 베이스라인 코드가 공개되는 날이었어요. 그리고 해당 코드를 환경이 모두 다른 상황에서 팀원들이 일괄적으로 적용하는 것에도 많은 시간이 들었습니다.</p><p>빠르게 베이스라인을 작성하고 빠르게 실험을 이것저것 해보자는 계획을 실천하지 못한것에 꽤 아쉬움이 남네요. 그리고 지금 생각해보면 데드라인이 존재하는 대회의 경우에는 파이토치 템플릿처럼 유지보수와 기능 추가에 많은 비용이 드는 코드가 적절한지 잘 모르겠습니다. 차라리 주피터노트북에서 실행 가능한 코드들로 파이썬 스크립트를 만들었으면 어땠을까 하는 생각이 많이 들어요.</p><p>그렇다면, 지금부터 저희가 어떻게 베이스라인을 작성했고 파이토치 템플릿에 익숙해졌는지 말씀드려보겠습니다.</p><p><br></p><h3 id="🥷-닌자-프로젝트"><a href="#🥷-닌자-프로젝트" class="headerlink" title="🥷 닌자 프로젝트"></a>🥷 닌자 프로젝트</h3><p>모던 자바스크립트 튜토리얼에는 <code>Ninza Code</code> 에 대한 이야기가 있습니다. 닌자라고 불리던 전설 속 개발자들이 사용했던 코드에 대한 이야긴데요. 코드는 짧게, 변수명과 함수명은 한글자만, 약어를 쓰고 동의어쓰고 최대한 헷갈리게 형용사도 많이쓰고 외부변수를 덮어쓰는 굉장히 멋있는(?) 방법의 코드 이야기에요. 개발자라면 분명 지양해야 하는 습관들이지만 파이토치 템플릿을 쓰기로 결정한 이상, 전설속의 닌자처럼 코드를 한 번 망쳐보자는 취지에서 Fashion-MNIST를 파이토치 템플릿으로 Fine Tuning 해보자! 라는 과제를 저희끼리 수행했어요. 그렇게 파이토치 템플릿을 한 번 망쳐보고 난 뒤에야 비로소 베이스라인을 작성하기 위한 최소한의 준비가 되었습니다.</p><p><br></p><h3 id="✏️-Efficient-Baseline"><a href="#✏️-Efficient-Baseline" class="headerlink" title="✏️ Efficient Baseline"></a>✏️ Efficient Baseline</h3><p>그렇게 닌자프로젝트를 진행한 이후에는 파이토치 템플릿으로 모든 코드를 작성할 수 있었습니다. 우선은 이전에 만들어두었던 csv 파일을 활용하여 CustomDataset과 CustomDataLoader를 우선적으로 만들고 config 파일에서 받아올 수 있는 인자들과 모듈들을 커스텀했습니다. 파이토치 템플릿 자체가 워낙 간단한 config 파일 수정만으로 모든 학습이 이루어질 수 있는 구조로 되어있다는 점을 인지하여 여러 가능성을 포괄한 코드를 만들기 위해 노력했어요.</p><p>베이스라인 코드를 작성할 때 아래의 것들을 원칙으로 삼아 구현했습니다.</p><ul><li>config 파일 수정만으로 분리해놓았던 각각의 라벨을 학습할 수 있게 하자.</li><li>config 파일 수정만으로 pretrained 모델을 바꿀 수 있게 하자.</li><li>config 파일 수정만으로 Augmentation의 값들을 변경할 수 있게 하자.</li><li>config 파일 수정만으로 학습할 수 있는 데이터를 변경할 수 있게 하자.</li><li>config 파일 수정만으로 여러 모델을 함께 학습할 수 있게 하자.</li><li>config 파일 수정만으로 Evaluation 까지 동작할 수 있게 하자.</li></ul><p>마침내 위의 원칙들을 적용한 베이스라인 코드를 만들게 되었고, 여러 기능이 추가된 탓에 복잡도는 올라갔고 시간이 오래 걸렸지만 좋은 코드를 입맛대로 변경하여 대회에 최적화된 코드를 만들 수 있었다는 것에 큰 즐거움을 얻었습니다.</p><p>하지만…</p><h3 id="😂-코드-어떻게-실행해요-ㅜㅜ"><a href="#😂-코드-어떻게-실행해요-ㅜㅜ" class="headerlink" title="😂 코드 어떻게 실행해요? ㅜㅜ"></a>😂 코드 어떻게 실행해요? ㅜㅜ</h3><p>가장 큰 문제가 발생했습니다. 파이토치 템플릿으로 멋스러운 베이스라인 코드를 만들었지만, 서로의 환경이 다름을 고려하지 못했고 그로인해 편하게 하려고 만들었던 몇몇 기능들이 다른 팀원분들의 환경에서 동작하지 않는 문제가 있었습니다. 특히 데이터는 용량문제로 깃에 올리지 못하기 때문에 저장된 경로가 모두 달랐는데, 경로 관련된 문제를 해결하는 것이 가장 힘들었던 것 같습니다. 또한 잘못된 Label을 수정하지 않은 팀원분의 경우 이미지를 불러올 수 없다거나, python이나 필요한 package들의 버전이 달라서 실행이 안된다거나 $PATH 와 같은 Alias를 수정하여 아예 필요한 패키지가 호출이 안된다거나 하는 기괴한 에러들이 수도없이 발생했습니다. 결국에는 어떻게든 해결하긴 했지만, 서로의 환경을 고려하여 패키지 버전을 Fix하는 등의 방식을 적용하지 못했다는 것에서 추가적인 비용이 많이 들었습니다.</p><p>이러한 이유로 1주차 때는 많은 실험을 못해봤고 이것이 더 성능을 올리지 못한 원인 중 하나가 될 것 같습니다. 다음부터는 Dot ENV 등을 활용한다거나 애초에 모두가 동일한 가상환경을 세팅하고 시작한다거나 하는 방식을 사용하는 것이 좋을 것 같아요. 정말 이 부분에서 많은 것을 배운 것 같습니다.</p><p><br></p><h3 id="🥐-예상치-못한-파이토치-템플릿의-단점"><a href="#🥐-예상치-못한-파이토치-템플릿의-단점" class="headerlink" title="🥐 예상치 못한 파이토치 템플릿의 단점"></a>🥐 예상치 못한 파이토치 템플릿의 단점</h3><p>어찌저찌 모두가 파이토치 템플릿을 사용하긴 했지만.. 예상치 못한 문제가 발생하기도 했습니다.</p><p>시간이 지날수록 여러가지 실험을 하고, 유용한 라이브러리를 사용해보는 등 여러 시도를 많이 하게 되는데요. 이 때 파이토치 템플릿으로 이루어진 코드들은 구조를 완전히 꿰고 있지 않으면 <code>이 코드 어디에다가 붙여넣어야 되지..?</code> 처럼 어디에 기능을 추가해야 될지 혼란스러워 지더라구요. 특히 대회에서는 빠른 실험을 통해 성능 향상이 보이면 해당 결과를 빠르게 공유하고 그 실험에 집중하는 것이 중요한데 빠르게 유용한 코드들을 손쉽게 적용할 수 없었어요.</p><p>또 하나, 파이토치 템플릿은 학습을 시킬 때 모델에 config 파일 역시 함께 저장을 시키는데 그렇기 때문에 모델을 불러올 때 config 파일이 없으면 load가 되지 않는다는 치명적인 단점이 존재해요. 이말은 즉슨, 이미 공유된 튜토리얼 코드라도 템플릿에 적용을 하거나 config 파일을 추가하는 수정비용이 들어간다는 뜻이겠죠.</p><p>고로 이런 부분들까지 생각을 해본다면, 원래 파이토치 템플릿에 익숙하지 않은 이상 다른 베이스를 찾아보는 게 더 효율적일 것 같다고 느끼게 되었습니다.</p><p><br></p><hr><p><br></p><h2 id="👨‍🔬-모델링과-실험"><a href="#👨‍🔬-모델링과-실험" class="headerlink" title="👨‍🔬 모델링과 실험"></a>👨‍🔬 모델링과 실험</h2><p><br></p><p align="center">  <em>이렇게 하면 성능 오르는거 맞아?</em></p><p><br></p><blockquote><p>서로가 무엇을 하고 있는지, 무엇을 해봤는지 <strong><em>모두가</em></strong> 자세하게 알고 있고,<br>어떤 것을 추가해봤을 때 오히려 성능이 떨어졌다는 것을 <strong><em>모두가</em></strong> 알고 있어야 하며,<br>성능이 올랐을 때 해당 성능을 재현할 수 있는 코드를 <strong><em>모두가</em></strong> 가지고 있어야 한다.</p></blockquote><p><br></p><h3 id="✂️-정말-많은-실험을-시도함-근데-실험-일지는"><a href="#✂️-정말-많은-실험을-시도함-근데-실험-일지는" class="headerlink" title="✂️ 정말 많은 실험을 시도함. 근데 실험 일지는?"></a>✂️ 정말 많은 실험을 시도함. 근데 실험 일지는?</h3><p>간신히 베이스라인을 완성한 이후 저희는 정말 많은 실험을 해보았습니다.</p><p>Hard하게 Augmentation을 적용한 이미지를 데이터셋에 추가하기도 하고, albumentations 에 있는 모든 기능들을 정리하고 실험해 보았으며, ResNet, EfficientNet, RegNext, RegNet, MLP-Mixer, ViT 등 정말 많은 사전학습 모델을 사용해보고 Optimizer와 Loss를 계속 바꿔도 보고, CutMix, MixUp, AugMix와 같은 augmentation 기법들은 물론 TTA, Label Smoothing, Pseudo-Labeling 등 Data Imbalancing을 해결하는 방법들까지 사용해봤습니다.</p><p>시도해볼 수 있는 코드를 모두가 개인의 노력만으로 작성해서 실험해본 것도 너무 좋고, 프로그래밍 스킬이 늘어나고 파이토치 템플릿에도 익숙해지는 것은 정말 좋았으나, 결과적으론 위의 것들을 모두 시도해봤을 때 전체적인 성능 향상이 이루어지지 않았습니다.</p><p>그 이유는 체계적인 실험일지를 관리하지 않았던 것에서 찾을 수 있을 것 같습니다. 수많은 기법과 수많은 하이퍼파라미터들의 조합 중에서 최적을 찾는 것. 그것이 성능을 끌어올릴 수 있는 방법이지만 해당 실험들의 결과를 철저하게 기록하지 않고, 실험 순서를 정리하지 않았기 때문에 모두가 다른 조합들로 시도했을 텐데, 그 결과들이 모두 수집되지 않아서 체계적인 실험을 할 수가 없었습니다.</p><p>또한 실험을 통해서 성능이 오르는 것을 찾는 것도 중요하지만, 성능이 떨어지는 요인을 찾는 것도 그만큼 중요한데요. 왜냐하면 성능이 향상된 모델에서 해당 요인을 적용하지 않을 수 있기 때문입니다. 이렇듯 기록의 부재로 저희는 수많은 조합들을 매번 새롭게 찾아야만 했습니다. 다시 생각해보니 이것도 정말 아쉽습니다 ㅎㅎ.</p><p><br></p><h3 id="🍟-공유에-진심이어야-한다"><a href="#🍟-공유에-진심이어야-한다" class="headerlink" title="🍟 공유에 진심이어야 한다."></a>🍟 공유에 진심이어야 한다.</h3><p>또 한가지, 여러 이유가 있겠지만 그 중에서 ‘공유’에 관해서 이야기하고 싶어요.</p><p>저희는 Github Project 를 통해 서로의 TODO와 일정을 관리하고 있었지만, 시간이 지날 수록 점차 관리에 소홀해졌고 조금의 성능 향상이 있었을 때 그 방법을 올바르게 전파하지 못한 것 같습니다. 너무 빡센 일정에 다들 지친 문제도 있었지만, 베이스라인의 스노우볼이 여기까지 도달했다는 생각도 들었습니다. 사실 제가 안했다는게 가장 크지만요… (팀원분들 정말 죄송합니다.) 당연한 이야기겠지만 <strong><em>저는 이렇게 했더니 성능이 올랐어요~</em></strong>, <strong><em>저 이거 실험해봤는데 별로였어요.</em></strong> 정도의 공유만으로는 팀 전체적으로 다양한 실험을 통해 성능을 이끌어 낼 수가 없었습니다.</p><p>지금 와서 돌이켜보면 왜 그랬지 싶은데, 저를 포함한 팀원 모두가 그만큼 순위에 집착했었고 다른 것을 신경쓸 여유가 없었던 것도 같아요. 하지만 정말 공유의 중요성을 뼈저리게 알 수 있었습니다.</p><p><br></p><p><strong>그럼에도 종료 전날에 기적적으로 성능을 향상시켰으며 그때부터 조금이라도 성능을 올리기 위해 모두가 힘을 합쳐 앙상블과 TTA에 전념했고 종료 1초전까지 성능을 올릴 수 있었습니다.</strong></p><p><br></p><h3 id="🥉-최종-모델-개요"><a href="#🥉-최종-모델-개요" class="headerlink" title="🥉 최종 모델 개요"></a>🥉 최종 모델 개요</h3><p><img src="https://user-images.githubusercontent.com/49181231/132098313-6f9e7f2e-376c-44c8-a57d-e56cffd10168.png" alt="아이콘 제작자 Freepik from [www.flaticon.com](http://www.flaticon.com/)"></p><p><br></p><p>최종적으로 사용한 모델은 아래 모델들이 결합되어 있는 구조입니다.</p><ul><li>성별을 분류하는 regnety_006</li><li>마스크를 착용한 사람들로만 학습시킨 efficientnet_b1</li><li>마스크를 잘못 착용한 사람들로만 학습시킨 efficientnet_b1</li><li>마스크를 착용하지 않은 사람들로만 학습시킨 efficientnet_b1</li><li>마스크 착용 여부를 분류하는 regnety_006</li></ul><p>처음 사전학습된 모델을 단일구성하여 18개의 라벨을 한 번에 분류하게 했을 때는 약 20%의 정확도와 0.18 정도의 f1 스코어를 기록했습니다. 성능을 향상시키기 위한 여러 방법을 고안하던 중, <strong>3개의 Task로 나누어서 세 개의 모델이 각각 Gender, Age, Mask 착용여부를 분류</strong>하게 한 뒤 나온 결과를 조합하여 최종 라벨을 도출하게 하였을 때 약 70%의 정확도와 0.69 정도의 f1 스코어를 기록할 수 있었습니다.</p><p>위의 실험을 토대로 각각의 분류 모델을 활용하게 되었고, 여러 추가적인 실험을 바탕으로 Age 모델의 예측 결과가 좋지 않다는 것을 파악하게 되었습니다. 그래서 Age 모델도 <em>(마스크를 착용한 사람으로만 학습한 모델, 마스크를 잘못 착용한 사람으로만 학습한 모델, 마스크를 착용하지 않은 사람으로만 학습한 모델)</em> 로 분리하였고 결과적으로 나이 예측 성능을 향상시킬 수 있었습니다다.</p><p><br></p><h3 id="🫂-앙상블"><a href="#🫂-앙상블" class="headerlink" title="🫂 앙상블"></a>🫂 앙상블</h3><p align="center">  <img src="/image/image-20210903215626128.png"></p><p align="center">  <em>너 아이큐 150, 내 아이큐 150, 합치면 300!</em></p><p>하지만 이렇게 총 5개의 모델을 활용했을 때 ‘성별-나이’ 혹은 ‘마스크-성별’ 등 연관성이 존재할 수 있는 정보를 활용하지 못하고 독립적인 Feature로만 정답을 도출하게 되는데요. 따라서 의존적인 정보를 추가하고자 마스크 착용여부로 분리한 <strong>각각의 Age 모델들을 앙상블</strong>하였고 <strong>이후 Gender와 Mask 모델까지 통합하여 Multi Label Classification을 수행</strong>하게 하였습니다. 이렇게 0.75의 f1 스코어를 기록하였고 이후  <strong>최적의 TTA를 적용</strong>하고 제출 결과들을 <strong>Hard Voting</strong> 하여 최종적으로 0.766의 f1 스코어를 기록할 수 있었습니다.</p><p><br></p><hr><p><br></p><h2 id="💯-굿-프랙티스"><a href="#💯-굿-프랙티스" class="headerlink" title="💯 굿 프랙티스"></a>💯 굿 프랙티스</h2><p><br></p><p align="center">  <em>결과적으로 옳았던 선택들</em></p><p><br></p><blockquote><p>시각화와 로그는 무조건 옳다!</p></blockquote><p><br></p><h3 id="🎉-Tensorboard에-Confusion-Matrix-시각화"><a href="#🎉-Tensorboard에-Confusion-Matrix-시각화" class="headerlink" title="🎉 Tensorboard에 Confusion Matrix 시각화"></a>🎉 Tensorboard에 Confusion Matrix 시각화</h3><p align="center">  <img src="/image/image-20210903185413325.png"></p><h3 id="🎉-Wandb-사용"><a href="#🎉-Wandb-사용" class="headerlink" title="🎉 Wandb 사용"></a>🎉 Wandb 사용</h3><p align="center">  <img src="/image/image-20210903224929293.png"></p><h3 id="🎉-Evaluation에-대한-클래스별-오차-비율-시각화"><a href="#🎉-Evaluation에-대한-클래스별-오차-비율-시각화" class="headerlink" title="🎉 Evaluation에 대한 클래스별 오차 비율 시각화"></a>🎉 Evaluation에 대한 클래스별 오차 비율 시각화</h3><p align="center">  <img src="/image/image-20210903225031759.png"></p><h3 id="🎉-TTA-실험일지-작성"><a href="#🎉-TTA-실험일지-작성" class="headerlink" title="🎉 TTA 실험일지 작성"></a>🎉 TTA 실험일지 작성</h3><p align="center">  <img src="https://user-images.githubusercontent.com/49181231/132093978-4f1efd70-2d0a-49bf-839c-12becf4b3ee1.png"></p><p>지금 생각해도 저희가 잘 한 것은 바로 모든 것에 로그를 남겼고, 시각화 툴을 잘 이용했다는 것입니다. Wandb와 Tensorboard를 적극적으로 활용하였으며, 데이터 분석 단계에서 0~17로만 채워서 제출한 결과를 통해 Evaluation 데이터의 정답 분포를 알아냈고 저희가 제출할 정답과 클래스별 오차비율을 계산해냈습니다. 그 결과 제출 전에 나름 신뢰할 수 있는 근거들을 잘 만들어놓았다는 것이 미흡한 협업에도 좋은 결과를 이끌어낼 수 있었던 전략이라고 생각합니다.</p><p>너무나도 쉽게 과적합이 일어나는 이번 대회에서 최소한의 제출 모델 선택 전략은 필수적이며, 아무리 Validation 데이터셋을 유의미하게 나누었다고 해도 웬만하면 90% 이상의 정확도를 기록했기 때문에 이러한 시각화 자료들이 꽤 중요한 모델 선택의 지표로 작용할 수 있습니다.</p><p>또한 최종 모델에서 TTA를 적용할 때 augmentation 기법들에 대한 조합별로 스코어를 기록했으며 최적의 augmentation을 찾아냈다는 것이 성능향상에 유의미하게 작용하였습니다. 저희는 TTA를 적용했을 때 최대 0.746 -&gt; 0.763 까지 0.017 정도의 f1 스코어 향상을 보았고, 이 부분만큼은 체계적인 실험이 이루어졌기에 많은 도움을 받을 수 있었습니다.</p><p><br></p><hr><p><br></p><h2 id="마지막으로"><a href="#마지막으로" class="headerlink" title="마지막으로"></a>마지막으로</h2><p>돌이켜봤을 때 잘한 것보다 아쉬운 것들이 더 많이 남는 대회였던 것 같습니다. 비록 대회 전에 세웠던 1등이라는 목표를 달성하진 못했지만, 그럼에도 이후엔 어떤 식으로 협업을 하는 것이 좋을지 고민해보고 배울 수 있었던 경험이었던 것 같습니다.</p><p>너무 좋은 팀원분들과 함께할 수 있어서 정말 좋았고, 이번 경험을 통해서 저를 비롯한 다른 분들도 이후 대회에서는 모두 원하는 목표 충분히 이루실 수 있을 것이라 생각합니다.</p><p>저희의 이야기가 조금이라도 도움이 되었길 바라면서 마치겠습니다.</p><p>긴 글 읽어주셔서 정말 감사합니다!</p><p><br></p><hr><p><br></p><h2 id="🤜-부록-팀원들의-한마디"><a href="#🤜-부록-팀원들의-한마디" class="headerlink" title="🤜 부록: 팀원들의 한마디"></a>🤜 부록: 팀원들의 한마디</h2><blockquote><p>요한: 실험 일지와 코드를 체계적으로 관리하지 못한 것이 가장 아쉽습니다. 서버가 5킬을 당한게 좀 컸던 것 같아요. 그래도 너무 좋은 팀원들과 미흡하지만 재밌는 대회 함께 진행할 수 있어서 너무 좋았습니다!</p><p>지민: 모두 항상 열심히 하시는 모습이 자극이 되어서 같이 완주할 수 있었던 것 같습니다. 특히 요한님 베이스라인 부터 모르는 부분 질문까지 친절하게 답해주셔서 감사했습니다!!</p><p>하겸: 아쉬운 점 투성이지만 성장통이라고 생각합니다.</p><p>다영: 첫 대회라 곱씹을수록 아쉬운 점이 많지만 좋은 팀원들을 만나 끝맺음이 좋았습니다 ! 덕분에 많이 배웠고, 자극도 많이 받았습니다. 지속 가능한 성장을 위해 파이팅 !!</p><p>민지: 잘한점이 너무 많지만!! 지나고 보니 아쉬움 투성이인 것 같습니다. 특히 리더보드 스코어 향상을 목표로 하신다면 다른 팀원의 코드로 빠르게 업데이트하고, 실험을 잘 기록해두는 것이 중요하다고 느꼈습니다. 지식 뿐 아니라 협업 과정 측면에서 배운 점이 많은 대회였습니다!</p><p>아경: 멋있는 분들과 함께해서 좋았습니다. 되돌아보면 아쉬운 점도 많았지만 그만큼 배운 점도 많은 것 같습니다. 이번 대회를 시작으로 더 발전하길! 화이팅 🙂</p><p>준영: 처음하는 DL, 처음하는 Competition, 처음 써보는 pytorch. 모든 것이 어색했지만, 좋은 팀원과 함께 해서 영광이었습니다. 항상 열심히 하시는 모든 팀원분들께 많은 자극을 받았습니다. 감사했습니다.</p><p>✨빛예닮 멘토님✨: Boostcamp에서 스스로를 정말 잘 Boost시킨 팀이라고 생각합니다! 혼자 앞서가기보다 모두가 함께 가는 것을 택했고, 오늘보다 내일을 더 기대되게 했던 우리 15조! 부족한 멘토와 함께였지만 여러분들 덕분에 우리 조가 더 빛난 것 같아요! 언제나 응원합니다! 감사합니다!</p></blockquote><p><br><br><br></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Programming/">Programming</category>
      
      <category domain="https://l-yohai.github.io/categories/Programming/Tips/">Tips</category>
      
      
      <category domain="https://l-yohai.github.io/tags/AI-%EB%8C%80%ED%9A%8C-%ED%98%91%EC%97%85-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/">AI, 대회, 협업, 플래닝가이드</category>
      
      
      <comments>https://l-yohai.github.io/AI-Competition%EC%9D%98-%ED%98%91%EC%97%85%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech Pstage1 회고 (2021-08-23 ~ 2021-09-04)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-Pstage1-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-09-04/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-Pstage1-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-09-04/</guid>
      <pubDate>Fri, 03 Sep 2021 15:35:38 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;🔥-Final-Score&quot;&gt;&lt;a href=&quot;#🔥-Final-Score&quot; class=&quot;headerlink&quot; title=&quot;🔥 Final Score&quot;&gt;&lt;/a&gt;🔥 Final Score&lt;/h2&gt;&lt;h3 id=&quot;🥉Public-Leaderboard&quot;&gt;&lt;a href=&quot;#🥉Public-Leaderboard&quot; class=&quot;headerlink&quot; title=&quot;🥉Public Leaderboard&quot;&gt;&lt;/a&gt;🥉Public Leaderboard&lt;/h3&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="🔥-Final-Score"><a href="#🔥-Final-Score" class="headerlink" title="🔥 Final Score"></a>🔥 Final Score</h2><h3 id="🥉Public-Leaderboard"><a href="#🥉Public-Leaderboard" class="headerlink" title="🥉Public Leaderboard"></a>🥉Public Leaderboard</h3><ul><li>Acc: 81.508</li><li>F1 (macro): 0.766 (11등)</li></ul><h3 id="🥉Private-Leaderboard"><a href="#🥉Private-Leaderboard" class="headerlink" title="🥉Private Leaderboard"></a>🥉Private Leaderboard</h3><ul><li>Acc: 80.746</li><li>F1 (macro): 0.754 (13등 - 2위 하락)</li></ul><hr><h2 id="📝-Task-Description"><a href="#📝-Task-Description" class="headerlink" title="📝 Task Description"></a>📝 Task Description</h2><p>COVID-19의 확산으로 우리나라는 물론 전 세계 사람들은 경제적, 생산적인 활동에 많은 제약을 가지게 되었습니다. 우리나라는 COVID-19 확산 방지를 위해 사회적 거리 두기를 단계적으로 시행하는 등의 많은 노력을 하고 있습니다. 과거 높은 사망률을 가진 사스(SARS)나 에볼라(Ebola)와는 달리 COVID-19의 치사율은 오히려 비교적 낮은 편에 속합니다. 그럼에도 불구하고, 이렇게 오랜 기간 동안 우리를 괴롭히고 있는 근본적인 이유는 바로 COVID-19의 강력한 전염력 때문입니다.</p><p>감염자의 입, 호흡기로부터 나오는 비말, 침 등으로 인해 다른 사람에게 쉽게 전파가 될 수 있기 때문에 감염 확산 방지를 위해 무엇보다 중요한 것은 모든 사람이 마스크로 코와 입을 가려서 혹시 모를 감염자로부터의 전파 경로를 원천 차단하는 것입니다. 이를 위해 공공 장소에 있는 사람들은 반드시 마스크를 착용해야 할 필요가 있으며, 무엇 보다도 코와 입을 완전히 가릴 수 있도록 올바르게 착용하는 것이 중요합니다. 하지만 넓은 공공장소에서 모든 사람들의 올바른 마스크 착용 상태를 검사하기 위해서는 추가적인 인적자원이 필요할 것입니다.</p><p>따라서, 우리는 카메라로 비춰진 사람 얼굴 이미지 만으로 이 사람이 마스크를 쓰고 있는지, 쓰지 않았는지, 정확히 쓴 것이 맞는지 자동으로 가려낼 수 있는 시스템이 필요합니다. 이 시스템이 공공장소 입구에 갖춰져 있다면 적은 인적자원으로도 충분히 검사가 가능할 것입니다.</p><hr><h2 id="😷-Dataset"><a href="#😷-Dataset" class="headerlink" title="😷 Dataset"></a>😷 Dataset</h2><p>마스크를 착용하는 건 COIVD-19의 확산을 방지하는데 중요한 역할을 합니다. 제공되는 이 데이터셋은 사람이 마스크를 착용하였는지 판별하는 모델을 학습할 수 있게 해줍니다. 모든 데이터셋은 아시아인 남녀로 구성되어 있고 나이는 20대부터 70대까지 다양하게 분포하고 있습니다. 간략한 통계는 다음과 같습니다.</p><ul><li>전체 사람 명 수 : 4,500</li><li>한 사람당 사진의 개수: 7 [마스크 착용 5장, 이상하게 착용(코스크, 턱스크) 1장, 미착용 1장]</li><li>이미지 크기: (384, 512)</li></ul><hr><h2 id="🔎-Class-Description"><a href="#🔎-Class-Description" class="headerlink" title="🔎 Class Description"></a>🔎 Class Description</h2><p><img src="/image/image-20210903122042492.png" alt=""></p><p align="center">  아이콘 제작자 <a href="https://www.freepik.com" title="Freepik">Freepik</a> from <a href="https://www.flaticon.com/kr/" title="Flaticon">www.flaticon.com</a></p><p><br><br><br></p><div class="table-container"><table><thead><tr><th style="text-align:center">Label</th><th style="text-align:center">Mask</th><th style="text-align:center">Gender</th><th style="text-align:center">Age</th></tr></thead><tbody><tr><td style="text-align:center">0</td><td style="text-align:center">Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">1</td><td style="text-align:center">Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">2</td><td style="text-align:center">Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 60</td></tr><tr><td style="text-align:center">3</td><td style="text-align:center">Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">4</td><td style="text-align:center">Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">5</td><td style="text-align:center">Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 60</td></tr><tr><td style="text-align:center">6</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Male</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">7</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">8</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 60</td></tr><tr><td style="text-align:center">9</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Female</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">10</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">11</td><td style="text-align:center">Incorrect</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 60</td></tr><tr><td style="text-align:center">12</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">13</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">14</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Male</td><td style="text-align:center">&gt;= 60</td></tr><tr><td style="text-align:center">15</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&lt; 30</td></tr><tr><td style="text-align:center">16</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 30 and &lt; 60</td></tr><tr><td style="text-align:center">17</td><td style="text-align:center">Not Wear</td><td style="text-align:center">Female</td><td style="text-align:center">&gt;= 60</td></tr></tbody></table></div><h2 id="회고록"><a href="#회고록" class="headerlink" title="회고록"></a>회고록</h2><p>Naver Connect 재단에서 운영하는 Boostcamp AI Tech의 첫 번째 대회인 마스크 착용 분류 대회에 참여하게 되었다. 나는 이번이 세 번째 대회 참가인데, 지난 두 번의 대회에서는 그저 참여에만 의의를 두었었기 때문에 이번 대회에서는 그간 공부하고 성장한 것을 조금이나마 입증하는 겸, 무조건 순위권에 들어야겠다는 목표를 가지고 대회에 임하게 되었다.</p><p>결과적으로는 38팀 중 13위에 그쳤고, 처음 목표에 비해서는 너무나도 아쉬운 결과지만 지난 두 번의 대회에 비하면 매우 주도적으로 대회에 임했고 많은 코드를 작성하고 실험을 하며 어느정도 인공지능 대회라는 프로세스에 익숙해질 수 있었다는 것에서 나름 만족스러운 대회였던 것 같다.</p><h2 id="Timetable-21-08-23-21-09-02"><a href="#Timetable-21-08-23-21-09-02" class="headerlink" title="Timetable (21.08.23 ~ 21.09.02)"></a>Timetable (21.08.23 ~ 21.09.02)</h2><ul><li>08.23 ~ 08.24: EDA 및 Data Labeling</li><li>08.25 ~ 08.26: Baseline 코드 작성</li><li>08.27 ~ 08.30: Augmentation 적용 및 실험</li><li>08.31 ~ 09.01: 개별모델 분리 및 앙상블</li><li>09.02: 외부데이터셋 추가 및 최종제출</li></ul><h2 id="대회-전략"><a href="#대회-전략" class="headerlink" title="대회 전략"></a>대회 전략</h2><p>이번 대회는 2주동안 팀 단위로 진행되었는데, 1주차는 개인제출, 2주차는 팀제출로 진행되었다. </p><p>팀원들이 모두들 순위에 욕심이 있었기 때문에 1주차에는 데이터 EDA에 집중하며 노이즈나 Labeling이 잘못된 데이터를 최대한 발견하고 수정하는 쪽으로 진행했으며, 2주차에는 최대한 성능을 끌어올리는 식으로 계획을 가져갔다.</p><p>하지만 깃과 협업문화에 익숙하지 않은 팀원들이 있었고, 내 서버가 계속 죽는 불상사가 있었기 때문에 Github를 이용한 프로젝트 관리에 어려움이 있었고 더 나은 환경에서 서로의 작업을 상세하게 공유하는 것이 생각보다 쉽지 않았다.</p><p>결국에는 전략이랄 것도 없이 그저 각자의 환경에서 서로다른 실험을 계속하는 것밖엔 이루어지지 않았는데 이것이 가장 큰 아쉬움으로 남는 것 같다.</p><h2 id="데이터-분석과-EDA"><a href="#데이터-분석과-EDA" class="headerlink" title="데이터 분석과 EDA"></a>데이터 분석과 EDA</h2><p>데이터에는 예상대로 노이즈가 몇 군데 있었다. 애초에 잘 정제되어있는 데이터였기 때문에 많지는 않았으나 남자를 여자로, 여자를 남자로 적어놓은 데이터가 있는가 하면, 마스크를 끼고 있는데 파일이름에는 마스크를 착용하지 않았다고 나와있는 것들이 존재했다. 다행히 AIStage의 토론게시판과 팀원들의 도움으로 모든 노이즈를 찾아서 제거해줄 수 있었다.</p><p>EDA 과정에서도 토론게시판과 팀원들의 도움을 많이 받았다. 특히 데이터가 잘 정제되어 있다는 것을 해당 단계에서 알아차릴 수 있었으며 마스크를 잘못 착용한 데이터에는 턱스크는 물론 마스크를 눈에 착용한다던지 하는 괴상한 사진도 있었다. 가장 좋은 분석 중 하나는 데이터의 분포를 시각화로 알아낼 수 있었다는 것인데, 60세 이상 데이터는 60세만 존재하였고 해당 데이터가 가장 적은 비율로 존재했기 때문에 ‘나이’를 제대로 예측하는 것이 가장 중요하겠다는 판단을 할 수 있었다.</p><p>지금까지는 Visualization에 대한 일종의 편견이 있었기 때문에 오히려 배우려고 들지 않았던 것 같은데, 이번 대회를 통해서 그 중요성을 인지할 수 있었던 것 같다. Seaborn과 Matplotlib을 잘 사용하는 것이 실무에서도 굉장한 도움을 얻을 수 있을 것이라고 생각된다.</p><h2 id="베이스라인-코드-및-협업환경-구성"><a href="#베이스라인-코드-및-협업환경-구성" class="headerlink" title="베이스라인 코드 및 협업환경 구성"></a>베이스라인 코드 및 협업환경 구성</h2><p>팀원들과 베이스라인 코드로 U Stage 에서 배웠던 <a href="https://github.com/victoresque/pytorch-template">파이토치 템플릿</a>을 사용하기로 했다. 워낙 우리 조에서 하는 것이 많다 보니 다들 파이토치 템플릿에 익숙하지가 않았기 때문에 간단하게 바로 실험을 돌려볼 수 있는 베이스라인 코드를 잘 작성하는 것이 생각보다 매우 어려웠다. 구조적으로 잘 짜여있고 모듈화가 잘 되어있기 때문에 파이토치 템플릿을 사용하기로 했었는데, 지금 생각해보면 이것도 더 좋은 성능을 내지 못한 이유 중 하나일 것으로 생각된다.</p><p>그렇게 생각하는 원인은 아래와 같다.</p><ol><li>협업을 위한 환경 Fix가 어렵다.</li><li>모델에 Config 파일 정보를 같이 저장하기 때문에 Config파일 없이 모델을 불러올 수가 없다.</li><li>Remote 환경으로 작업하기 때문에 CLI에 익숙하지 않으면 원하는 대로 파이썬 스크립트를 실행해보기가 어렵다.</li><li>모듈화가 잘 되어있지만 의존성을 가지는 함수들이 있기 때문에 구조를 제대로 파악하지 않으면 원하는대로 커스텀하기가 어렵다.</li></ol><p>대부분 대회에서는 빠른 커스터마이징과 실험을 통해 계속해서 성능을 올리는 것을 목표로 한다. 하지만 우리 조원들중에는 CLI와 Git을 처음 써보는 팀원도 있었고 파이썬 코드에 익숙하지 않은 팀원도 있었기 때문에 파이토치 템플릿같은 구조를 완벽하게 이해하지 못해서 내가 만든 베이스라인을 팀원들이 이해하고 실행하는데 꽤 오랜 시간을 소비하게 되었다.</p><p>Dot ENV 혹은 이미지 경로 등 MetaData와 환경을 고정할 수 있는 요소들을 미리 감안하지 않은 것, 그리고 애초에 쉽게 구조를 파악할 수 없는 템플릿을 사용하여 많은 실험을 못하게 된 것을 다음 협업때부터는 고려할 수 있도록 해보아야 할 것 같다. 또한 Python 스크립트도 좋지만 Jupyter 환경에서 돌아가는 코드를 작성하는 것도 매우 좋을 것 같다..!=</p><h2 id="모델링과-실험"><a href="#모델링과-실험" class="headerlink" title="모델링과 실험"></a>모델링과 실험</h2><p>이번 P Stage에서 협업을 제외하고 가장 큰 아쉬움으로 다가오는 것은 실험일지를 체계적으로 정리하지 않았고, 파일관리를 제대로 하지 않았다는 것이다. 총 100기가의 저장공간 중 실제 사용할 수 있는 용량은 약 6-70 기가정도였는데 Epochs 단위마다 모델을 저장해놓았기 때문에 계속해서 서버용량에 제약을 받게 되었다. 그리고 점점 끝이 다가올수록 성능향상도 안되고 마음도 급해져서 잘 확인하지도 않고 파일을 삭제하는 것을 반복했었는데, 그러다가 최고 성능을 낸 모델까지 지워버리는 참사가 발생했다. 최종 제출 직전에 확인하고 현재 최고 스코어가 재현이 안된다는 것을 알았을 때 정말, 그 좌절감은 이루말할 수 없다.</p><p>그리고 기존 실험하고 있던 모델과 하이퍼파라미터를 픽스하고 여러가지 개선방안 중에서 하나씩 적용해보면서 성능이 오른 것을 취하는 전략이 가장 좋았을 것 같은데, 그렇게 하지 않았기 때문에 분명히 성능이 오를 요소들을 제대로 포착하지 못했다. 예를들어 Augmentation이 적용된 이미지들을 만들고 거기에 Augmix 등 여러 Augmentaiton을 Soft하게 적용하여 실험했는데 성능이 떨어졌다. 이 때 데이터를 추가해서 성능이 떨어진건지, 아니면 다른 Augmentation 때문에 성능이 떨어진 건지 알지 못했다.</p><p>실험일지를 잘 작성하고 기존에서 성능을 올리는 쪽으로만 전략을 취할 수 있도록 다음 대회부터는 일지를 작성해봐야할 것 같다.</p><p>또한 위의 이야기와 더불어 실험을 했던 코드를 체계적으로 정리했으면 어땠을까 싶다. 모듈화가 잘 되어 있는 템플릿을 사용하면서 여러 실험을 할 때는 대부분 하드코딩 아니면 모듈화를 시키지 않은 채 코드를 작성했기 때문에 실험환경을 바꿀 때마다 피로도가 굉장히 컸다. 그리고 Wandb와 Tensorboard 등 시각화 도구들을 사용할 때 Runtime 에러가 발생했을 때도 모든 로그를 기록하게 해서 서버 용량이 굉장히 빠르게 차올랐는데, 이 부분 역시 쉽게 적용여부를 결정할 수 있도록 만들었으면 더 좋았을 것 같다.</p><h2 id="총평"><a href="#총평" class="headerlink" title="총평"></a>총평</h2><p>근본없이 너무 무지성 실험을 반복했던 것 같다. 결국 그렇게 하다가 성능이 오르긴 했었지만, 그렇게 성능이 올랐을 때의 환경을 픽스하지 못하고 다른 추가적인 것들을 적용하다 보니 최종스코어를 재현할 수도 없고 오히려 성능이 떨어지는 상황이 수도없이 반복되었다.</p><p>또한 계속 협업의 중요성을 말하고 다녔지만 결국엔 나부터도 버전관리와 공유에 소홀했다. 다시금 그 중요성을 생각하면서 같은 실수는 반복하지 않도록 노력해야겠다.</p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-Pstage1-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-09-04/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech 4주차 회고 (2021-08-23 ~ 2021-08-27)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-4%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-08-27/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-4%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-08-27/</guid>
      <pubDate>Sat, 28 Aug 2021 10:38:28 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;강의내용&quot;&gt;&lt;a href=&quot;#강의내용&quot; class=&quot;headerlink&quot; title=&quot;강의내용&quot;&gt;&lt;/a&gt;강의내용&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="강의내용"><a href="#강의내용" class="headerlink" title="강의내용"></a>강의내용</h2><ul><li>강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.</li></ul><h3 id="학습현황"><a href="#학습현황" class="headerlink" title="학습현황"></a>학습현황</h3><ul><li>21.08.23 월 (09:40 ~ 20:20)</li><li>21.08.24 화 (10:30 ~ 23:59)</li><li>21.08.25 수 (07:00 ~ 23:59)</li><li>21.08.26 목 (09:50 ~ 23:59)</li><li>21.08.27 금 (10:00 ~ 23:59)</li></ul><h3 id="진도"><a href="#진도" class="headerlink" title="진도"></a>진도</h3><ul><li>이미지 분류 대회를 위한 EDA - Dataset - Data Generation - Model - Training - Inference - Ensemble - Experiment Toolkits</li></ul><h3 id="이번-주의-개선할-사항-및-자기성찰"><a href="#이번-주의-개선할-사항-및-자기성찰" class="headerlink" title="이번 주의 개선할 사항 및 자기성찰"></a>이번 주의 개선할 사항 및 자기성찰</h3><ul><li>협업을 위한 코드에서 고려하지 못한 점들이 너무 많이 있었다.<ul><li>내가 아는 것을 다른 사람도 알 거라는 착각</li><li>아 이걸로 되겠지 하면서 push 했다가 오류 발생하는 지점들</li><li>예외처리 고려 여부</li><li>Package fix</li><li>TODO List 공유 x</li><li>수정사항 및 기능 설명이 우선시 되었어야 하는데 하지 않았음.</li></ul></li><li>앞으로 팀 전체 코드를 위해서는 상대방의 아무것도 없는 환경 상태에서 바로 돌아갈 수 있도록 더 많은 신경을 써서 코드를 작성해야겠다.</li></ul><hr><h2 id="피어세션"><a href="#피어세션" class="headerlink" title="피어세션"></a>피어세션</h2><h3 id="프로젝트-관리"><a href="#프로젝트-관리" class="headerlink" title="프로젝트 관리"></a>프로젝트 관리</h3><ul><li>Github에서 Kanban 기반의 Project Dashboard를 도입하였다.</li><li>깃허브 프로젝트 이외의 Google Docs를 활용하여 회의록을 기록하고 실험결과를 공유하는데 신경써야 할 채널이 너무 많아져서 헷갈린다.</li><li>P Stage 이전에 특히 우리조가 하는 것이 많았어서 어떤 것을 선택하고 집중해야 할 지 잘 몰랐기 때문에 대회 초반에는 어수선했는데 조금은 안정이 된 것 같다.</li></ul><h3 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h3><ul><li>나는 베이스라인 코드를 작성하는 것에 집중하였다. Pytorch-Template을 사용하기로 했었기 때문에 해당 템플릿에 우리 과제를 적용하려고 많은 노력을 했다. 여러 실험을 하고 싶었지만 깃에 익숙하지 않거나 코드를 실행할 줄 모르는 팀원들이 있었기에 같은 환경을 조성하고 서로가 많은 실험을 할 수 있도록 코드를 공유하고 알려주는 것에 많은 시간을 할애했다.</li><li>또한 논문리뷰 정리를 위한 Git Pages 를 만들었고 설명과 함께 공유하였으나 이번주는 너무 바빠서 아무도 진행하지 못했다.</li><li>원래 우리 조가 하는 것이 많았고 협업에 익숙하지 않은 팀원들이 대다수였기 때문에 진행했어야 하는 부분들은 책임을 지고 내가 수행했어야 했던 것이 좀 아쉬운 것 같다. 깃 페이지와 베이스라인 작성을 하기 위해 이번주에는 거의 아무것도 진행하지 못했다. ㅠㅠ 다음주에는 좀 많은 실험들을 할 수 있었으면 좋겠다.</li></ul><hr><h2 id="총평"><a href="#총평" class="headerlink" title="총평"></a>총평</h2><ul><li>P Stage가 되면서 많이 정신없는 것이 느껴진다. 이번주에만 학습시작 누르는 것을 두 번이나 지각하고 연락오는 것들도 다 못받고 대회에만 집중했기 때문이다. 잘 하고 있는 거겠지..?</li><li>내가 팀에는 많은 기여를 한 것 같지만 부족한 점이 너무 많이 느껴진다. 더 좋은 코드, 환경을 조성하기 위해 더 열심히 살아야겠다.</li><li>서버좀 그만 다운됐으면 좋겠다. 벌써 두번째라 작업한게 다 날아가서 의욕이 많이 떨어진다. 하..</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-4%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-08-27/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Paper Review를 위한 깃허브 협업 페이지 관리하기</title>
      <link>https://l-yohai.github.io/Paper-Review%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%ED%98%91%EC%97%85-%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</link>
      <guid>https://l-yohai.github.io/Paper-Review%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%ED%98%91%EC%97%85-%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/</guid>
      <pubDate>Mon, 23 Aug 2021 20:49:16 GMT</pubDate>
      
      <description>&lt;p&gt;팀프로젝트를 진행하면서 여러 목적성을 가진 채널이 필요해졌고, 그 중 하나가 논문리뷰한 자료들을 어떻게 관리할지였다. 팀원들 모두 퀄리티가 너무 좋게 준비를 잘 해오셔서 시각적인 요소들을 사용하여 자료를 공유할 수 있으면 좋겠다는 생각을 했고, 여러 이야기 끝에 깃허브 페이지를 사용해보기로 했다.&lt;/p&gt;
&lt;h2 id=&quot;Jekyll-or-Hexo&quot;&gt;&lt;a href=&quot;#Jekyll-or-Hexo&quot; class=&quot;headerlink&quot; title=&quot;Jekyll or Hexo?&quot;&gt;&lt;/a&gt;Jekyll or Hexo?&lt;/h2&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>팀프로젝트를 진행하면서 여러 목적성을 가진 채널이 필요해졌고, 그 중 하나가 논문리뷰한 자료들을 어떻게 관리할지였다. 팀원들 모두 퀄리티가 너무 좋게 준비를 잘 해오셔서 시각적인 요소들을 사용하여 자료를 공유할 수 있으면 좋겠다는 생각을 했고, 여러 이야기 끝에 깃허브 페이지를 사용해보기로 했다.</p><h2 id="Jekyll-or-Hexo"><a href="#Jekyll-or-Hexo" class="headerlink" title="Jekyll or Hexo?"></a>Jekyll or Hexo?</h2><p>깃허브 페이지를 관리할 때 주로 Jekyll과 Hexo를 사용한다. 처음에는 Jekyll로 페이지를 꾸며보았지만 동적 사이트 생성방식인 Jekyll은 반영시간이 매우 길고 관리가 귀찮기 때문에 Hexo로 옮기게 되었다.</p><h2 id="Previous-Install"><a href="#Previous-Install" class="headerlink" title="Previous Install"></a>Previous Install</h2><ul><li><a href="https://nodejs.org/ko/download/">Node.js</a><ul><li>Mac의 경우 <code>brew install node</code> 로 설치할 수 있다.</li><li>설치가 잘 되었는지 확인하기 위해서는 cmd(window) 혹은 터미널(mac)에서 <code>npm version</code> 을 쳐보자!</li></ul></li><li>git</li></ul><h2 id="환경-세팅"><a href="#환경-세팅" class="headerlink" title="환경 세팅"></a>환경 세팅</h2><p>docs 등 로컬에서 관리할 폴더만 하나 만들어두고, PaperReview 레포지토리를 클론받습니다.</p><p><code>git clone https://github.com/Boostcamp-AI-Tech-1-15/PaperReview.git</code></p><p>PaperReview 레포지토리를 클론받고 현재 <code>develop</code> 브랜치에 관련 파일들을 전부 올려두었는데,</p><ol><li><code>cd PaperReview</code></li><li><code>git checkout develop</code></li></ol><p>위의 명령어를 통해 develop 브랜치로 이동한후 자신이 관리하고 싶은 로컬 디렉토리에 관련된 정보들을 모두 copy 하면 됨.</p><ul><li>현재 npm 으로 모듈들을 설치해놓았고, 버전관리 시스템이 구축되어있지 않기 때문에 충돌을 피하기 위해 로컬 디렉토리로 카피해서 옮겨주는 것을 권장한다!</li></ul><p>ex) 로컬에 docs 라는 폴더를 만든 후 docs 디렉토리로 이동 후 <code>cp -rf ../PaperReview/* .</code> 그냥 ctrl+C,V 혹은 드래그앤드롭 해도 된다. (<strong>숨김파일 체크한 후 모두 복사해와야함!!!</strong>)</p><ul><li>로컬에서 저장할 폴더는 .git 이없어도 가능. (git remote가 없는 상태에서도 가능)</li><li>모든 파일이 복사가 끝났으면 <code>npm list</code> 를 통해 아래와 같은 모듈이 전부 잘 있는지 체크한다. (없다면 저한테 연락을..)</li></ul><p><img src="/image/image-20210824050525454.png" alt="image-20210824050525454"></p><h2 id="포스트-생성하기"><a href="#포스트-생성하기" class="headerlink" title="포스트 생성하기"></a>포스트 생성하기</h2><p>현재 초기세팅은 진행해놓았기 때문에 post 할 파일만 마크다운으로 생성하고 깃에 배포만 하면 된다.</p><p>포스트 생성하는법</p><ul><li><p><code>hexo new post &quot;파일이름&quot;</code></p><p><img src="/image/image-20210824045441184.png" alt="image-20210824045441184"></p><p>위의 명령어를 치게 되면 ~/docs/source/_posts/ 경로에 내가 생성한 이름의 md 파일이 생성됨.</p><ul><li>이 때 내가 생성한 파일 이름이 라우터로 들어간다.</li><li>test라는 이름으로 생성했을 경우 url = <code>https://boostcamp-ai-tech-1-15.github.io/PaperReview/test</code></li></ul><p><img src="/image/image-20210824050652022.png" alt="image-20210824050652022"></p><p>Test.md 파일을 열어보면 위와 같은 상태일텐데, <code>date, category, tags, title</code> 정도만 수정해주면 된다.</p><ul><li><p>category 작성법:</p><ul><li><p>string 과 array 형식으로 넣을 수 있다.</p><ul><li><p>단일 스트링의 경우 카테고리 하나만 등록됨.</p></li><li><p>[A, B] 배열 형태로 집어넣었을 경우</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">A</span><br><span class="line">.. B</span><br></pre></td></tr></table></figure><p>형태로 순차적인 child 계층으로 카테고리가 만들어짐.</p><p>예를들어 CV의 Detection 카테고리에 넣고 싶다면, [Computer Vision, Object Detection] 이라고 입력하시면 된다.</p></li></ul></li></ul></li><li><p>Tags 작성법:</p><ul><li>Category 와 똑같음.</li></ul></li></ul></li><li><p>카테고리와 날짜, 제목, 태그까지 전부 작성이 끝났으면 그 아래에 마크다운으로 작성한 글을 그대로 복사붙여넣기 하면 된다.</p></li><li><p>이 때 Header 순(#, ##, ###) 으로 TOC(Table of Contents)가 작동하게 되니 참고바랍니다!</p></li></ul><h3 id="Warning"><a href="#Warning" class="headerlink" title="Warning"></a>Warning</h3><p>포스트를 만들 때 layout에 썸네일을 추가할 수 있고, 마크다운 글 안에 이미지나 gif를 추가할 수 있다.</p><p>썸네일 추가는 자동으로 추가되는 layout에 <code>thumbnail: 이미지경로</code> 로 추가할 수 있다.</p><p><img src="/image/image-20210824051724425.png" alt="image-20210824051724425"></p><p>마크다운 글 안에서 이미지 추가는 마크다운 문법인 <code>![html에 보여질 이미지 설명](이미지경로)</code> 로 추가할 수 있다.</p><p><img src="/image/image-20210824051814501.png" alt="image-20210824051814501"></p><p>이미지는 <code>source/image</code> 폴더에 추가해주시면 된다.</p><p>이 때 주의할 것은 <strong><em>썸네일과 이미지 경로가 다릅니다.</em></strong> 이걸 해결해보려고 하다가 시간이 너무 지체되는 바람에.. 일단은 포기했습니다. ㅜㅜ</p><ul><li>썸네일 이미지 경로: <code>/image/이미지</code></li><li>포스팅 글에서의 이미지 경로: <code>/PaperReview/image/이미지</code></li></ul><p>ex) <code>source/image</code>에 test.jpg 를 넣었다고 했을 때 </p><ul><li>썸네일 경로: <code>thumbnail: /image/test.jpg</code></li><li>포스팅 글에서의 이미지 경로: <code>/PaperReview/image/test.jpg</code></li></ul><p>같은 파일이라고 하더라도 링크가 걸리는 상대경로가 다르기 때문에 꼭 주의 요망!!</p><h2 id="깃허브-페이지에-배포하기"><a href="#깃허브-페이지에-배포하기" class="headerlink" title="깃허브 페이지에 배포하기"></a>깃허브 페이지에 배포하기</h2><p>여기까지 작성이 끝났다면 로컬에서 <code>hexo s</code> 혹은 <code>hexo server</code> 명령어를 통해 서버를 실행한 뒤 <code>localhost:4000</code> 으로 접속해서 배포 전에 미리 확인할 수 있다. (생략 가능)</p><p>배포는 정말 간단하게도</p><p><code>hexo deploy --generate</code></p><p>명령만 실행하면 알아서 <code>PaperReview 레포지토리의 gh-pages 브랜치</code> 로 add/commit/push 가 일어난다. 이후 몇 분 뒤 저희 깃허브 페이지로 접속하면 반영되어있는 것을 확인할 수 있다!</p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Programming/">Programming</category>
      
      <category domain="https://l-yohai.github.io/categories/Programming/Tips/">Tips</category>
      
      
      
      <comments>https://l-yohai.github.io/Paper-Review%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%ED%98%91%EC%97%85-%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Fazil Say - Paganini Variation 도입부</title>
      <link>https://l-yohai.github.io/Fazil-Say-Paganini-Variation-%EB%8F%84%EC%9E%85%EB%B6%80/</link>
      <guid>https://l-yohai.github.io/Fazil-Say-Paganini-Variation-%EB%8F%84%EC%9E%85%EB%B6%80/</guid>
      <pubDate>Sun, 22 Aug 2021 10:39:23 GMT</pubDate>
      
      <description>&lt;p&gt;Fazil Say는 여러 유명 작품들을 많이 편곡했는데 그 중에서 터키행진곡과 이 곡이다.&lt;/p&gt;
&lt;p&gt;현대음악적인 테크닉들이 매우 많이 등장하며, 현대음악답게.. 뒷부분은 내 취향이 아니다. 이해하기 넘 어려워&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>Fazil Say는 여러 유명 작품들을 많이 편곡했는데 그 중에서 터키행진곡과 이 곡이다.</p><p>현대음악적인 테크닉들이 매우 많이 등장하며, 현대음악답게.. 뒷부분은 내 취향이 아니다. 이해하기 넘 어려워</p><p>리스트의 파가니니 연습곡 6번(주제와변주)을 재즈느낌으로 편곡했는데 참 아름답게 잘 살린 것 같다.</p><p>원곡은 입시곡이었어서 맨 첫 시작 긁는 것만 들어도 PTSD 오는데</p><p>이건 좀 재밌다. 이게 편곡의 순기능인가 ㅎㅎ;</p><p><a href="https://www.youtube.com/watch?v=qh8INgcDOOQ">내가친거!</a><br><div class="video-container"><iframe src="https://www.youtube.com/embed/qh8INgcDOOQ" frameborder="0" loading="lazy" allowfullscreen></iframe></div></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/fur-Musik/">für Musik</category>
      
      
      
      <comments>https://l-yohai.github.io/Fazil-Say-Paganini-Variation-%EB%8F%84%EC%9E%85%EB%B6%80/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Arensky - Suite No.1 in F major, Op. 15 - Part 2/3</title>
      <link>https://l-yohai.github.io/Arensky-Suite-No-1-in-F-major-Op-15-Part-2-3/</link>
      <guid>https://l-yohai.github.io/Arensky-Suite-No-1-in-F-major-Op-15-Part-2-3/</guid>
      <pubDate>Sun, 22 Aug 2021 10:33:41 GMT</pubDate>
      
      <description>&lt;p&gt;Anton Stepanovich Arensky 의 &lt;code&gt;두 대의 피아노를 위한 모음곡 1번 F장조, Op. 15&lt;/code&gt; 중 2번 왈츠이다.&lt;/p&gt;
&lt;p&gt;림스키코르사코프에게 작곡을 배운 뒤 라흐마니노프와 스크리아빈을 제자로 둔 거장이다.&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<p>Anton Stepanovich Arensky 의 <code>두 대의 피아노를 위한 모음곡 1번 F장조, Op. 15</code> 중 2번 왈츠이다.</p><p>림스키코르사코프에게 작곡을 배운 뒤 라흐마니노프와 스크리아빈을 제자로 둔 거장이다.</p><p>3개의 소품으로 이루어진 이 곡 중 두 번째 ‘Valse’ 는 아기자기하고 귀여운 멜로디가 낭만적으로 느껴진다.</p><p>First와 Second가 주 선율을 주고받으며 고조되다가 화려한 아르페지오로 곡의 분위기를 반복적으로 반전시킨다.</p><p>유명하진 않지만 은근 힐링음악.</p><p><a href="https://www.youtube.com/watch?v=d-FFCen-51g">내가친거!</a><br><div class="video-container"><iframe src="https://www.youtube.com/embed/d-FFCen-51g" frameborder="0" loading="lazy" allowfullscreen></iframe></div></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/fur-Musik/">für Musik</category>
      
      
      
      <comments>https://l-yohai.github.io/Arensky-Suite-No-1-in-F-major-Op-15-Part-2-3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>부스트캠프 AI Tech 3주차 회고 (2021-08-17 ~ 2021-08-20)</title>
      <link>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-3%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-17-2021-08-20/</link>
      <guid>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-3%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-17-2021-08-20/</guid>
      <pubDate>Fri, 20 Aug 2021 08:01:05 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;강의내용&quot;&gt;&lt;a href=&quot;#강의내용&quot; class=&quot;headerlink&quot; title=&quot;강의내용&quot;&gt;&lt;/a&gt;강의내용&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="강의내용"><a href="#강의내용" class="headerlink" title="강의내용"></a>강의내용</h2><ul><li>강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다.</li></ul><h3 id="학습현황"><a href="#학습현황" class="headerlink" title="학습현황"></a>학습현황</h3><ul><li>21.08.17 화 (09:50 ~ 19:15)</li><li>21.08.18 수 (10:20 ~ 23:50)</li><li>21.08.19 목 (09:36 ~ 05:00)</li><li>21.08.20 금 (09:48 ~ 23:59 예정)</li></ul><h3 id="진도"><a href="#진도" class="headerlink" title="진도"></a>진도</h3><ul><li>Pytorch (Basic - Pytorch Template - Custom Model - AutoGard &amp; Optimizer - Dataset &amp; Dataloader - Custom Dataset - Monitoring tools - Multi GPU - Hyperparameter Tuning - Troubleshooting)</li></ul><h3 id="지난-주-선언에서-지킨-것과-지키지-못한-것"><a href="#지난-주-선언에서-지킨-것과-지키지-못한-것" class="headerlink" title="지난 주 선언에서 지킨 것과 지키지 못한 것"></a>지난 주 선언에서 지킨 것과 지키지 못한 것</h3><ul><li>수식 하나, 키워드 하나조차 가볍게 넘기지 말라는 다짐을 어느정도 지킬 수 있었던 것 같다. 파이토치의 공식문서를 트래킹하면서 어떤 역할을 하는지 몰랐던 함수들에 대해서 하나씩 정리해나갈 수 있었다.</li></ul><h3 id="이번-주의-개선할-사항-및-자기성찰"><a href="#이번-주의-개선할-사항-및-자기성찰" class="headerlink" title="이번 주의 개선할 사항 및 자기성찰"></a>이번 주의 개선할 사항 및 자기성찰</h3><ul><li>한 번에 너무 많은 양의 코드를 실행해보고, 너무 많은 지식을 받아들이려고 하다 보니 뇌용량에 한계가 온 것 같다. 단기간에 기억해야 하는 것들을 전부 기억할 수 있다는 자신감이 떨어지는데, 오래 볼 수 있도록 한 번 정리를 하는게 좋지 않았을까 하는 아쉬움이 남는다. 다음주에는 성실한 블로그 포스팅에 조금 더 신경써보자.</li></ul><hr><h2 id="과제-수행과정과-결과물"><a href="#과제-수행과정과-결과물" class="headerlink" title="과제 수행과정과 결과물"></a>과제 수행과정과 결과물</h2><ul><li>2개의 필수과제와 1개의 선택과제</li></ul><p>분류해결 여부 or 결과</p><table style="border-collapse: collapse; width: 100%;" border="1" data-ke-align="alignLeft"><tbody><tr><td><span><span>필수과제 1 - Custom Model 개발</span></span></td><td><span><span>O</span></span></td></tr><tr><td><span><span>필수과제 2 - Custom Dataset 개발</span></span></td><td><span><span>O</span></span></td></tr><tr><td><span><span>선택과제 1 - Transfer Learning &amp; Parameter Tuning</span></span></td><td><span><span>O</span></span></td></tr></tbody></table><h3 id="Facts-사실-객관"><a href="#Facts-사실-객관" class="headerlink" title="Facts (사실, 객관)"></a>Facts (사실, 객관)</h3><ul><li>파이토치 공식문서를 읽는 것에 대해 어느정도 익숙해질 수 있었음.</li><li>내부 코드들을 뜯어보면서 어떤 식으로 함수를 작성하고 디버깅해야 하는지를 대략적으로 파악할 수 있었음.</li><li>과제 양이 역대급으로 많아서 과제 일부는 문제 해결에만 급급하여 제대로 보지 않았다.</li></ul><h3 id="Feelings-느낌-주관"><a href="#Feelings-느낌-주관" class="headerlink" title="Feelings (느낌, 주관)"></a>Feelings (느낌, 주관)</h3><ul><li>파이토치에 대한 막연한 이해(?) 때문에 무지성코딩만 해왔었는데 이제는 조금 구조적으로, 빠른 시간안에 코딩할 수 있을 것 같다는 자신감이 어느정도 생긴 것 같다.</li><li>그와 동시에 너무 많은 양이 있기 때문에 이것을 다 알 수 있을까? 라는 두려움이 생겼다.</li></ul><h3 id="Findings-배운-점"><a href="#Findings-배운-점" class="headerlink" title="Findings (배운 점)"></a>Findings (배운 점)</h3><ul><li>공식문서 Tracking 방법과 내부 모듈 코드를 Tracking 하면서 함수의 호출 시점 및 반환여부와 동작과정을 어떻게 알 수 있는지 배울 수 있었다.</li></ul><h3 id="Affimation-자기선언"><a href="#Affimation-자기선언" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>지난 주 선언대로 이번 주의 모든 과제에 도전하였고, 코드 전체 내용을 트래킹할 수 있었다. 다만 다음주에는 중간중간 채 신경쓰지 못하거나 급하게 넘기는 부분들이 더 줄어들어야 할 것이다.</li></ul><hr><h2 id="피어세션"><a href="#피어세션" class="headerlink" title="피어세션"></a>피어세션</h2><h3 id="알고리즘-스터디"><a href="#알고리즘-스터디" class="headerlink" title="알고리즘 스터디"></a><a href="https://github.com/Boostcamp-AI-Tech-1-15/algorithm_study">알고리즘 스터디</a></h3><ul><li>지난주보다 어려운 문제에 도전했고 대부분이 잘 풀어냈다. 특히 깔끔한 코드를 짜거나 신박한 방식으로 문제에 접근한 팀원분들의 코드를 보면서 많은 것을 배울 수 있었던 것 같다. 또한 점점 팀원들이 변수명 혹은 함수명에 대한 고찰을 하기 시작했고 관련된 discussion도 리뷰에서 활발히 일어난 것 같아서 매우 긍정적으로 생각하고 있다.</li></ul><h3 id="논문리뷰"><a href="#논문리뷰" class="headerlink" title="논문리뷰"></a>논문리뷰</h3><ul><li>지난주 피드백에 따라 이번주에는 세 줄 요약과 더불어 그림이나 사진 및 이해에 도움이 되는 요소들을 추가하여 발표를 진행했다. 기존 피어세션 타임을 조금 넘을 정도로 모두가 열심히 준비했고 덕분에 지난 주 보다는 다른 사람들의 논문을 수월하게 대략적으로 이해할 수 있었다.</li><li>영어를 읽는 시간이 너무 오래걸린다. 처음부터 끝까지 간단한 번역작업을 동시에 했음에도 4시간 30분 정도가 걸렸는데 (약 12p) 영어 공부의 필요성이 점점 실감나는 것 같다…</li><li>저번 주의 회고대로 틈틈히 읽는 버릇을 들여야 하는데 아직은 시간을 효율적으로 쓰는 것이 어려운 것 같다.</li></ul><h3 id="멘토링"><a href="#멘토링" class="headerlink" title="멘토링"></a>멘토링</h3><ul><li>지난 주 선택과제 리뷰와 2주차 활동에 대한 피드백을 받을 수 있었다.</li><li>나를 조금 과대평가하시는 것 같은데..!? 기대에 부응하고 싶다.</li><li>지난 주 챌린징한 논문을 일부러 주셨다는데.. 역시 아직은 좀 ‘많이’ 어렵다. 이번 주 논문 난이도 정도만 돼도 읽을만 할 듯!!</li></ul><h3 id="그-외"><a href="#그-외" class="headerlink" title="그 외"></a>그 외</h3><ul><li>팀원들끼리 친해지고 익숙해져서 점점 얘기도 많이하고 즐거운 시간들을 보내고 있다.</li><li>가장 좋게 생각하는 것은 의견을 많이 낸다는 것!</li><li>이렇게 또 성장하는 것 같다.</li></ul><hr><h2 id="학습회고"><a href="#학습회고" class="headerlink" title="학습회고"></a>학습회고</h2><h3 id="Feelings-느낌-주관-1"><a href="#Feelings-느낌-주관-1" class="headerlink" title="Feelings (느낌, 주관)"></a>Feelings (느낌, 주관)</h3><ul><li>여전히 처음부터 백지상태 코딩을 하라그러면 못할 것 같다. 너무 많은 대량의 코드를 구조적으로 인식하고 있어야 하며, 각 구조 속에 어떤 코드가 들어가야 할 지 인지하고 있어야 하는데 아직 그 정도 레벨까진 못되는 것 같다. 좋은 코드를 더 많이 보고 기억하고 작성하려는 노력을 해야할 것 같다.</li></ul><h3 id="Affimation-자기선언-1"><a href="#Affimation-자기선언-1" class="headerlink" title="Affimation (자기선언)"></a>Affimation (자기선언)</h3><ul><li>이번 주차에는 대부분의 시간을 강의를 보고 정리하는 시간보다는 공식문서를 읽고 과제를 해결하는데 시간을 썼던 것 같다. 강의 내용이 겉핥기 수준이었던 반면 과제 자체는 위키독스 수준으로 내용이 많았기 때문에 과제만으로 어느정도 만족할만한 학습시간을 쏟은 것 같긴 하다.</li><li>하지만 그럼에도 내실이 살짝 부족한 느낌이 계속 드는데.. 다음 주에는 조금 더 많이 생각하고 많이 질문하며 최대한 체득할 수 있었으면 좋겠다.</li></ul><h3 id="내가-따로-노력한-것들"><a href="#내가-따로-노력한-것들" class="headerlink" title="내가 따로 노력한 것들"></a>내가 따로 노력한 것들</h3><ul><li>이번 주는 없음.</li></ul><hr><h2 id="총평"><a href="#총평" class="headerlink" title="총평"></a>총평</h2><ul><li>저번 주 회고의 선언들이 대부분 지켜진 것 같아서 만족스럽다!</li><li>다음 주 역시 이번주의 선언을 토대로 개선된 학습을 진행할 수 있었으면 좋겠다.</li><li>본격적으로 파이토치를 다루면서 파이토치 내부 클래스 및 함수들에 대한 공부를 많이 했는데, 여전히 차원을 다루고 원하는대로 슬라이싱하고 쪼개고 뽀개고 합치고 하는 걸 잘 못하겠다. 시간이 해결해주려나..?</li><li>취준에 대한 시간을 평일에는 따로 투자하지 못했는데, 주말에는 꼭 해보자!!</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Diary/">Diary</category>
      
      
      
      <comments>https://l-yohai.github.io/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-3%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-17-2021-08-20/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Data Centric 특강 - 최성철 교수님</title>
      <link>https://l-yohai.github.io/Data-Centric-%ED%8A%B9%EA%B0%95-%EC%B5%9C%EC%84%B1%EC%B2%A0-%EA%B5%90%EC%88%98%EB%8B%98/</link>
      <guid>https://l-yohai.github.io/Data-Centric-%ED%8A%B9%EA%B0%95-%EC%B5%9C%EC%84%B1%EC%B2%A0-%EA%B5%90%EC%88%98%EB%8B%98/</guid>
      <pubDate>Thu, 19 Aug 2021 09:03:43 GMT</pubDate>
      
      <description>&lt;h1 id=&quot;Data-Centric&quot;&gt;&lt;a href=&quot;#Data-Centric&quot; class=&quot;headerlink&quot; title=&quot;Data Centric&quot;&gt;&lt;/a&gt;Data Centric&lt;/h1&gt;&lt;p&gt;더이상 성능을 미세하게 올리는 것에 집착하는 AI 엔지니어링은 요구되지 않는다. 과연 어떤 능력이 더 중요할 것인가?&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Data-Centric"><a href="#Data-Centric" class="headerlink" title="Data Centric"></a>Data Centric</h1><p>더이상 성능을 미세하게 올리는 것에 집착하는 AI 엔지니어링은 요구되지 않는다. 과연 어떤 능력이 더 중요할 것인가?</p><ul><li>수학을 임성빈 교수님만큼 해야 AI를 할 수 있는가? Nope</li><li>대신에 ML/AI Engineer들은 이제는, 코딩을 잘해야 한다. 네트워크에 대한 지식이나, CS에 대한 지식들이 더 많은 Area에서 기회를 줄 것임.</li></ul><p>실전 AI에서는 매우 복잡한 단계를 거쳐서 프로젝트가 진행된다.</p><p>Issues</p><ul><li>Data<ul><li>실제 ML 프로젝트에서는 양질의 데이터 확보가 관건임.</li><li>Production time 데이터(프로젝트가 끝난 후 Serving 단계에서의 데이터) 와  Experiment 데이터가 다를 때 발생하는 문제들</li><li>데이터 생성에 관한 문제<ul><li>User generated data: inputs, clicks for recommendation</li><li>System generated data: logs, metadata, prediction</li><li>Data Flywheel (데이터 선순환): 사용자들 참여로 데이터 개선<ul><li>구글 포토를 사용하면 가족들의 얼굴을 계속 같은 사람인지 물어보게 하여 사람들이 직접 태깅하게 하는 것. 데이터가 스스로 라벨링을 하게 하는 작업.</li></ul></li><li>Data augmentation: 데이터를 임의로 추가 확보</li></ul></li><li>Data drift: 시간이 지나면서 데이터는 계속 바뀔 것임. 어떻게 production 레벨과 experiment 레벨의 데이터 간극을 맞출지가 중요한 문제이다.<ul><li>2주나 4주 등 일정 간격을 정한 뒤 새로운 데이터로 새로운 모델을 뽑아냄. 이 때 새로운 모델을 뽑아낼 때는 hyper parameter tuning이 되어있을 것임. 이것을 가지고 Dynamic 하게 학습을 시키는 등 대책이 있으면 좋음.</li></ul></li><li>Data Feedback Loop<ul><li>사용자로부터 오는 데이터를 자동화하여 모델에 피딩해주는 것이 중요함.</li></ul></li></ul></li><li>Model</li><li>Algorithms</li><li>Metrics</li><li>Hyper parameter tuning</li></ul><p>앞으로 알아야 할 것들.</p><ul><li>MLOps 도구들</li><li>당연히 데이터베이스!! SQL</li><li>Cloud - AWS, GCP, Azure</li><li>Spark (+ Hadoop)</li><li>Linux + Docker + 쿠버네티스</li><li>Scheduling 도구 (KubeFlow, MLFlow, <strong>AirFlow</strong>)</li></ul><p>위의 것들을 써보는 것을 추천하지만, 전문가가 되라는 것은 아님. 최근엔 이러한 역량들을 전부 요구하기 때문이다.</p><p>하나의 시스템으로서의 ML/DL 개발</p><ul><li>우리는 보통 주피터노트북을 키는 것부터 시작하는데, 모든 일이 주피터노트북에서 일어나지 않는다. 주피터노트북은 그저 실험을 위한 도구일 뿐.</li><li>개발에는 Code - Test - Monitor - Deploy 단계가 반드시 포함된다. 그리고 해당 단계에서의 도구들을 사용할 줄 아는 역량이 필요하다.</li></ul><p>정리</p><ul><li>앞으로는 알고리즘 연구자보다 ML/DL 엔지니어의 필요성이 더 증대</li><li>단순히 ML/DL 코드 작성을 넘어서야 함<ul><li>자동화, 데이터연계, 실험결과에 기반한 설득, 시스템화</li></ul></li><li>좋은 기획자적인 요소들이 필요.</li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Boostcamp-AI-Tech/">Boostcamp AI Tech</category>
      
      <category domain="https://l-yohai.github.io/categories/Boostcamp-AI-Tech/Master-Class/">Master Class</category>
      
      
      
      <comments>https://l-yohai.github.io/Data-Centric-%ED%8A%B9%EA%B0%95-%EC%B5%9C%EC%84%B1%EC%B2%A0-%EA%B5%90%EC%88%98%EB%8B%98/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>WaveNet 리뷰</title>
      <link>https://l-yohai.github.io/WaveNet-Review/</link>
      <guid>https://l-yohai.github.io/WaveNet-Review/</guid>
      <pubDate>Thu, 19 Aug 2021 08:01:05 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;Short-Summary&quot;&gt;&lt;a href=&quot;#Short-Summary&quot; class=&quot;headerlink&quot; title=&quot;Short Summary&quot;&gt;&lt;/a&gt;Short Summary&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;적은 데이터셋과 단일 모델만으로 다양한 오디오 파형을 생성할 수 있어서 TTS, 음악, Voice Conversion 등 분야에서 SOTA(state-of-the-art)를 기록한 WaveNet은 화자의 Identity나 음악의 장르 등을 특징($h$)으로 추가하면 특징에 맞는 output을 산출할 수 있는 확률적 모델이자 자기회귀(AR) 모델이다.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/image/unnamed.gif&quot; alt=&quot;Causal Convolution&quot;&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;이전에 신호처리와 Image Segmentation에 사용되던 Causal Convolution은 많은 레이어를 필요로했고, Receptive Field를 확장시키기 위해서는 큰 필터가 필요하여 연산에 많은 비용이 들었다. WaveNet은 이러한 문제들을 Dilated Convolution을 사용하여  Receptive Fields를 효율적으로 넓힐 수 있었다. 그 결과 Causal Convolution보다 연산비용을 줄이고 학습속도를 시퀀셜 모델을 처리하던 RNN, LSTM보다 획기적으로 줄일 수 있었다.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;기존 16bit 정수형 시퀀스로 저장되던 음성신호들의 연산에는 각 레이어마다 65,536개의 확률 계산이 필요했는데, $\mu$-law companding transformation 방법을 사용하여 256개의 신호로 양자화시킬 수 있었다. Activation Unit Gate (LSTM에서의 input게이트와 유사) 를 사용하여 linguistic features (주파수, 음의 높낮이, 숨소리, 세기 등)들을 재현할 수 있었다.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="Short-Summary"><a href="#Short-Summary" class="headerlink" title="Short Summary"></a>Short Summary</h2><ul><li><p>적은 데이터셋과 단일 모델만으로 다양한 오디오 파형을 생성할 수 있어서 TTS, 음악, Voice Conversion 등 분야에서 SOTA(state-of-the-art)를 기록한 WaveNet은 화자의 Identity나 음악의 장르 등을 특징($h$)으로 추가하면 특징에 맞는 output을 산출할 수 있는 확률적 모델이자 자기회귀(AR) 모델이다.</p><p><img src="/image/unnamed.gif" alt="Causal Convolution"></p></li><li><p>이전에 신호처리와 Image Segmentation에 사용되던 Causal Convolution은 많은 레이어를 필요로했고, Receptive Field를 확장시키기 위해서는 큰 필터가 필요하여 연산에 많은 비용이 들었다. WaveNet은 이러한 문제들을 Dilated Convolution을 사용하여  Receptive Fields를 효율적으로 넓힐 수 있었다. 그 결과 Causal Convolution보다 연산비용을 줄이고 학습속도를 시퀀셜 모델을 처리하던 RNN, LSTM보다 획기적으로 줄일 수 있었다.</p></li><li><p>기존 16bit 정수형 시퀀스로 저장되던 음성신호들의 연산에는 각 레이어마다 65,536개의 확률 계산이 필요했는데, $\mu$-law companding transformation 방법을 사용하여 256개의 신호로 양자화시킬 수 있었다. Activation Unit Gate (LSTM에서의 input게이트와 유사) 를 사용하여 linguistic features (주파수, 음의 높낮이, 숨소리, 세기 등)들을 재현할 수 있었다.</p></li></ul><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><ul><li>WaveNet은 음성의 파형을 생성하는 모델.</li><li>과거의 음성 데이터 $x_1, x_2, … , x_{t - 1}$ 가 주어졌을 때 $t$ 시점을 기준으로 $x_t$ 라는 데이터가 음성으로써 성립할 확률 $P(x_1, …, x_{T-1}, x_T)$ 을 학습한 확률론적(probabilistic) 모델이자 자귀회귀(autoregressive) 모델이다.</li><li>TTS(Test-To-Speech)에 적용할 때는 SOTA(당시 2016년)를 달성했고 영어와 중국어에서 사람의 음성만큼 자연스러웠음.</li><li>WaveNet은 다양한 화자의 음성적 특징들을 동일한 정확도로 감지할 수 있음.</li><li>음악을 학습했을 때도 사실적인 음악 파형들을 생성했음.</li></ul><p>Waveform을 결합확률분포로 표현 -&gt; Conv Layer를 쌓아서 모델링하겠다.</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>WaveNet은 PixelCNN 구조를 기반으로 한 Audio 생성모델이다. </p><h4 id="WaveNet의-특징"><a href="#WaveNet의-특징" class="headerlink" title="WaveNet의 특징"></a>WaveNet의 특징</h4><ul><li>WaveNet은 이전에는 TTS 분야에서 불가능했던 자연스러운 음성 신호를 생성할 수 있다.</li><li>long-range temporal dependencies를 해결하기 위해 <code>dilated causal convolution</code> 을 개발했으며, receptive fields를 매우 크게 넓힐 수 있었다.</li><li>단일 모델로 다양한 음성을 생성할 수 있다.</li><li>적은 음성인식 데이터셋으로도 좋은 성능을 낼 수 있으며 TTS, 음악, Voice Conversion 등 여러 분야에 응용될 수 있다.</li></ul><h2 id="WaveNet"><a href="#WaveNet" class="headerlink" title="WaveNet"></a>WaveNet</h2><p>WaveNet은 Audio의 파형을 직접적으로 생성할 수 있다.</p><p>파형 $x = \{x_1, \ …,\  x_T\}$  의 결합확률은 아래와 같이 조건부확률의 곱으로 표현될 수 있다.</p><script type="math/tex; mode=display">P(x_1, ..., x_{T-1}, x_T) = \prod_{t=1}^{T} P\left(x_{t} \mid x_{1}, x_{2}, \ldots, x_{t-1}\right)</script><p>즉, 각각의 오디오데이터 $x_t$ 는 모든 이전 timestep을 바탕으로 조건부 확률이 계산되고, 그 확률을 이용하여 다음 시점의 오디오를 생성할 수 있다.</p><p>PixelCNN과 유사하게 조건부확률분포는 Convolution Layer의 스택으로 모델링된다. 여기에는 Pooling Layer가 없고 모델의 output은 input의 차원과 동일하다. 모델은 현재 시점의 데이터 $x_t$ 에 대해서 softmax와 최적화를 거쳐 다항분포를 산출한다.</p><h3 id="Dilated-Causal-Convolutions"><a href="#Dilated-Causal-Convolutions" class="headerlink" title="Dilated Causal Convolutions"></a>Dilated Causal Convolutions</h3><p><img src="/image/image-20210819110421508.png" alt=""></p><p>WaveNet의 핵심은 Causal Convolutions이다. 이것을 사용함으로써 데이터의 순서를 훼손하지 않을 수 있으며 그와 동시에 $P(x_{t+1}\ | \ x_1,\ …, \ x_t)$ 는 미래의 timestep인 $x_{t+1},\ x_{t+2},\ …,\ x_T$ 에 의존하지 않는다.</p><p>Causal Convolution의 핵심은 마스크텐서를 구성하고 convolution kernel에 적용하기 전에 mask와 elementwise 곱 연산을 수행하게 된다.</p><p>실측된 x를 알고있기 때문에 학습 시점에서 모든 타임스텝의 조건부확률은 병렬적으로 계산된다. 모델은 음성을 sequential한 확률로 만드는데, 각각의 예측값들이 산출된 이후에는 다음 예측으로 샘플들이 되돌아간다. Causal Convolutions 모델은 반복되는 connection들을 가지고 있지 않기 때문에 매우 긴 시퀀스를 적용하더라도 RNN보다 학습이 빠르다. Causal Convolution의 문제점 중 하나는 많은 레이어를 필요로하며, receptive field 를 확장시키기 위해 큰 필터가 필요하다는 것이다. 해당 논문에서는 엄청난 연산비용의 증가 없이 대규모의 receptive fields를 위해 dilated convolutions을 사용했다.</p><p>dilated convolution (옆으로 팽창한 합성곱)은 특정 스텝에서의 input을 스킵함으로써 그 길이보다 큰 영역에 필터를 적용시킨 합성곱이다. 이것은 Convolution에서의 pooling이나 strides와 비슷하지만 input과 output의 차원을 동일하게 하고 훨씬 효율적이다. </p><p><img src="/image/image-20210819112432243.png" alt=""></p><p>Dilated Convolution은 이전에 신호처리나 Image Segmentation에 사용되었다.</p><p>해당 논문에서는 $1, 2, 4, \ …, \ 512, 1, 2, 4,\ …, \ 512, 1, 2, 4, \ …, \ 512$ 총 30개의 dilations를 사용하였다.</p><h3 id="Softmax-Distributions"><a href="#Softmax-Distributions" class="headerlink" title="Softmax Distributions"></a>Softmax Distributions</h3><p>조건부 확률 분포를 모델링할 때의 접근방법 중 하나는 Mixture Density Network 혹은 MCGSM(mixture of conditional Gaussian scale mixtures) 혼합모델이었다. 그러나 PixelCNN 등장 이후 단순 연속된 데이터(이미지 픽셀이나 오디오 샘플 등)에도 Softmax 분포가 대체로 성능이 더 좋다고 알려졌다. 이 이유 중 하나는 shape에 대한 가정이 없기 때문에 다항분포가 더 유연하고 쉽게 임의의 확률분포를 모델링할 수 있다는 것이다.</p><p>원시 오디오샘플은 전형적으로 16비트 정수형 시퀀스로 저장되기 때문에, softmax layer는 매 타임스텝마다 65,536개의 확률을 다뤄야 할 필요가 있다. 이것을 더 다루기 쉽게 하기 위해 $\mu$-law companding transformation(ITU-T) 방법을 적용했고, 256개로 양자화시킬 수 있었다.</p><script type="math/tex; mode=display">f\left(x_{t}\right)=\operatorname{sign}\left(x_{t}\right) \frac{\ln \left(1+\mu\left|x_{t}\right|\right)}{\ln (1+\mu)}\ \ \ \  (-1 < x_t < 1\  and \ \mu=255)</script><p>이러한 비선형적 양자화는 선형적인 스키마보다 상당히 의미있는 재현을 해냈다. 특히 Speech 분야에서 더욱 original과 가까운 신호를 재현해냈다.</p><h3 id="Gated-Activation-Units"><a href="#Gated-Activation-Units" class="headerlink" title="Gated Activation Units"></a>Gated Activation Units</h3><p>WaveNet에는 PixelCNN에서 사용된 Activation Unit Gate가 똑같이 사용된다.</p><script type="math/tex; mode=display">\mathbf{z}=\tanh \left(W_{f, k} * \mathbf{x}\right) \odot \sigma\left(W_{g, k} * \mathbf{x}\right)</script><ul><li>$*$: convolution operator</li><li>$\odot$: element-wise multiplication</li><li>$\sigma$: sigmoid</li><li>$k$: layer index</li><li>$f\ and\ g$: filter and gate.</li><li>$W$: learnable convolution filter.</li></ul><h3 id="Residual-And-Skip-Connections"><a href="#Residual-And-Skip-Connections" class="headerlink" title="Residual And Skip Connections"></a>Residual And Skip Connections</h3><p><img src="/image/image-20210819125922828.png" alt=""></p><p>Residual 과 Skip Connections가 네트워크에 사용되었는데, 이 덕분에 수렴하는 속도가 빨라지고 더욱 깊은 신경망도 학습이 가능해졌다. Residual과 Skip-Connections 들이 신경망마다 Stacking 되어 있다.</p><h3 id="Conditional-WaveNets"><a href="#Conditional-WaveNets" class="headerlink" title="Conditional WaveNets"></a>Conditional WaveNets</h3><p>WaveNet은 Conditional Modeling $P(x|h)$ 가 가능하다. 이것은 새로운 input인 특징(h)를 추가하면 특징에 맞는 output을 산출할 수 있다는 것이다.</p><script type="math/tex; mode=display">p(\mathbf{x} \mid \mathbf{h})=\prod_{t=1}^{T} p\left(x_{t} \mid x_{1}, \ldots, x_{t-1}, \mathbf{h}\right)</script><p>새로운 input인 $h$ 가 추가적으로 들어왔을 때의 조건부 확률은 위의 수식을 따르게 된다. 이러한 상황은 예를들어 다중화자가 있는 환경에서, 우리는 화자를 한 명 선택하고 모델에 추가적인 input으로 화자의 특징을 전달하게 될 때 발생하게 된다. TTS에서 텍스트 정보를 추가적인 인풋으로 주는 것과 유사하다.</p><p>다른 인풋에 대한 모델의 조건부확률은 Global Conditioning 과 Local Conditioning 두 가지 방법으로 나눌 수 있다.</p><ul><li>Global Conditioning: 시점에 따라 변하지 않는 조건 정보를 추가하는 방법<script type="math/tex; mode=display">\mathbf{z}=\tanh \left(W_{f, k} * \mathbf{x}+V_{f, k}^{T} \mathbf{h}\right) \odot \sigma\left(W_{g, k} * \mathbf{x}+V_{g, k}^{T} \mathbf{h}\right)</script><ul><li>모델로부터 여러 화자의 음성을 생성하고 싶을 때 사용하는 방법으로 화자의 특징(h)를 모든 시점에 동일하게 추가하여 모델을 학습시킨다.</li><li>화자의 특징은 timestep 별로 변하는 정보가 아니기 때문에 전역적으로 모든 시점에 영향을 주게 된다.</li></ul></li><li>Local Conditioning: 시점에 따라 변하는 조건 정보를 추가하는 방법<script type="math/tex; mode=display">\mathbf{z}=\tanh \left(W_{f, k} * \mathbf{x}+V_{f, k} * \mathbf{y}\right) \odot \sigma\left(W_{g, k} * \mathbf{x}+V_{g, k} * \mathbf{y}\right) \\ y=f(h)</script></li></ul><h3 id="Context-Stacks"><a href="#Context-Stacks" class="headerlink" title="Context Stacks"></a>Context Stacks</h3><p>Receptive Field를 증가시키는 방법으로</p><ol><li>Dilation 숫자를 증가시킴</li><li>더 많은 Layer 구성</li><li>더 큰 Filter 사용</li><li>Dilation factor 증가</li></ol><p>등이 있다.</p><p>이것을 보완하는 접근법이 바로 context stack이다.</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Multi-Speaker-Speech-Generation"><a href="#Multi-Speaker-Speech-Generation" class="headerlink" title="Multi-Speaker Speech Generation"></a>Multi-Speaker Speech Generation</h3><p>단일 WaveNet이 여러 화자의 특징을 포함한 음성을 생성할 수 있는지를 검증함.</p><h3 id="Text-To-Speech"><a href="#Text-To-Speech" class="headerlink" title="Text-To-Speech"></a>Text-To-Speech</h3><p>Linguistic Features (음소, 음소길이, 주파수 등)을 추가로 학습한 후 HMM, LSTM-RNN Model과 비교.</p><p>Mean Opinion Score(MOS) Test 결과 압도적으로 WaveNet이 가장 높은 점수 획득.</p><h3 id="Music"><a href="#Music" class="headerlink" title="Music"></a>Music</h3><p>영어와 중국어 음악 데이터셋을 활용해 LSTM-Concat, HMM과 비교.</p><h3 id="Speech-Recognition"><a href="#Speech-Recognition" class="headerlink" title="Speech Recognition"></a>Speech Recognition</h3><p>생성모델이지만 음성인식 과제로도 실험.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>본 논문은 음성 파형 수준에서 직접적으로 음성을 생성하는 심층 생성 모델 WaveNet을 제안했다. WaveNet은 AutoRegressive하며 Receptive Fields를 확장시킨 Dialted Convolutions과 결합한 Causal Filters를 사용하고 있다. 여기에서 WaveNet이 입력에 따라 global (화자의 특징) 혹은 local (언어적 특징) 적으로 어떻게 조절되는지를 보여주었다. WaveNet을 TTS에 적용했을 때 현존 최고의 성능을 보여주었으며, Music과 Speech Recognition에도 매우 좋은 결과를 보여주었다.</p><h2 id="출처"><a href="#출처" class="headerlink" title="출처"></a>출처</h2><ul><li><a href="https://joungheekim.github.io/2020/09/17/paper-review/">https://joungheekim.github.io/2020/09/17/paper-review/</a></li><li><a href="https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623">https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623</a></li><li><a href="https://www.youtube.com/watch?v=GyQnex_DK2k&amp;t=1068s">https://www.youtube.com/watch?v=GyQnex_DK2k&amp;t=1068s</a></li><li><a href="https://www.youtube.com/watch?v=CqFIVCD1WWo&amp;t=294s">https://www.youtube.com/watch?v=CqFIVCD1WWo&amp;t=294s</a></li></ul>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Paper-Review/">Paper Review</category>
      
      
      <category domain="https://l-yohai.github.io/tags/WaveNet/">WaveNet</category>
      
      
      <comments>https://l-yohai.github.io/WaveNet-Review/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>프로그래머로 산다는 것 - 라이엇 게임즈 유석문 CTO</title>
      <link>https://l-yohai.github.io/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EB%A1%9C-%EC%82%B0%EB%8B%A4%EB%8A%94-%EA%B2%83-%EC%9C%A0%EC%84%9D%EB%AC%B8-CTO/</link>
      <guid>https://l-yohai.github.io/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EB%A1%9C-%EC%82%B0%EB%8B%A4%EB%8A%94-%EA%B2%83-%EC%9C%A0%EC%84%9D%EB%AC%B8-CTO/</guid>
      <pubDate>Tue, 17 Aug 2021 09:01:26 GMT</pubDate>
      
      <description>&lt;h2 id=&quot;1-개발자&quot;&gt;&lt;a href=&quot;#1-개발자&quot; class=&quot;headerlink&quot; title=&quot;1. 개발자???&quot;&gt;&lt;/a&gt;1. 개발자???&lt;/h2&gt;&lt;p&gt;2013 문화체육관광부 우수학술도서 - 프로그래머로 산다는 것(황상철, 하호진, 이상민, 김성박 - 로드북)&lt;/p&gt;</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="1-개발자"><a href="#1-개발자" class="headerlink" title="1. 개발자???"></a>1. 개발자???</h2><p>2013 문화체육관광부 우수학술도서 - 프로그래머로 산다는 것(황상철, 하호진, 이상민, 김성박 - 로드북)</p><p>해당 책이 출판되고 나서 많은 개발자들이 화를 냈음. 책 표지가 화장실에서 노트북을 하는 장면이었기 때문임. 미래도 없고 꿈도 없듯이 밤낮없이 화장실에서도 일하면서 과로로 쓰러지라는 것이냐! 라는 느낌이었기 때문. 기존 개발자들은 연봉도 오르지 않고 쇠퇴하는 경우가 많았기 때문에 프로그래머를 평가절하한 것이냐는 의견이었음. 프로그래머가 왜 되려고 하는 것인가? 대부분의 경우 프로그래밍이 어렸을 때부터 너무 재밌었기 때문에 프로그래머가 된다고 한다. 보통 모바일 게임의 경우 화장실에서도 볼일을 다보고도 계속 하듯이, 프로그래머가 되려고 한 자체는 결국에 이 일이 재밌고 즐거워서, 평생 할 수 있어서이기 때문이어야 할 것이다. 해당 책 소개를 의도한 이유는, 재밌다면 그 일을 계속 하라는 것을 이야기하고 싶어서이다. 코딩이 재밌다면 화장실이 아니라 어디에서 하든 상관없다. 하지만 나를 소진해가면서 밥벌이를 위해서라면 하지 마라는 것을 이야기하고 싶었다.</p><h3 id="1-1-개발자-or"><a href="#1-1-개발자-or" class="headerlink" title="1.1 개발자??? or ?????"></a>1.1 개발자??? or ?????</h3><p>보통 신입사원 면접을 진행할 때 겪는 일이다. 기술면접 할 때 아이스브레이킹 시간을 가지고 간단한 기술문제를 푼 이후 난이도 조절을 통해 다음단계를 진행하게 된다. 아래는 3-5년차 자바 개발자를 대상으로 면접을 할 때 냈던 문제이다.</p><p><img src="/image/image-20210817181151099.png" alt="image-20210817181151099" style="zoom:50%;" /></p><p>위의 문제를 가지고 해당 자료구조(스택)를 만들어보라는 문제를 냈다. 하지만 클래스 선언도 못하는 경우가 허다했다. 이력서는 화려한데 왜 못하지..? 라고 물어보면 “최근에는 개발보단 관리를 많이 하느라..” 라는 답변이 돌아온다. 이런 상황이 이해가 되지 않았음. ‘읭?’</p><h3 id="1-2-개발-놈-Begins-업무-할당"><a href="#1-2-개발-놈-Begins-업무-할당" class="headerlink" title="1.2 개발(놈) Begins - 업무 할당"></a>1.2 개발(놈) Begins - 업무 할당</h3><p>프로그래밍 관련 학과를 졸업하고, 현업에서의 시간도 어느정도 지났는데 왜 하지 못할까라는 고민에 대한 고찰.</p><p>회사에 처음 가서 어떤 과제를 받았다고 해보자. 정상적이지 않은 회사라고 한다면 대부분 “이 일을 언제까지 끝낼 수 있겠니? 참고로 시간이 없어.” 라면서 일을 시킨다. 무조건 그 시간안에 다 해야 하고, 심지어 충분한 레퍼런스나 리소스도 없다. 관리자가 시간만 관리하고 기술적 지원/교육도 없는 상태에 데드라인에 맞춰서 일을 끝내야 하는 상황에 놓였다고 상상해봐라..</p><ol><li>검색</li><li>복사 &amp; 붙여넣기</li><li>되는것처럼 보일때까지 삽질.</li></ol><h3 id="1-3-개발-놈-의-탄생-주역"><a href="#1-3-개발-놈-의-탄생-주역" class="headerlink" title="1.3 개발(놈)의 탄생 주역"></a>1.3 개발(놈)의 탄생 주역</h3><p>네이버든 어떤 조직이든간에 개발 잘하는 사람들이 있는 반면 저런 사람들도 (당연히) 있을 것이다. 운이 좋으면 좋은 팀을 만나서 성장을 막 할 수 있을테고, 운이 나쁘면 저런사람들을 만나서 소진될 것이다. (운 나쁠 확률이 더 많음 ㅋ.)</p><p>기피해야 하는 대상</p><ol><li>나쁜 고객과 상사 - 어떤 기술을 쓰든 좋은 기술을 쓰든 관심없고 대부분은 기술능력도 없고 결과만 나오면 되길 원함. 이러면 내 인생을 낭비하게 되는 것과 다름없다.</li><li>탐욕스러운 회사 - 개발자들은 백날천날 같은 기술만으로는 살아남을 수 없다. 새로운 것을 배우고 적용해봐야 실력이 늘며 성장하는 것인데, 탐욕스러운 회사는 똑같은 일만 시킨다. 그렇게 개발자들은 소진될 것이다.</li><li>비협조적인 동료 - 서로 긍정적인 영향을 주고받을 수 있으면 좋은데, 대부분의 동료는 내가 하는 일에 관심이 없을 것임. 더 좋은 방법이 있다고 했을 때 대부분은 그 사실을 설득시키기가 어려움. 거부감이 많기 때문임. 직장생활을 할 때 당연히 겪게되는 문제이다.</li></ol><p>하지만 위의 것들은 <code>통제할 수 없는 외부 요인</code>이다. 통제할 수 없는 것들은 깨끗하게 포기하고 스스로를 지키고 성장시킬 수 있는 방법을 터득해야만 한다.</p><h3 id="1-4-개발자의-필수능력"><a href="#1-4-개발자의-필수능력" class="headerlink" title="1.4 개발자의 필수능력"></a>1.4 개발자의 필수능력</h3><p>깔끔한 코드</p><ul><li>사람이 이해하기 쉬운 코드</li><li>변경이 용이한 코드</li><li>유지보수 비용이 낮은 코드</li></ul><p>하지만 생각보다 코드 자체의 가독성에 관심없는 사람이 많다. 가독성 없는 코드에 오류가 발생한 뒤 그것을 내가 고쳐야한다..? 어떤 기업에서는 다른 사람이 짠 코드들을 다른 사람이 고치는 경우가 많다. 즉 쉽게 고칠 수 있는 코드가 필요하다. 따라서 개발자라면 매우 깔끔한 코드를 짤 수 있어야 한다. 컴파일 오류든 실행오류든 이딴거 필요없다. 사람이 이해할 수 있는 코드를 작성해야 한다. 이렇게 되면 코드 변경이 용이해져서 버그 수정도 편하고 얼마든지 다른 기능도 쉽게 추가할 수 있다.</p><p>신입개발자들 이력서를 보고 자신을 어떤 개발자라고 표현하는지를 보면 굉장히 모호할 때가 많다. 개발자로서 자신을 어필하기 위한 가장 좋은 방법은, ‘코드 깔끔하게 잘 짠다. 변경하기 용이한 코드를 잘짠다. 초등학생도 이해한다.’ 와 같은 말을 쓰는 것이다.</p><p>“코드를 지우는 사람입니다.” 라는 말에 엄청난 인상을 준 사람이 있었다. 개발자는 코드를 생산한다고 흔히 생각하는데, 그 분은 불필요한 코드를 줄이고 유지보수할 수 있는 코드를 짜는 사람이라는 이야기가 얼마나 내공이 높은 것인가.</p><p>적절한 논리력</p><ul><li>원리 탐색 능력</li><li>제약조건을 고려한 해법</li><li>단순한 디자인</li></ul><p>“절대 죽지 않는 웹서버를 만들 것이다.” 와 같은 극단으로 가는 것은 불필요하다. 엔지니어라고 한다면 현재 가용가능한 리소스를 가지고 적절한 동작을 하게 하는 것이 필요하다. 나는 이 기술을 많은 사람들이 썼으면 좋겠다고 생각해서 백만명을 수용할 수 있는 서버를 만들었는데 막상 오픈했더니 사용자가 열 명이다. 이러면 말짱도루묵이지 않은가. 따라서 적절한 리소스로 적절하게 문제를 해결하는 것이 가장 좋다. 원리를 잘 탐색하고 무작정 만드는 것이 아니라 제약조건을 고려한 해답을 찾고 단순한 디자인을 만들 수 있는 것이 가장 좋다.</p><h3 id="1-5-깔끔한-코드-작성법"><a href="#1-5-깔끔한-코드-작성법" class="headerlink" title="1.5 깔끔한 코드 작성법"></a>1.5 깔끔한 코드 작성법</h3><p>ATDD (Acceptance Test Driven Development)</p><p>TDD(Test Driven Development)</p><ul><li>두 가지를 모두 잘 해야 한다.</li><li>둘 중에 하나를 선택해야 한다고 하면, 당연히 TDD가 될 것이다.<ul><li>테스트 코드를 만들어놓고 테스트 코드에 걸맞는 양의 코드만 생산을 하는 것임.</li></ul></li></ul><ol><li>사용하는 코드만 만들기 (Caller Create)</li><li>리팩토링 (Refactoring)</li><li>코드 읽기 (Code Review)</li></ol><p>이 세 가지 원칙만 꼭 기억하자.</p><p>PR에서 변수명, 함수명도 제안하는 것이 매우 좋음. 대부분 코드리뷰에서 버그를 찾으려고 하는데, 보통의 경우 눈으로 봤을 때 버그를 찾을 수 있으면 매우 이상한 것임. 그러진 말고 ‘짧고 간결하게’ 어떤 작업을 했는지 적는 것이 좋음.</p><h3 id="1-6-적절한-논리력"><a href="#1-6-적절한-논리력" class="headerlink" title="1.6 적절한 논리력"></a>1.6 적절한 논리력</h3><ol><li>알고리즘과 데이터 구조</li><li>단순한 디자인</li><li>진화적 디자인</li><li>협업</li><li>기술 벤치마킹</li></ol><p>보통 가장 복잡한 케이스부터 고려를 많이 한다. 그러다보면 디자인도 매우 복잡해진다. 대부분의 경우 복잡한 문제들은 매우 작은 문제들의 집합이기 때문에, 복잡한 문제를 처음부터 고민한 것에 비해 작은 문제들부터 작성하는 것이 더 좋음.</p><p>코드 리뷰를 할 때는 공격적인 피드백이 매우 중요함. 그리고 계속해서 새로운 기술, 문제를 하기 위해 더 좋은 기술이 있는지 벤치마킹 해야 한다.</p><h3 id="1-7-실천법"><a href="#1-7-실천법" class="headerlink" title="1.7 실천법"></a>1.7 실천법</h3><ul><li>꾸준한 연습(Daily Practice)</li><li>매일 몸값 올리는 시간을 가져라.</li></ul><p>취준 입장에서 우리의 동기부여가 무엇일까? 회사에서는 연봉올리는 것 등. 이런 동기가 있으면 시간 투자를 할 이유가 있다. 우리를 동기부여해주는 동기가 무엇인지 찾아보자.</p><ul><li>멀리 가고 싶다면 함께 가라</li></ul><p>혼자 실천하는 것은 어려운데 동료와 함께면 쉽다. 하기싫어하는 사람 아무나 붙잡고 하면 당연히 안됨!</p><ul><li>현재 필요한 만큼만 하라</li><li>간단하게 하라</li></ul><p>무엇을 하나 만들어야 한다고 하면 미리 하지 말고 현재 딱 필요한 만큼만 해라.</p><h2 id="2-좋은-개발자"><a href="#2-좋은-개발자" class="headerlink" title="2. 좋은 개발자???"></a>2. 좋은 개발자???</h2><h3 id="2-1-좋은-OO-개발자"><a href="#2-1-좋은-OO-개발자" class="headerlink" title="2.1 좋은 OO 개발자"></a>2.1 좋은 OO 개발자</h3><p>“좋은” OO 개발자 (서버, 웹, 클라이언트, 임베디드, 모바일, 게임 등..)</p><p>불과 몇 년 전까지만 해도 AI 개발자는 취업할 곳이 없었음. 이렇듯 분야가 다양하고 시간 변동이 크다. 어느순간 표준화된 AI 모델이 나오고 나서 파인튜닝하는 것처럼 특별해지지 않을 수 있음. 나는 AI가 전부다! 라는 생각은 너무 위험함. 몇 년 뒤에는 너무 보편적인 기술이 될 수도 있기 때문. 도메인과 함께 망할 수 있기 떄문에 나는 어떤 개발자가 되려는 것에 유동적일 수 있어야 한다. 물론 도메인에 특별한 전문성이 있는 것도 중요하지만 리스크가 있음.</p><p>“좋은” 이라는 것은 공유와 협업이 잘 이루어지는 사람을 뜻한다. 협업 전혀 안되는데 코딩 졸라 잘하면 그 팀 내에서 주변에 있던 모든 사람들이 회사를 뛰쳐 나간다. 회사 입장에서는 굉장히 위험한 일이다. 그래서 많은 회사들이 협업 잘하는 개발자를 원하게 된다. 즉 좋은 개발자라는 것은 공유를 잘하고 협업이 잘 이루어지는 영역 내에서 말할 수 있다.</p><h3 id="2-2-공유하는-이유"><a href="#2-2-공유하는-이유" class="headerlink" title="2.2 공유하는 이유"></a>2.2 공유하는 이유</h3><p>잘난척하는 거 절대 아님.</p><p>공유를 하는 가장 중요한 이유는, 주변이 똑똑해져야 내가 편해지기 때문이다.</p><ul><li>사고를 수습하는 일이 줄어듬</li><li>중요한 일을 할 여유를 가질 수 있음.</li></ul><p>주변이 똑똑하지 않으면 나의 일은 그 사람들이 친 사고를 급급하게 해결하는 것에 국한될 것이다. 그렇기 때문에 주변을 나보다 똑똑하게 만들기 위해 투자하는 것이 좋음.</p><p>투자를 하고 나면 좋은 평판을 얻을 수 있고, 그 좋은 평판은 나를 밥먹여준다. 이직을 할 때라던지. 좋은 평판을 받았다고 하면 주변의 추천도 많이 들어오고, 주변의 덕을 볼 확률이 올라간다.</p><h3 id="2-3-공유-대상"><a href="#2-3-공유-대상" class="headerlink" title="2.3 공유 대상"></a>2.3 공유 대상</h3><p>무엇이든.</p><p>잘난 것만 공유해야 할 것 같지만, 실패를 공유하는 것도 중요하다. 내가 이렇게 해봤더니 개망했어. 라는 이야기를 공유하면 다른 사람들은 그 방법대로 하지 않을 수 있기 때문이다. 실패했다는 것은 창피한 것이 아니고, 그만큼 투자를 해서 값진 시행착오를 얻게 된 것이다.</p><p>새로운 기술을 공유했다라고 하면 그 기술을 시도해보고 장단점에 대해 명확하게 설명할 수 있어야 하는 정도는 되어야 한다.</p><h3 id="2-4-협업의-전제조건-상대를-이해하자"><a href="#2-4-협업의-전제조건-상대를-이해하자" class="headerlink" title="2.4 협업의 전제조건: 상대를 이해하자"></a>2.4 협업의 전제조건: 상대를 이해하자</h3><p>고슴도치도 제 새끼는 함함하다.</p><ul><li>기획자의 입장에서, 기획자는 고객들이 원하는 것을 바탕으로 디자인을 만들거나 문서를 만들어낸다. 이러한 기획문서의 산출물을 개발자 QA 마케터 등 모두에게 공유하게 된다. 대부분 오 이거 쩐다!라고 하면 좋겠지만, 대부분 그렇지 않고 ‘이거 왜 해야 돼요?’라는 말을 듣게 된다. 기획 아이디어를 외부에 공유했는데 칭찬은 커녕 욕만 들으면 협업하기 빡친다.</li><li>개발자의 입장에서, 코드라는 산출물을 공유했는데 ‘이거 이상해요! 버그 있어요!’라는 말을 많이 하게 되는데, 이것도 개빡침. 열 개 잘되고 하나 안되면 열 개 잘된건 무시하고 하나 안된거에 대해서 콕 지적하고 지랄임.</li><li>QA의 입장에서, 테스트케이스나 버그레포트를 썼는데 ‘그럴리가 없는데?’와 같은 반응을 얻게 됨.</li></ul><h3 id="2-5-협업의-필수요소"><a href="#2-5-협업의-필수요소" class="headerlink" title="2.5 협업의 필수요소"></a>2.5 협업의 필수요소</h3><p>누구든 산출물에 대해 ‘이상하다’라는 말을 듣게 되면 이 새끼가 날 공격하나? 라는 생각이 들 수 밖에 없음.</p><p>자아존중감</p><ul><li>자신이 존중받을 가치가 있다고 믿음.</li><li>있는 그대로의 자신을 인정함.</li><li>타인의 부정적 견해에 크게 영향 받지 않음.</li></ul><p>결국에 협업을 잘하기 위해서는 자아존중감이 있어야 한다. 실패를 하건 실수를 하건 성공적인 일을 했건 내가 누구보다 위대하고 누구보다 쓰레기고 이런게 절대 아니다. 어떤 상황에서든 나는 나를 사랑할 수 있는 믿음이 있어야 함. 이것이 높으면 사람들이 협업이 잘 된다. 보는 개인의 입장에서 잘 상처받지 않고 상처받더라도 쉽게 회복할 수 있음.</p><p>우리는 앞으로 사고 졸라 칠거고 그것이 전혀 나쁜것이 아님. 지극히 정상적인 것인데 그런 사고들을 잘 겪고 이겨낼 수 있어야 한다.</p><h2 id="3-좋은-개발자"><a href="#3-좋은-개발자" class="headerlink" title="3. 좋은 개발자!!!"></a>3. 좋은 개발자!!!</h2><p><img src="/image/image-20210817185648800.png" alt="image-20210817185648800"></p><p>이미지출처: <a href="http://ifather.tistory.com/category/재밌는세상?page=2">http://ifather.tistory.com/category/재밌는세상?page=2</a></p>]]></content:encoded>
      
      
      <category domain="https://l-yohai.github.io/categories/Boostcamp-AI-Tech/">Boostcamp AI Tech</category>
      
      <category domain="https://l-yohai.github.io/categories/Boostcamp-AI-Tech/Master-Class/">Master Class</category>
      
      
      
      <comments>https://l-yohai.github.io/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EB%A1%9C-%EC%82%B0%EB%8B%A4%EB%8A%94-%EA%B2%83-%EC%9C%A0%EC%84%9D%EB%AC%B8-CTO/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
