{"pages":[{"title":"About Me","text":"- Email: yohan9612@yonsei.ac.kr - Phone: +821063236913 - Github: l-yohai - Linkedin: l-yohai I am currently working as a NLP Research Scientist at Riiid.I am a graduate of Yonsei University with a Bachelor’s degree in German Language &amp; Literature and Cognitive Science. My research interests are as follows: Instruction Tuning for Large Language Models Text Qualification, Scoring Open-Domain Dialogue Chatbot, Question Answering Multimodal Learning Knowledge Distillation Education Bachelor’s Degree German Language &amp; Literature, Cognitive ScienceYonsei University (2015 ~ 2022) Work Experience [Research Scientist (NLP)] Riiid (2023.07 ~ ) [NLP Engineer] TUNiB (2021.12 ~ 2023.02) [Researcher] NRF - Language Information Processing 2020 (2019.07 ~ 2019.09) Extracurricular Activities [Student] Boostcamp AI Tech - NLP Track (2021.07 ~ 2021.12) [Student] Innovation Academy - 42 Seoul (2020.01 ~ 2021.03) Awards and Honors Grand Prize (President’s Award, Sungkyunkwan University), 3rd Annual University Student AI x Bookathon, 2021.11 3rd Place (Ministry of Science and ICT Minister’s Award), 2022 AI Grand Challenge: Policy Support AI, 2023.01 2nd Place in Main Competition(CJ Logistics CEO’s Award), 1st Place in Preliminary Round, 2023 CJ Logistics Future Tech Challenge, 2023.09","link":"/about/index.html"}],"posts":[{"title":"AI Competition의 협업을 위한 플래닝가이드","text":"🍡 떡볶이조 팀원들 🔥 김다영 김아경 문하겸 박지민 이요한 전준영 정민지 7명의 팀원이 2주동안 잠도 거의 안자가면서 올인했던 P Stage의 ‘마스크 착용 분류 대회’에 대한 플래닝 가이드입니다. 저희는 각자 실력도 다 다르고 협업문화도 잘 모르는 상태로 대회에 임하였습니다. 그러다 보니 뭘 해야할지 모르는 순간들도 너무 많았고, 여러 시행착오를 겪었기 때문에 다음 기수분들 혹은 인공지능 대회 프로세스 중 협업관리를 어떻게 하는 것이 좋을지 저희의 경험을 조금 나누어볼까 합니다. 당연히 처음엔 너무 막막했어요. 이게 잘 될까? 싶었던 대회였는데, 계속해서 시간과 에너지를 쏟으니 별로 경험이 없는 사람들 7명이서 38개 조 중 13등을 하게 되었습니다. 물론 순위권에 들었으면 좋았겠지만, 대회는 처음인 사람들끼리는 나름 성공적인 결과였다고 생각합니다. 이러한 저희 조의 이야기들이 어떤 형태로든 도움이 되고, 좋은 레퍼런스가 되기를 바랍니다. 🦆 대회 전략 수립 및 협업환경 구축 이번생에 협업은 처음이라.. 협업을 위한 적절한 채널을 설정하고, 최소한의 Git에 익숙해지자. 대회의 목표를 수립하고, 핵심 가치를 팀원들과 공유하자. 🔥 대회 프로세스 소개 및 목표 설정인공지능 대회는 간단하게 주어진 데이터셋으로 모델을 훈련시키고 성능만 올리면 되는 단순한 프로세스로 이루어져 있습니다. 하지만 세부적으로 보면 데이터 정제 및 전처리와 Labeling -&gt; 실험을 위한 베이스라인 코드 작성 -&gt; 모델링과 실험 -&gt; 제출 의 프로세스를 가지고 있고, 보통 데이터셋 구축과 베이스라인 코드가 주어지는 경우가 많습니다. 이번 P Stage의 경우 지난 U Stage를 통한 학습이 끝나자마자 시작되었기 때문에 이러한 프로세스를 익히는 차원에서 데이터만 먼저 공개되고 베이스라인은 1주일이 지나서야 공개가 되는 형태였습니다. 따라서 빠른 실험을 위해서는 데이터셋 구축 작업과 베이스라인이 절실했고 그것을 하기 위해서는 팀원들과의 협업 방식에 대한 상의를 하고 대회 전략을 수립해야 했습니다. 지금 생각해보면 무모했으나 저희 조의 목표는 무조건 1등! 이었고 그에 따라 2주간의 대회 기간 중 1주차는 베이스라인코드 작성과 데이터 전처리에 집중하고, 2주차에는 성능향상을 위한 실험들을 전략으로 잡았습니다. 🤔 협업을 위한 채널 정하기목표가 잡혔으니 이제는 협업을 어떻게 할 건지 결정해야 했습니다. 사전에 사용하던 채널로는 화상회의를 위한 줌, 공식적인 이야기를 위한 슬랙 채널, 그리고 코드관리를 위한 깃허브의 팀 Repo가 있었습니다. 하지만 이것만으로는 프로젝트 관리나 일정관리를 하기가 애매했어서, 새로운 채널을 만들 필요가 있었습니다. 해당 후보로는 Github Project의 대시보드, Jira, Notion 등이 있었는데 최종적으로는 Github Project를 사용하기로 결정했습니다. 칸반보드 기반의 Dashboard를 생성하면 TODO를 시각적으로 관리하기가 용이하고, Milestone이나 Label, Assignee 등 여러 장치들을 직관적으로 사용할 수 있기 때문입니다. 그리고 다른 템플릿 역시 다들 처음이지만, 사전학습 비용이 Github Project가 가장 적을 것 같았고, 채널이 다양해지면 다양할수록 팀원들의 혼란이 가중될 것 같다는 이유 또한 있었습니다. 저희는 위의 사진처럼, Github Project의 Kanban Dashboard를 사용했고, 매일 아침 9시 30분마다 당일 작업할 TODO 리스트를 업데이트하는 식으로 진행했습니다. 그리고 각각의 이름을 가진 라벨을 만들어서 누가 해당 TODO를 작업중인지 표시하도록 했습니다. 이렇게 서로의 할일과 작업 결과를 공유할 수 있는 채널을 만들어놓고 어떻게 사용할지에 대한 Flow를 미리 정해놨다는 것 자체가 저희같은 협업 초보들에게는 큰 성과이자 시작점이었습니다. 채널마다 역할과 책임을 명확히하고, 해당 채널을 어떻게 사용할건지 팀원들과의 상의가 지속해서 이루어질수록 더욱 긴밀한 협업이 진행될 수 있음을 알게 되었습니다. 💻 코드관리와 Git (실패 🤮)협업을 위한 채널 설정이 끝나고 마지막으로 코드관리를 어떻게 할 것인지 정해야했습니다. 1주차는 개인제출이 가능하다곤 하지만 결국에는 팀프로젝트이기 때문에 모두가 같은 베이스라인 코드를 가지고 중복된 실험을 하지 않고, 더 좋은 성능을 가진 코드가 나왔을 때 바로바로 버전업을 할 수 있도록 Git Flow를 따라서 협업을 진행하기로 결정했었습니다. 하지만 막상 대회 시작 후 베이스라인 코드가 너무 어렵고, 다른 할 것들이 많아서 정신이 없었으며 그리고 협업이 처음인 저희는 Git 자체가 너무나도 어려웠기 때문에 결국에는 Git Flow를 활용한 코드관리에 실패하였습니다. 하지만 Git을 활용하지 않으면서 점점 대회의 끝이 다가올수록 서로의 코드가 점점 달라졌습니다. 결국에는 어떤 한 명이 Best Score를 달성했을 때 그 방법을 공유받고 다른 사람들이 그 모델을 같은 방식으로 학습시켰을 때 다른 모두가 재현할 수 없다는 문제가 생겼습니다. 그래서 지금 생각으로는 다시 대회 초반부터 진행하게 된다면 무조건 Git을 활용해서 버전관리에 신경쓰면서 작업을 하고 싶습니다. 성능을 올린 결과를 계속해서 베이스라인을 삼고 해당 코드에서 다른 방법들을 실험하는 쪽으로요. 다른 동료분이 성능을 올렸다고 했을 때 “저거 어떻게 했지..”라는 좌절스러운 생각도 많이 했거든요.. ㅎㅎ; 그렇게 한두명씩 뒤쳐지게 되면 추후에는 더 많은 실험을 할 수 없게 돼요. 또한 대회에서는 성능 개선이 우선시 되기 때문에 시간을 오래 투자해야하는 Git flow까지는 아니더라도 간단한 flow를 사전에 정하는 것이 좋다고 생각합니다. 실험하기도 바쁜데 커밋 메세지 신경쓰고 PR 템플릿 찾아보고 있으면 빡치잖아요. 🤮 ❤️‍🔥 핵심가치를 공유했다면 어땠을까?협업에 있어서 가장 중요한 것 중 하나는 핵심 가치를 공유하는 것입니다. 많은 실험이 이루어지고 코드 규모가 커질수록 정말 정신없어지는 상태에서 수많은 의사결정을 해야합니다. 그러면 자연스럽게 의사결정 비용이 높아질 수 밖에 없고, 논의를 위한 시간이 길어지면 길어질수록 피로도는 높아지고, 일관되지 않은 코드를 짜고, 공유해야 할 것을 공유하지 못하는 등 어떻게든 본래의 목적을 달성할 순 있어도 협력이란 관점에서 정말 좋지 않은 일이 일어날 수 있겠죠. 처음에 정했던 목적 달성을 위해 조금 귀찮았어도 여러 가치중에서 우선순위를 정했으면 어땠을까 하는 아쉬움이 남습니다. 사실 대회 끝물로 갈 수록 여러 의사결정이 필요할 때 결론을 짓지 못하고 흐지부지 끝나게 되는 등 첫 주 차때의 열정이 점점 사라지고 있었거든요. 감정과 시간의 소모를 줄이는 일, 그것을 해결하려고 하지 않았던 것이 아쉬움으로 남는 것 같습니다. 구성원의 만족, 빠른 실험, 가독성, 일관성, 단순성, 통제가능성, 학습가능성, 안정성 등 협업에 필요한 가치들을 리스트업하고 그것들의 우선순위를 매겼으면 더욱 행복한 협업이 되었을 수도 있겠다는 생각이 듭니다. 🔎 EDA와 Custom Dataset 구축 데이터, 그게 뭔데? EDA의 중요성을 잊지 말고, 다양한 실험을 위해 Dataset을 다양하게 구성해보자. 🌄 EDA와 데이터 정제대회를 시작했다면 가장 먼저 해야할 중요한 일을 하나만 꼽으라고 한다면 그것은 바로 무조건 데이터의 형태를 파악하는 것입니다. 데이터의 분포는 어떻게 되는지, 데이터에 노이즈는 없는지, 잘못 Labeling 된 데이터는 없는지 등을 판단하고, 그렇게 판단한 결과를 토대로 데이터를 정제해야 원활한 학습이 이루어질 수 있기 때문입니다. 저희는 데이터를 지속적으로 관찰하고 분석하는 다양한 전략을 구상하였습니다. 1. 라벨 별 데이터가 어떻게 분포되어있는지 판단하자. 저희가 EDA 과정에서 가장 중요하게 생각한 것은 데이터가 Imbalancing 하지 않은지 확인하는 것이었습니다. 어느 한 쪽으로 데이터가 쏠려있으면 모델은 데이터가 적은 라벨을 쉽게 맞추지 못할 것이기 때문입니다. 2. Evaluation Dataset의 분포는 어떻게 되어있는지 확인하자. 대회 1주차에는 하루에 개인별 10회씩 제출이 가능했습니다. 저희는 대회 초반엔 분명히 제출을 하지 못할 것이라 예상하고, 제출할 때 0부터 17까지 모든 라벨로만 Submission을 구성하여 제출했습니다. 이렇게 각 라벨별로 제출해보고 얻은 Accuracy 정보를 토대로 Evaluation 셋에 대한 정답 분포를 시각화할 수 있었습니다. 해당 방법은 이번 대회가 서버에 코드를 올려서 실행시키는 방식이 아닌, Eval Dataset을 주고 정답지만 제출하라는 방식의 대회였기 때문에 가능한 것이었습니다. 아래에서 또 이야기하겠지만, 이번 대회의 데이터셋으로는 과적합이 너무 쉽게 일어나고 Validation 데이터셋을 아무리 잘 나누어도 Validataion 관련 Score들을 신뢰할 수 없었습니다. 따라서 제출할 때 내가 도출한 정답이 어느정도의 Score일지 예상할 수가 없었습니다. 하지만 해당 방식으로 정답 분포를 알아낸 뒤 제출 전에 만든 정답지와 오차를 비교해보면 그 결과를 비슷하게 예측할 수 있었기 때문에 모델을 선택하고 제출할 정답을 고를 때 굉장히 좋은 판단 근거가 되었습니다. 3. 모든 데이터를 찍어보자. 제공받은 데이터는 사람마다 7장의 사진이 있고, 총 2700명의 사람이 있었습니다. 그렇게 총 18900개의 학습 데이터가 있었는데 저희는 팀원들끼리 분량을 나누어 위와 같이 모든 데이터를 확인하였습니다. 이 때 Labeling이 잘못된 것은 없는지, 데이터는 어떻게 생겼는지, 학습 중에 노이즈가 있을만한 요소들이 있는지 확인했습니다. 이 과정에서 남&lt;-&gt;녀가 뒤바뀌거나 마스크를 착용하지 않았는데 마스크를 착용하고 있다고 파일명이 되어있는 데이터들을 탐지할 수 있었습니다. 또한 안경을 썼거나, 마스크를 눈에다 착용하는 등 학습에 방해가 될 수 있다거나 기이한 데이터들을 확인했습니다. 이렇게 모든 데이터를 확인하면서 학습 전략을 어떻게 취하거나 데이터 정제는 어떻게 하는 것이 좋을지 등 여러 실험 아이디어들을 얻을 수 있었습니다. 이와 더불어 실제 학습을 거쳐 모델을 생성한 뒤에 진행한 시각화로는 Confusion Matrix와 Wandb, Tensorboard 활용 등이 있는데 이것은 아래에서 이어서 말씀드리겠습니다. 🔧 Custom Dataset 구축하기데이터를 확인했으니 Dataset을 구축하는 일을 진행해야 했습니다. 거창한 것이 아니라 데이터가 있는 경로의 폴더구조라던지, 이미지 파일의 확장자 등을 판단하고 데이터를 쉽게 불러올 수 있도록 csv파일을 만드는 것까지로 결정했습니다. 이 때 중요하게 생각했던 요소들은 Labeling을 어떻게 해야 할 지와 Validation 데이터셋을 만들어야 할 지에 대한 것이었습니다. 해당 내용을 가지고 많은 상의를 했으며 결론적으로는 통합라벨, Mask 착용여부, 성별, 나이를 포함하여 4개의 라벨을 갖도록 하자. 사람을 기준으로 Stratify 하게 나누어서 10%, 15%, 20% 비율로 split된 Validation Dataset을 만들자. 위와 같은 결과를 얻게 되었습니다. 저희가 분류해야 하는 라벨 개수는 총 18개입니다. 단일 모델로 18개의 라벨을 분류하라고 하면 성능이 굉장히 안좋게 나올 가능성이 크죠. 따라서 저희는 Mask 착용여부와 성별, 나이를 각각 분류하는 세 개의 모델을 학습시킨다음에 결과를 합치자는 쪽으로 전략이 기울었고, 그에따라 통합라벨을 포함한 세 개의 라벨을 추가한 csv 파일을 만들게 되었습니다. 그리고 Validation Dataset의 경우 동료 캠퍼분이신 서광채님의 토론글을 참고하여 사람을 기준으로 Stratify 한 데이터셋을 구축하였습니다. 빠르게 베이스라인을 만들고 먼저 실험을 진행하셨던 분들의 이야기를 들어보니 과적합 이슈와 Validation Score를 신뢰할 수 없다는 이야기를 듣게 되었습니다. 그렇기 때문에 Validation에는 처음보는 사람들이 들어갈 수 있도록 데이터셋을 분리하는 것이 좋겠다고 판단했으며 추후에 최종 제출을 할 때에도 판단의 도움이 될 것이라고 생각했습니다. 하지만 생각해볼 것은 어떤 특정 라벨에 대해서는 데이터가 굉장히 적게 분포되어 있는데, 적게 분포된 데이터가 Validation Set에 포함되어 있을 가능성에 대한 것입니다. 실제로 위의 방법을 사용했을 때 Validation Score는 조금이나마 신뢰할 수 있게 되었지만, 가뜩이나 적은 데이터가 학습에 사용되지 않았고, 대회가 끝나고 나서야 알게 되었습니다. 그것을 사전에 파악하지 못한 것이 조금의 성능향상을 방해했다고 생각합니다. 즉, Dataset 을 어떻게 구축하냐도 성능에 많은 영향을 미치기 때문에, 사전에 많은 상의를 통해서 여러 데이터셋을 구축하고 이후에는 많은 실험을 통해 택1을 하는 것이 가장 좋은 전략인 것 같습니다. 🎮 Baseline 구축과 적용 이 코드 어떻게 실행시키나요? 팀원들의 실력을 고려하여 베이스라인으로 삼을 템플릿 혹은 코드를 사전에 정해보자. 각자의 환경을 고려하여 베이스라인을 작성하자. (데이터 경로나 패키지 버전 등을 꼭 신경쓸 것!) 저희는 U Stage에서 배웠던 pytorch-template 을 이용하여 베이스라인 코드를 작성해보기로 사전에 협의를 했습니다. 왜냐하면 모듈화가 너무 잘 되어있고, 좋은 코드로 실험을 해야 좋은 모델이 나올것이라는 나름의 기대감이 있었기 때문입니다. (ㅎㅎ;) 하지만 결과적으로 성공이라고만은 말할 수 없을 것 같은데요, 왜냐하면 깃으로만도 머리아파 죽겠는데 여기에 어려운 코드까지 써야한다?? 네. 베이스라인 코드를 작성하고 이해하는데 너무 많은 시간이 걸렸습니다. 대회 기준 4일차에 베이스라인 코드가 완성되었고, 다음날에는 기존에 제공하기로 되어있던 베이스라인 코드가 공개되는 날이었어요. 그리고 해당 코드를 환경이 모두 다른 상황에서 팀원들이 일괄적으로 적용하는 것에도 많은 시간이 들었습니다. 빠르게 베이스라인을 작성하고 빠르게 실험을 이것저것 해보자는 계획을 실천하지 못한것에 꽤 아쉬움이 남네요. 그리고 지금 생각해보면 데드라인이 존재하는 대회의 경우에는 파이토치 템플릿처럼 유지보수와 기능 추가에 많은 비용이 드는 코드가 적절한지 잘 모르겠습니다. 차라리 주피터노트북에서 실행 가능한 코드들로 파이썬 스크립트를 만들었으면 어땠을까 하는 생각이 많이 들어요. 그렇다면, 지금부터 저희가 어떻게 베이스라인을 작성했고 파이토치 템플릿에 익숙해졌는지 말씀드려보겠습니다. 🥷 닌자 프로젝트모던 자바스크립트 튜토리얼에는 Ninza Code 에 대한 이야기가 있습니다. 닌자라고 불리던 전설 속 개발자들이 사용했던 코드에 대한 이야긴데요. 코드는 짧게, 변수명과 함수명은 한글자만, 약어를 쓰고 동의어쓰고 최대한 헷갈리게 형용사도 많이쓰고 외부변수를 덮어쓰는 굉장히 멋있는(?) 방법의 코드 이야기에요. 개발자라면 분명 지양해야 하는 습관들이지만 파이토치 템플릿을 쓰기로 결정한 이상, 전설속의 닌자처럼 코드를 한 번 망쳐보자는 취지에서 Fashion-MNIST를 파이토치 템플릿으로 Fine Tuning 해보자! 라는 과제를 저희끼리 수행했어요. 그렇게 파이토치 템플릿을 한 번 망쳐보고 난 뒤에야 비로소 베이스라인을 작성하기 위한 최소한의 준비가 되었습니다. ✏️ Efficient Baseline그렇게 닌자프로젝트를 진행한 이후에는 파이토치 템플릿으로 모든 코드를 작성할 수 있었습니다. 우선은 이전에 만들어두었던 csv 파일을 활용하여 CustomDataset과 CustomDataLoader를 우선적으로 만들고 config 파일에서 받아올 수 있는 인자들과 모듈들을 커스텀했습니다. 파이토치 템플릿 자체가 워낙 간단한 config 파일 수정만으로 모든 학습이 이루어질 수 있는 구조로 되어있다는 점을 인지하여 여러 가능성을 포괄한 코드를 만들기 위해 노력했어요. 베이스라인 코드를 작성할 때 아래의 것들을 원칙으로 삼아 구현했습니다. config 파일 수정만으로 분리해놓았던 각각의 라벨을 학습할 수 있게 하자. config 파일 수정만으로 pretrained 모델을 바꿀 수 있게 하자. config 파일 수정만으로 Augmentation의 값들을 변경할 수 있게 하자. config 파일 수정만으로 학습할 수 있는 데이터를 변경할 수 있게 하자. config 파일 수정만으로 여러 모델을 함께 학습할 수 있게 하자. config 파일 수정만으로 Evaluation 까지 동작할 수 있게 하자. 마침내 위의 원칙들을 적용한 베이스라인 코드를 만들게 되었고, 여러 기능이 추가된 탓에 복잡도는 올라갔고 시간이 오래 걸렸지만 좋은 코드를 입맛대로 변경하여 대회에 최적화된 코드를 만들 수 있었다는 것에 큰 즐거움을 얻었습니다. 하지만… 😂 코드 어떻게 실행해요? ㅜㅜ가장 큰 문제가 발생했습니다. 파이토치 템플릿으로 멋스러운 베이스라인 코드를 만들었지만, 서로의 환경이 다름을 고려하지 못했고 그로인해 편하게 하려고 만들었던 몇몇 기능들이 다른 팀원분들의 환경에서 동작하지 않는 문제가 있었습니다. 특히 데이터는 용량문제로 깃에 올리지 못하기 때문에 저장된 경로가 모두 달랐는데, 경로 관련된 문제를 해결하는 것이 가장 힘들었던 것 같습니다. 또한 잘못된 Label을 수정하지 않은 팀원분의 경우 이미지를 불러올 수 없다거나, python이나 필요한 package들의 버전이 달라서 실행이 안된다거나 $PATH 와 같은 Alias를 수정하여 아예 필요한 패키지가 호출이 안된다거나 하는 기괴한 에러들이 수도없이 발생했습니다. 결국에는 어떻게든 해결하긴 했지만, 서로의 환경을 고려하여 패키지 버전을 Fix하는 등의 방식을 적용하지 못했다는 것에서 추가적인 비용이 많이 들었습니다. 이러한 이유로 1주차 때는 많은 실험을 못해봤고 이것이 더 성능을 올리지 못한 원인 중 하나가 될 것 같습니다. 다음부터는 Dot ENV 등을 활용한다거나 애초에 모두가 동일한 가상환경을 세팅하고 시작한다거나 하는 방식을 사용하는 것이 좋을 것 같아요. 정말 이 부분에서 많은 것을 배운 것 같습니다. 🥐 예상치 못한 파이토치 템플릿의 단점어찌저찌 모두가 파이토치 템플릿을 사용하긴 했지만.. 예상치 못한 문제가 발생하기도 했습니다. 시간이 지날수록 여러가지 실험을 하고, 유용한 라이브러리를 사용해보는 등 여러 시도를 많이 하게 되는데요. 이 때 파이토치 템플릿으로 이루어진 코드들은 구조를 완전히 꿰고 있지 않으면 이 코드 어디에다가 붙여넣어야 되지..? 처럼 어디에 기능을 추가해야 될지 혼란스러워 지더라구요. 특히 대회에서는 빠른 실험을 통해 성능 향상이 보이면 해당 결과를 빠르게 공유하고 그 실험에 집중하는 것이 중요한데 빠르게 유용한 코드들을 손쉽게 적용할 수 없었어요. 또 하나, 파이토치 템플릿은 학습을 시킬 때 모델에 config 파일 역시 함께 저장을 시키는데 그렇기 때문에 모델을 불러올 때 config 파일이 없으면 load가 되지 않는다는 치명적인 단점이 존재해요. 이말은 즉슨, 이미 공유된 튜토리얼 코드라도 템플릿에 적용을 하거나 config 파일을 추가하는 수정비용이 들어간다는 뜻이겠죠. 고로 이런 부분들까지 생각을 해본다면, 원래 파이토치 템플릿에 익숙하지 않은 이상 다른 베이스를 찾아보는 게 더 효율적일 것 같다고 느끼게 되었습니다. 👨‍🔬 모델링과 실험 이렇게 하면 성능 오르는거 맞아? 서로가 무엇을 하고 있는지, 무엇을 해봤는지 모두가 자세하게 알고 있고,어떤 것을 추가해봤을 때 오히려 성능이 떨어졌다는 것을 모두가 알고 있어야 하며,성능이 올랐을 때 해당 성능을 재현할 수 있는 코드를 모두가 가지고 있어야 한다. ✂️ 정말 많은 실험을 시도함. 근데 실험 일지는?간신히 베이스라인을 완성한 이후 저희는 정말 많은 실험을 해보았습니다. Hard하게 Augmentation을 적용한 이미지를 데이터셋에 추가하기도 하고, albumentations 에 있는 모든 기능들을 정리하고 실험해 보았으며, ResNet, EfficientNet, RegNext, RegNet, MLP-Mixer, ViT 등 정말 많은 사전학습 모델을 사용해보고 Optimizer와 Loss를 계속 바꿔도 보고, CutMix, MixUp, AugMix와 같은 augmentation 기법들은 물론 TTA, Label Smoothing, Pseudo-Labeling 등 Data Imbalancing을 해결하는 방법들까지 사용해봤습니다. 시도해볼 수 있는 코드를 모두가 개인의 노력만으로 작성해서 실험해본 것도 너무 좋고, 프로그래밍 스킬이 늘어나고 파이토치 템플릿에도 익숙해지는 것은 정말 좋았으나, 결과적으론 위의 것들을 모두 시도해봤을 때 전체적인 성능 향상이 이루어지지 않았습니다. 그 이유는 체계적인 실험일지를 관리하지 않았던 것에서 찾을 수 있을 것 같습니다. 수많은 기법과 수많은 하이퍼파라미터들의 조합 중에서 최적을 찾는 것. 그것이 성능을 끌어올릴 수 있는 방법이지만 해당 실험들의 결과를 철저하게 기록하지 않고, 실험 순서를 정리하지 않았기 때문에 모두가 다른 조합들로 시도했을 텐데, 그 결과들이 모두 수집되지 않아서 체계적인 실험을 할 수가 없었습니다. 또한 실험을 통해서 성능이 오르는 것을 찾는 것도 중요하지만, 성능이 떨어지는 요인을 찾는 것도 그만큼 중요한데요. 왜냐하면 성능이 향상된 모델에서 해당 요인을 적용하지 않을 수 있기 때문입니다. 이렇듯 기록의 부재로 저희는 수많은 조합들을 매번 새롭게 찾아야만 했습니다. 다시 생각해보니 이것도 정말 아쉽습니다 ㅎㅎ. 🍟 공유에 진심이어야 한다.또 한가지, 여러 이유가 있겠지만 그 중에서 ‘공유’에 관해서 이야기하고 싶어요. 저희는 Github Project 를 통해 서로의 TODO와 일정을 관리하고 있었지만, 시간이 지날 수록 점차 관리에 소홀해졌고 조금의 성능 향상이 있었을 때 그 방법을 올바르게 전파하지 못한 것 같습니다. 너무 빡센 일정에 다들 지친 문제도 있었지만, 베이스라인의 스노우볼이 여기까지 도달했다는 생각도 들었습니다. 사실 제가 안했다는게 가장 크지만요… (팀원분들 정말 죄송합니다.) 당연한 이야기겠지만 저는 이렇게 했더니 성능이 올랐어요~, 저 이거 실험해봤는데 별로였어요. 정도의 공유만으로는 팀 전체적으로 다양한 실험을 통해 성능을 이끌어 낼 수가 없었습니다. 지금 와서 돌이켜보면 왜 그랬지 싶은데, 저를 포함한 팀원 모두가 그만큼 순위에 집착했었고 다른 것을 신경쓸 여유가 없었던 것도 같아요. 하지만 정말 공유의 중요성을 뼈저리게 알 수 있었습니다. 그럼에도 종료 전날에 기적적으로 성능을 향상시켰으며 그때부터 조금이라도 성능을 올리기 위해 모두가 힘을 합쳐 앙상블과 TTA에 전념했고 종료 1초전까지 성능을 올릴 수 있었습니다. 🥉 최종 모델 개요 최종적으로 사용한 모델은 아래 모델들이 결합되어 있는 구조입니다. 성별을 분류하는 regnety_006 마스크를 착용한 사람들로만 학습시킨 efficientnet_b1 마스크를 잘못 착용한 사람들로만 학습시킨 efficientnet_b1 마스크를 착용하지 않은 사람들로만 학습시킨 efficientnet_b1 마스크 착용 여부를 분류하는 regnety_006 처음 사전학습된 모델을 단일구성하여 18개의 라벨을 한 번에 분류하게 했을 때는 약 20%의 정확도와 0.18 정도의 f1 스코어를 기록했습니다. 성능을 향상시키기 위한 여러 방법을 고안하던 중, 3개의 Task로 나누어서 세 개의 모델이 각각 Gender, Age, Mask 착용여부를 분류하게 한 뒤 나온 결과를 조합하여 최종 라벨을 도출하게 하였을 때 약 70%의 정확도와 0.69 정도의 f1 스코어를 기록할 수 있었습니다. 위의 실험을 토대로 각각의 분류 모델을 활용하게 되었고, 여러 추가적인 실험을 바탕으로 Age 모델의 예측 결과가 좋지 않다는 것을 파악하게 되었습니다. 그래서 Age 모델도 (마스크를 착용한 사람으로만 학습한 모델, 마스크를 잘못 착용한 사람으로만 학습한 모델, 마스크를 착용하지 않은 사람으로만 학습한 모델) 로 분리하였고 결과적으로 나이 예측 성능을 향상시킬 수 있었습니다다. 🫂 앙상블 너 아이큐 150, 내 아이큐 150, 합치면 300! 하지만 이렇게 총 5개의 모델을 활용했을 때 ‘성별-나이’ 혹은 ‘마스크-성별’ 등 연관성이 존재할 수 있는 정보를 활용하지 못하고 독립적인 Feature로만 정답을 도출하게 되는데요. 따라서 의존적인 정보를 추가하고자 마스크 착용여부로 분리한 각각의 Age 모델들을 앙상블하였고 이후 Gender와 Mask 모델까지 통합하여 Multi Label Classification을 수행하게 하였습니다. 이렇게 0.75의 f1 스코어를 기록하였고 이후 최적의 TTA를 적용하고 제출 결과들을 Hard Voting 하여 최종적으로 0.766의 f1 스코어를 기록할 수 있었습니다. 💯 굿 프랙티스 결과적으로 옳았던 선택들 시각화와 로그는 무조건 옳다! 🎉 Tensorboard에 Confusion Matrix 시각화 🎉 Wandb 사용 🎉 Evaluation에 대한 클래스별 오차 비율 시각화 🎉 TTA 실험일지 작성 지금 생각해도 저희가 잘 한 것은 바로 모든 것에 로그를 남겼고, 시각화 툴을 잘 이용했다는 것입니다. Wandb와 Tensorboard를 적극적으로 활용하였으며, 데이터 분석 단계에서 0~17로만 채워서 제출한 결과를 통해 Evaluation 데이터의 정답 분포를 알아냈고 저희가 제출할 정답과 클래스별 오차비율을 계산해냈습니다. 그 결과 제출 전에 나름 신뢰할 수 있는 근거들을 잘 만들어놓았다는 것이 미흡한 협업에도 좋은 결과를 이끌어낼 수 있었던 전략이라고 생각합니다. 너무나도 쉽게 과적합이 일어나는 이번 대회에서 최소한의 제출 모델 선택 전략은 필수적이며, 아무리 Validation 데이터셋을 유의미하게 나누었다고 해도 웬만하면 90% 이상의 정확도를 기록했기 때문에 이러한 시각화 자료들이 꽤 중요한 모델 선택의 지표로 작용할 수 있습니다. 또한 최종 모델에서 TTA를 적용할 때 augmentation 기법들에 대한 조합별로 스코어를 기록했으며 최적의 augmentation을 찾아냈다는 것이 성능향상에 유의미하게 작용하였습니다. 저희는 TTA를 적용했을 때 최대 0.746 -&gt; 0.763 까지 0.017 정도의 f1 스코어 향상을 보았고, 이 부분만큼은 체계적인 실험이 이루어졌기에 많은 도움을 받을 수 있었습니다. 마지막으로돌이켜봤을 때 잘한 것보다 아쉬운 것들이 더 많이 남는 대회였던 것 같습니다. 비록 대회 전에 세웠던 1등이라는 목표를 달성하진 못했지만, 그럼에도 이후엔 어떤 식으로 협업을 하는 것이 좋을지 고민해보고 배울 수 있었던 경험이었던 것 같습니다. 너무 좋은 팀원분들과 함께할 수 있어서 정말 좋았고, 이번 경험을 통해서 저를 비롯한 다른 분들도 이후 대회에서는 모두 원하는 목표 충분히 이루실 수 있을 것이라 생각합니다. 저희의 이야기가 조금이라도 도움이 되었길 바라면서 마치겠습니다. 긴 글 읽어주셔서 정말 감사합니다! 🤜 부록: 팀원들의 한마디 요한: 실험 일지와 코드를 체계적으로 관리하지 못한 것이 가장 아쉽습니다. 서버가 5킬을 당한게 좀 컸던 것 같아요. 그래도 너무 좋은 팀원들과 미흡하지만 재밌는 대회 함께 진행할 수 있어서 너무 좋았습니다! 지민: 모두 항상 열심히 하시는 모습이 자극이 되어서 같이 완주할 수 있었던 것 같습니다. 특히 요한님 베이스라인 부터 모르는 부분 질문까지 친절하게 답해주셔서 감사했습니다!! 하겸: 아쉬운 점 투성이지만 성장통이라고 생각합니다. 다영: 첫 대회라 곱씹을수록 아쉬운 점이 많지만 좋은 팀원들을 만나 끝맺음이 좋았습니다 ! 덕분에 많이 배웠고, 자극도 많이 받았습니다. 지속 가능한 성장을 위해 파이팅 !! 민지: 잘한점이 너무 많지만!! 지나고 보니 아쉬움 투성이인 것 같습니다. 특히 리더보드 스코어 향상을 목표로 하신다면 다른 팀원의 코드로 빠르게 업데이트하고, 실험을 잘 기록해두는 것이 중요하다고 느꼈습니다. 지식 뿐 아니라 협업 과정 측면에서 배운 점이 많은 대회였습니다! 아경: 멋있는 분들과 함께해서 좋았습니다. 되돌아보면 아쉬운 점도 많았지만 그만큼 배운 점도 많은 것 같습니다. 이번 대회를 시작으로 더 발전하길! 화이팅 🙂 준영: 처음하는 DL, 처음하는 Competition, 처음 써보는 pytorch. 모든 것이 어색했지만, 좋은 팀원과 함께 해서 영광이었습니다. 항상 열심히 하시는 모든 팀원분들께 많은 자극을 받았습니다. 감사했습니다. ✨빛예닮 멘토님✨: Boostcamp에서 스스로를 정말 잘 Boost시킨 팀이라고 생각합니다! 혼자 앞서가기보다 모두가 함께 가는 것을 택했고, 오늘보다 내일을 더 기대되게 했던 우리 15조! 부족한 멘토와 함께였지만 여러분들 덕분에 우리 조가 더 빛난 것 같아요! 언제나 응원합니다! 감사합니다!","link":"/AI-Competition%EC%9D%98-%ED%98%91%EC%97%85%EC%9D%84-%EC%9C%84%ED%95%9C-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"title":"Arensky - Suite No.1 in F major, Op. 15 - Part 2&#x2F;3","text":"Anton Stepanovich Arensky 의 두 대의 피아노를 위한 모음곡 1번 F장조, Op. 15 중 2번 왈츠이다. 림스키코르사코프에게 작곡을 배운 뒤 라흐마니노프와 스크리아빈을 제자로 둔 거장이다. 3개의 소품으로 이루어진 이 곡 중 두 번째 ‘Valse’ 는 아기자기하고 귀여운 멜로디가 낭만적으로 느껴진다. First와 Second가 주 선율을 주고받으며 고조되다가 화려한 아르페지오로 곡의 분위기를 반복적으로 반전시킨다. 유명하진 않지만 은근 힐링음악. 내가친거!","link":"/Arensky-Suite-No-1-in-F-major-Op-15-Part-2-3/"},{"title":"2020 이노베이션 아카데미 컨퍼런스(INNO-CON) 후기","text":"2020 이노베이션 아카데미 컨퍼런스(INNO-CON) [키노트 스피치] “왜 42인가?” 소피비제_Ecole42 교장 42에 대한 소개와 42의 Global Interaction에 대한 이야기가 주를 이루었다. 국제적으로 10개가 넘는 42캠퍼스가 존재하며, 앞으로도 계속 확장을 이어갈 예정에 있다. 42가 국제적으로 각광받고 각국에서 요구되는 이유는 바로 ‘지식’의 접근법이 매우 달라졌기 때문이라고 볼 수 있을 것 같다. 그저 단순하게 ‘누가 이야기해주는 것을 듣는것’만으로는 좋은 교육방법이라고 할 수 없다. 스스로가 직접 문제를 파악하고 해결해볼 수 있는 Problem Solving 이야말로 42에서 가장 체득하기 좋은 ‘능력’이 아닐까 싶다. 소피비제님의 말씀을 들었을 때, 내가 42를 선택한 것은 정말 행운이었구나 하는 생각이 가장 크게 들었던 것 같다. 새로운 기술시대에 적응해야 하는 지금, 지식을 어떻게 습득하고 지식을 습득하는 방법을 어떻게 배우느냐가 새로운 시대를 맞을 수 있는 핵심적인 필요성이자 기업이 개발자에게 요구하는 가장 큰 능력이지 않을까 싶다. Global 42 소개 Global 42 캠퍼스 소개Global 42 입학생 인터뷰 박헌병_42 SV 교육생 / 선우문형_42 SV 교육생 / 황보미_42 SV 교육생 / 이재석_Ecole 42 교육생 일자리에 대한 ‘야망’이 부족하고, ‘IT’에 입문하는 것은 절.대. 꿈도 꿀 수 없고, 학위도 없고 알바만 해왔던 대부분의 사람들이 La Piscine을 통해 수영장에서 허우적대며 살아남고 버텨서 저마다의 자리에서 혁신을 이루어냈다는 것. 그것이 바로 42의 참된 가치라고 할 수 있지 않을까 싶다. 비전공자가 코딩한다는 것이 전혀 어색하지 않은 집단, 처음 보는 사람과 전혀 어색하지 않게 아무렇지 않게 이야기하는 것이 전혀 어색하지 않은 집단 등으로 42의 문화를 간략하게 소개할 수 있을 것 같다. 42Seoul을 비롯하여 글로벌 캠퍼스들의 경우도 매한가지인 것 같다. 서로 다른 의견을 가진 동료들, 각자의 분야에서 뛰어난 역할을 해내고 온 사람들, 힘들게 생계만 유지하다가 온 사람들까지 정말 다양한 사람들이 있고, 그런 사람들과 소통할 수 있는 42의 시스템. 매우 생소하고 특별한 42의 커리큘럼. 이 모든 것들을 각국에서 진행하고 있다는 것이 가장 큰 놀라움이자 42의 장점이며, 42의 네트워킹을 통해 같은 고민을 하고 있는 사람들에게 물어보고 조언을 얻으며 도움을 주고받을 수 있다는 것이 42만의 차별화된 혜택이 아닐까 싶다. 42의 시스템은 모든 것들이 유기적이고 체계적으로 구성되어 있기 때문에 서울에서는 42의 분위기를 상당히 낯설어 하는 사람들이 존재할 수밖에 없을 것 같다. 누가 이렇게 하라고 알려주는 것 하나없이 동료들과 무인도에 떨어져 생존을 해나가야하는 것만 같은 시스템 속에서, 그런 좋은 동료들과 논의하고 학습하는 것에서 굉장히 큰 메리트가 있지 않을까 싶다. ‘협력하는 방법’ 그것을 가장 큰 가치로 추구하다보면 좋은 개발자가 될 수 있을 것이라고 감히 단언하고 싶다. 42 SEOUL에 대한 모든 것 42 SEOUL 입학 시스템 소개 42 서울인의 하루 브이로그 42 SEOUL 활동 상세보기 영상을 보고 너무 재밌었다. 42Seoul 본과정생으로서 경험할 수 있었던 모든 것들이 너무 현실감있게 반영되어 있었기 때문이다 ㅋㅋ. 여러 다양한 동아리들도 그렇고, 같은 목표와 관심사들을 가지고 있는 사람들끼리 스터디를 하거나 사이드프로젝트를 진행하는 42Seoul 카뎃분들 너무 멋있는거 아니냐고~~ [LIVE] 성공적인 학습을 위한 전문가 대담 김수보_전) KTH개발자 / 오종인_전) 소셜박스 CTO / 이호준_전) 네오위즈 개발자 42Seoul의 오종인 멘토님 개발자들에게 있어서 ‘학습’은 생활이다. - 오종인 멘토님 러닝커브가 빠른 사람들의 특징은 솔직한 사람들이다. 내가 ‘잘 모른다’ 가 되면 모른다고 이야기 해야하고, ‘잘 안다’가 되면 잘 안다고 이야기할 수 있는 사람들이, 솔직하게 먼저 이야기하는 개발자들이 러닝커브가 좋았던 것 같다. - 이호준 멘토님 42Seoul을 지원하는 모든 분들이 가장 궁금해하셨던 것들 중 하나가 나에게 가장 ‘잘’ 맞는 영역이 무엇이며, 어떻게 공부해야 하는지에 대한 것들이다. 이번 세션에서는 세 분의 멘토님들께서 각 영역별 최적의 학습법이 무엇인지에 대한 이야기를 해주셨고, 관련해서 언제든 도움을 주실 것으로 말씀해주셨다. 나는 김수보 멘토님께 인생에 대한 조언을 구했었는데, 물론 도움되는 이야기들을 많이 해주시긴 했다. 하지만 멘토님들께서 말씀해주신 것처럼 멘토분들이 답을 찾아주시는 것은 절대 아니고 내가 치열하게 고민하고 찾아야할 것이기 때문에, 인생상담보다는 ‘코드’를 가지고 멘토링을 받는 것이 더 많은 도움이 될 것 같다… ㅋㅋ 상당히 많은 질의응답과 조언들이 오갔던 세션이었다. 인프라, 백엔드, 프론트엔드에 대한 이야기들이 주를 이루었는데, 사실 이 부분은 조금 아쉬웠다. 나처럼 AI나 시스템 프로그래밍 쪽에 관심을 가지고 계신 분들도 많을 것이기 때문이다. 뭐, 멘토님들은 웹에 대한 영역들을 주로 개발을 해오셨기 때문이지 않았나 싶은 생각이 든다. 그래도 모르는 것보단 아는 것이 나으므로! 열심히 들었다. 가장 핵심적인 부분들을 요약하자면, 어떤 영역을 선택하여 공부해야 할 지 그것은 기업들의 도메인이나 프로젝트에 따라 매우 많은 변수가 생기고 달라질 수 있으므로, 각 회사들의 대표적인 프로젝트들이 무엇인지 판단하고 거기에서 많이 사용하는 프레임워크 등을 탐색하여 학습해보는 것이 중요하다는 것. 또한 관련된 사이드프로젝트들을 도전해보는 것. 이렇게 정리할 수 있지 않을까 싶다. [LIVE] [키노트 스피치]“개발 생태계가 원하는 인재와 기업의 역할” 김성훈_업스테이지 대표 내가 진짜 기다리고 기다렸던 김성훈 대표님의 키노트. 김성훈 대표님의 강연에서 주로 언급된 내용은 어떤 개발자들을 찾고 있는지, AI 개발자들은 어떤 태도를 지녀야 하는지에 대한 이야기로 정리할 수 있을 것 같다. 본인이 봐왔던, 면접에서 합격하는 사람들은 ‘다큐예능’이라고 정리할 수 있다고 언급하시면서 어떤 새로운 일을 주었을 때 ‘Wow’를 보여주는 사람 또는 성장속도가 빠르거나 무언가를 끝까지 파본 경험이 있는 사람을 우선적으로 선호한다고 말씀하셨다. 그와 더불어 중요한 것은 이야기할 때 재미있는 사람. 어떤 개발자가 되어야 할 지, 어떤 측면을 조금 더 중요시 해야할 지에 대한 힌트가 되는 말씀이었던 것 같다. 결국에는 기업도 협업의 연속이고 그 배경에는 소통이 있어야 한다. 소통 시 답답하거나 재미가 없는 사람과는 당연히 일을 하고 싶지 않을 것. 또 한가지 중요한 것은 믿고 맡길 수 있는 사람 한 명을 뽑을 것이냐, 확인을 해야 하는 사람 세 명을 뽑을 것이냐에 대한 논의이다. 김성훈 대표님은 확인해야 하는 사람 세 명의 임금만큼 더 지급하더라도 믿을 수 있는 한 명을 뽑는 것이 좋다고 말씀하셨다. 넷플릭스에서의 영입정책을 인용하시면서 비슷한 것 같다고 말씀하셨다. 잘하는 사람과 보통하는 사람의 역량 차이는 두 배정도 차이나지만, 인공지능 관련 일에 한해서는 잘하는 사람과 보통 사람과의 차이는 10배, 혹은 100배 이상 차이가 날 수 있다고 한다. 다른 이야기들보다는 이러한 인재상, 개발자가 지녀야할 태도에 대한 이야기가 주가 되었다. 많이 바쁘신지 키노트 시간이 20분밖에 되지 않으셨어서 매우 아쉬웠다. 그래도 내가 드렸던 질문들과 사전에 받은 질문들은 추후에 사이트를 통해 다 답변을 해준다고 하셨으니, 기다려봐야겠다! Global 42 Alumni의 취업 성공기 이덕현_Ecole 42 교육생 / JEAN_Ecole 42 교육생 / Sevastien_Ecole 42 교육생 / Anselme_Ecole 42 교육생조준형_42 SEOUL 교육생 / 박영준_42 SEOUL 교육생 / 임지영_42 SEOUL 교육생 Ecole 42와 42Seoul의 졸업생분들이 나오셔서 취업에 대한 이야기, 그리고 해외에서 바라보는 42 출신들의 평판 등에 대한 이야기들이 주가 되었다. 42Band에서 드럼을 맡고 계신분이 쿠팡 개발자로 취업을 하셨다니,,, 대박! 나도 빨리 42Seoul 동료분들에게 희망과 도움을 줄 수 있는 졸업생이 되고 싶다는 생각을 했다. 42 SEOUL과 함께 하고 있는 기업 황정환_CJ올리브네트웍스 리더 / 임세현_BC카드 센터장 / 박노헌_스타셀 대표 / 최성진_코리아스타트업포럼 대표박준영_한화생명 드림플러스 파트장 / 김민현_커먼컴퓨터 대표 나는 지금까지 42Seoul에서 기업과의 협업프로젝트에 아무것도 참여해보지 않았다. 아직 나의 부족함을 너무나도 잘 알고 있기 때문이다. 그래도 참여하신 카뎃분들 보면 다 너무 멋지다… 다음에 또 좋은 기회가 생기면 나도 참가해봐야지! [LIVE] 선배 개발자들이 말하는 실제 취업현장과 개발 문화 우상훈_네이버 책임리더 / 김민태_우아한 형제들 그룹장 / 박민우_LINE 테크에반젤리스트 / 정창훈_당근마켓 CTO 오후 세션의 핵심 키워드는 ‘개발자로서 지녀야 할 태도’와 ‘협업능력’ 이지 않을까 싶다. 회사 동료로서 같이 일을 하고 싶은 사람인지, 그리고 소통이 얼마나 잘 되는 사람인지에 대한 이야기가 주가 되었다. 그래서 “이거다!” 싶을 만큼 꽂히는 이야기는 사실 별로 없긴 했다. 신기할 정도로 똑같이 답을 듣는 질문들을 계속해서 하게 되니, 실제로는 도움될만한 이야기가 크게 없지 않았나 싶은 아쉬움이 든다. 솔직히 모두 알고 있을 것이다. 어떻게 코딩을 하고 포트폴리오를 쌓아야 취업에 유리한 것인지.. ㅋㅋ 그 태도를 가지기가 상당히 어려운 게 문제지 뭐,, 슬기로운 커뮤니티 생활 Microsoft MVP / OKKY / Korean R User Community / Keras Korea / Women who code Seoul 이노베이션 아카데미의 개발 생태계 지원 콘텐츠 제작기 최재규_매직에코 대표 / 노상범_이브레인 대표 / 김재인_경희대학교 교수 [LIVE] 개발 조직 문화 전문가 라이브 대담 노상범_이브레인 대표 / 권순선_구글코리아 리드 / 김종민_앨라스틱 에반젤리스트홍연의_LINE Developer Advocate / 조은옥_IBM Developer &amp; University Relations Manager 나머지 세션들 역시 후기를 조금씩이라도 작성할라고 했지만, 사정이 있어서 슬기로운 커뮤니티생활부터는 참석하지 못했다. 다른 분들이 정리하는 걸 보던가 유튜브 영상이라도 나중에 다시보기로 돌려봐야겠다. Innovation Academy의 첫 컨퍼런스. 역대급 라인업에 역대급 영상퀄에! 너무 알찬 하루였던 것 같다. 재단측과 서포터즈 분들, 참여인사분들에게 다시 한 번 감사를 드리고 싶다.","link":"/2020-%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98-%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8-%EC%BB%A8%ED%8D%BC%EB%9F%B0%EC%8A%A4-INNO-CON-%ED%9B%84%EA%B8%B0/"},{"title":"Attention is all you need - Transformer 논문 요약","text":"기존에 지배적이던 모델은 RNN이나 CNN에서 Attention Mechanism을 적용한 모델들이다. 구글이 이번 논문에서 발표한 Transformer는 Attention만으로 인코더와 디코더를 구성함으로써 보다 적은 계산량으로 높은 성능을 내는 모델이다. RNN모델은 문장이 길어질수록 메모리의 제약때문에 앞에 나온 sequence들을 반영하지 못한다는 단점이 있고, 기존 RNN을 이용하면 “The animal didn’t cross the street because it was too tired.” 라는 문장이 있을 때 it이 무엇인지 알아낼 수 없었다. 하지만 Transformer의 self-attention 기법을 사용하면서 그것이 가능해졌다. Model Architecture 트랜스포머의 가장 주요한 특징은 벡터간 계산이 필요하던 것을 행렬로 바꾸어 병렬처리를 가능하게 했다는 것이다 . Positional encoding으로 단어들의 위치정보를 계산하였으며, residual connection을 사용하여 레이어를 정규화 하면서 안정성을 더해주었다. 또한, 인코더의 input과 output의 차원이 같기 때문에 여러 인코더를 배치할 수 있다는 특징이 있다. 디코더 역시 인코더와 비슷하지만, 다른점은 마스킹을 적용한다는 것이다. 아직 나타나지 않은 단어에 대한 어텐션을 배재시키기 위함이다. 멀티헤드 어텐션에서는 key, value, query로 연산을 하고 피드포워드를 통해 행렬을 벡터로 변환시킨 후 리니어와 소프트맥스 레이어를 통해 벡터를 단어로 변환시킨다. 소프트맥스함수를 거친 이후에 label smoothing 기법이 사용되는데, 예를들어 thank you 라는 시퀀스가 한국어로 ‘고마워’ 와 ‘감사합니다’ 라는 라벨로 학습되어야 할 때, 고마워와 감사합니다는 전혀 상이한 벡터이기 때문에 학습에 지장이 생길 수 있음. 그러나 원핫벡터가 아닌 레이블 스무딩을 이용하게 되면 정답에 가까운 벡터값은 1에 가까운 실수로, 정답에 먼 벡터값은 0에 가까운 실수로 표현되기 때문에 어느정도 보완이 될 수 있음.","link":"/Attention-is-all-you-need-Transformer-%EB%85%BC%EB%AC%B8-%EC%9A%94%EC%95%BD/"},{"title":"AI Bookathon 대상 후기","text":"2021 SKKU AI x Bookathon 대상 후기대회 및 팀원 소개북📚이온앤온 팀원들 허은진 윤채원 김종현 곽진성 이요한 본 글은 5명의 팀원이 무박2일동안 한숨도 안자가면서 열중했던 제 3회 AI x Bookathon 대회 후기이자 솔루션입니다. 제 3회 AI x Bookathon 은 성균관대학교와 MindsLab 이 협약하여 주최되었고, Naver Clova 의 후원으로 진행되었습니다. 약 60여개 팀이 참여했던 예선을 거쳐 총 15개의 팀이 본선에 참여하게 되었습니다. 본선에서는 16일 오후 3시부터 17일 오후 12시까지, 참여팀들은 21시간동안 2만자의 수필을 생성해야 했습니다. 21시간이라는 짧은 시간이었지만 주제 선정 과정부터 모델링, 생성과정에서 여러 문제를 해결하고 차별점을 두기 위해 많은 시도와 실험을 하였고, 결과적으로 1등을 하며 대상을 받을 수 있었습니다. 막연한 순간들이 매우 많았지만, 그럼에도 여러 시행착오를 겪고 극복하며 완성도 있는 작품을 만들어낸 저희의 이야기가 좋은 레퍼런스가 되길 바라면서 시작해 보겠습니다. &gt;&gt; 코드 보러가기 &gt;&gt; 최종작품 보러가기 대회 전략Linux 서버에 T4 한 개의 GPU를 제공받았으며, MindsLab 의 자산인 GPT-2 를 Wrapping 된 형태로 제공해주었습니다. 제공해준 모델을 통해 Fine-tuning 을 진행하는 과정은 팀 별 주어진 Linux 서버에서 Mindslab 서버에 학습 요청을 하면 학습 로그와 함께 모델의 Checkpoint 를 Response 해주는 형태였습니다. 그렇다보니 실제 Train 과정의 Customizing 이 불가하였고, 데이터 정도만 변경할 수 있는 구조였습니다. 때문에 저희는 Public 으로 공개된 사전학습 GPT 모델을 선정하여 Train 과정을 직접적으로 조절하고 튜닝할 수 있도록 전략을 수립하게 되었습니다. 구조 자체를 파악하기 어려운 모델을 사용하는 것보다는 Benchmark 가 공개되어있고, 저희에게 익숙한 코드를 사용하는 것이 자유도나 작업 효율면에서 효과적일 것이라고 생각했기 때문입니다. 따라서 대회 시작 이후부터는 빠른 실험을 위하여 Huggingface 의 transformers 라이브러리를 사용한 GPT 모델의 training 과 inference 과정의 baseline 코드를 작성하였고 baseline 이 완성된 이후엔 생성해낼 작품의 주제를 결정하고, 결정된 주제에 맞게 데이터 수집과 전처리, fine-tuning, generating 과정을 거쳐 최종 제출물을 산출하기로 하였습니다. 또한 평가항목중에 유사도 검사가 있었기 때문에 AI가 만들어낸 원작을 최대한 변형시키지 않는 쪽으로 수필을 만들어내고자 했습니다. 데이터 수집 및 전처리데이터 수집적절하고 좋은 필체의 수필을 생성하기 위해서 퀄리티 좋은 데이터가 많이 필요했습니다. 데이터를 수집할 때 여러 고려사항들이 있었습니다. 모델이 적절한 문장을 생성하기 위하여 어느정도 scale 의 데이터를 사용하거나 어느정도 step 을 학습시켜야 할 지 몰랐으며, 저희 과제와 알맞는 데이터는 어떻게 선별할 것인지, 또한 대회 규정에 맞는 데이터 저작권 문제도 해결해야 했습니다. 따라서 대회 규정에 맞는 데이터 수집 채널을 먼저 선정하고, 최대한 많은 양의 데이터를 수집한 뒤 그 후에 선별작업을 추가적으로 하기로 계획했습니다. 저희는 수필에 해당하는 글들이 많은 브런치의 ‘감성 에세이’ 탭에서 ‘산문’, ‘산문집’, ‘수필’, ‘힐링에세이’, ‘일상에세이’ 정도의 Tagging 이 되어있는 웹페이지에서 약 8시간 정도의 크롤링 작업을 진행했습니다. 또한 문학광장 사이트의 글틴(명예의전당), 신문사들의 신춘문예 당선작을 추가로 수집하게 되었습니다. 그렇게 약 160Mb 의 데이터를 수집할 수 있었습니다. 데이터 전처리 수집한 데이터에는 \\n 과 같은 escape character 부터, 초성이 반복되는 ㅋㅋㅋㅋㅋ 혹은 광고 문구, bad character 들이 많이 포함되어 있었습니다. 이러한 noise 들은 KLUE: Korean Language Understanding Evaluation 논문에서도 유사하게 나타났던 문제였기 때문에 KLUE 에서 사용한 전처리 방법을 차용하여 사용하였습니다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def preprocessing(text): # 문제를 일으킬 수 있는 문자 제거 bad_chars = {&quot;\\u200b&quot;: &quot;&quot;, &quot;…&quot;: &quot; ... &quot;, &quot;\\ufeff&quot;: &quot;&quot;} for bad_char in bad_chars: text = text.replace(bad_char, bad_chars[bad_char]) error_chars = {&quot;\\u3000&quot;: &quot; &quot;, &quot;\\u2009&quot;: &quot; &quot;, &quot;\\u2002&quot;: &quot; &quot;, &quot;\\xa0&quot;:&quot; &quot;} for error_char in error_chars: text = text.replace(error_char, error_chars[error_char]) # 이메일 제거 text = re.sub(r&quot;[a-zA-Z0-9+-_.]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+&quot;, &quot;[이메일]&quot;, text).strip() # &quot;#문자&quot; 형식 어절 제거 text = re.sub(r&quot;#\\S+&quot;, &quot;&quot;, text).strip() # &quot;@문자&quot; 형식 어절 제거 text = re.sub(r&quot;@\\w+&quot;, &quot;&quot;, text).strip() # URL 제거 text = re.sub(r&quot;(http|https)?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*&quot;, &quot;[웹주소]&quot;, text).strip() text = re.sub(r&quot;pic\\.(\\w+\\.)+\\S*&quot;, &quot;[웹주소]&quot;, text).strip() # 뉴스 저작권 관련 텍스트 제거 re_patterns = [ r&quot;\\&lt;저작권자(\\(c\\)|ⓒ|©|\\(Copyright\\)|(\\(c\\))|(\\(C\\))).+?\\&gt;&quot;, r&quot;저작권자\\(c\\)|ⓒ|©|(Copyright)|(\\(c\\))|(\\(C\\))&quot; ] for re_pattern in re_patterns: text = re.sub(re_pattern, &quot;&quot;, text).strip() # 뉴스 내 포함된 이미지에 대한 레이블 제거 text = re.sub(r&quot;\\(출처 ?= ?.+\\) |\\(사진 ?= ?.+\\) |\\(자료 ?= ?.+\\)| \\(자료사진\\) |사진=.+기자 &quot;, &quot;&quot;, text).strip() # 중복 문자 처리 text = repeat_normalize(text, num_repeats=2).strip() # 문제를 일으킬 수 있는 구두점 치환 punct_mapping = {&quot;‘&quot;: &quot;'&quot;, &quot;₹&quot;: &quot;e&quot;, &quot;´&quot;: &quot;'&quot;, &quot;°&quot;: &quot;&quot;, &quot;€&quot;: &quot;e&quot;, &quot;™&quot;: &quot;tm&quot;, &quot;√&quot;: &quot; sqrt &quot;, &quot;×&quot;: &quot;x&quot;, &quot;²&quot;: &quot;2&quot;, &quot;—&quot;: &quot;-&quot;, &quot;–&quot;: &quot;-&quot;, &quot;’&quot;: &quot;'&quot;, &quot;_&quot;: &quot;-&quot;, &quot;`&quot;: &quot;'&quot;, '“': '&quot;', '”': '&quot;', '“': '&quot;', &quot;£&quot;: &quot;e&quot;, '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', } for p in punct_mapping: text = text.replace(p, punct_mapping[p]) # 연속된 공백 치환 text = re.sub(r&quot;\\s+&quot;, &quot; &quot;, text).strip() # 개행 문자 &quot;\\n&quot; 제거 text = text.replace('\\n', '') return text 사용한 전처리 코드는 위와 같습니다. 해당 방식으로 Bad characters, Email, Special Character, image link, duplicated, punctuation 등에 대한 cleansing 작업을 진행할 수 있었습니다. 그렇게 전처리된 데이터의 예시는 아래와 같습니다. 하지만 막상 수집한 데이터를 EDA 해보니 2만자 분량의 글을 생성해야 하는데 2만자를 초과하는 글이 하나도 없었습니다. 때문에 하나의 모델이 2만자를 한 번에 생성하고자 하면 분명이 내용적으로 많은 문제를 일으킬 것으로 예상했습니다. 따라서 저희가 모은 데이터를 효율적으로 활용하면서 적절한 글을 생성하기 위한 방법을 고민하게 되었습니다. 주제 선정 및 Retrieval한 대의 T4 GPU 에서 저희가 수집한 대량의 데이터를 학습시키고 하나의 주제가 관통하는 일관성있는 글을 쓰기엔 시간적으로, 그리고 하드웨어적으로 제약이 존재했습니다. 비슷한 방식을 찾아보던 중, Open Domain Question Answering 과 Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks 논문을 보고 Retrieval 을 통해 어떠한 키워드 및 주제에 유사도가 높은 데이터만을 학습시키면 해당 주제에서 기가막히는 글을 생성해 낼 것이다는 아이디어를 떠올리게 되었습니다. 하나의 모델에서 한 번에 긴 글을 생성해내는 것보다는, 각각의 주제에 professional 한 작가모델을 여러 개 생성해내고 그 모델들로 생성해낸 결과를 조합하는 방식입니다. 이러한 방식으로 주제와 Prompt만 잘 선정한다면 Supervised 하게 저희가 쓰고자 하는 내용을 쉽게 유도할 수 있을 것이라고 생각하였으며 일관성 면에서도 하나의 모델을 사용하는 것보다 좋은 결과가 있을 것이란 예상을 하게 되었습니다. 주제 선정주제에 대한 Retrieval 을 진행하기 위해서는 우선 저희가 쓰고자 하는 글의 주제를 선정할 필요가 있었습니다. 제 3회 AI x Bookathon 의 주제는 함께 였습니다. 해당 주제를 듣고 수많은 아이스 브레이킹을 하며 어떤 주제를 통해 글을 써내려가는 것이 좋을지에 대해 굉장히 많은 고민을 했습니다. 최종 주제의 아이디어는 미하일 바흐친 의 대화이론 철학으로부터 얻게 되었습니다. 바흐친은 다양한 관념과 목소리들이 어떠한 인식적 토대를 통해 조화로운 전체를 형성해 나가는 과정을 대화이론 으로 소개합니다. 대화이론의 핵심은 나 와 자아 에서 떠도는 단성악적 목소리들로부터 점점 의식과 관념의 다양성을 인식하며 다성악적인 전체를 나 와 타자 의 관계를 통해서 형성한다는 것입니다. 저희는 이 대화주의 철학에 공감하며 불특정한 개인이 함께 이지 않음에서 얻게되는 괴로움과 고뇌로부터 찰나의 순간 나 와 나 의 관계, 그리고 나 와 타자 의 관계성을 인식하며 추억과 행복감을 느끼는 스토리라인을 구성하게 되었습니다. 다만 이러한 철학과 감정 중심의 글은 너무 abstract 할 수 있다는 판단이 들었고, 최종적으로 만들어야 하는 글은 수필의 형태였기 때문에 긴장감을 조성하기 위한 사건, 함께라는 주제에 어울리는 교훈을 내포하기 위하여 소주제와 제목을 선정하게 되었습니다. 그렇게 선정된 내역은 아래와 같습니다. 저희는 이와같이 소주제를 선정하였고, 각각의 소주제에 해당하는 문장들을 Query 로 하여 Retrieval 을 진행하기로 하였습니다. Retrieval Open Domain Question Answering 과제에서는 Query 가 들어오면 Wiipedia 에서 주어진 쿼리와 유사한 Passage 들을 가져오는 Retrieval 시스템이 존재합니다. 이와 유사하게 저희의 소주제를 쿼리로 하여 수집한 약 3만 건의 데이터에서 유사도가 높은 문장들을 가져오는 Retrieval 을 구현하게 되었습니다. TF-IDF, Dense Passage Retrieval 등 다양한 방법이 많았지만 저희는 bm25 를 사용한 Elastic Search 를 사용하게 되었습니다. Elastic Search 는 오픈소스 검색엔진으로 빠른 속도로 방대한 양의 문서를 검색할 수 있습니다. BM25 scoring 알고리즘과 Nori Tokenizer 를 사용하여 저희가 수집한 데이터의 Sparse Embedding 을 만들어냈고 쿼리와의 유사도를 기반으로 관련 주제에 걸맞는 알맞은 Text 를 불러오게 하였습니다. 하지만 Retrieval 에 Pilot Test 를 진행하고 정성적으로 평가하였을 때 생각보다 의미에 매칭되는 텍스트가 별로 없었습니다. 이것의 원인으로는 Sparse Embedding 의 특성상 단어의 빈도, 문장 길이 등을 고려하기 때문에 저희의 소주제와 의미적으로 유사한 텍스트를 가져오지 못한다는 것을 에상할 수 있었습니다. 따라서 저희는 소주제를 대표하는 Keyword 를 일일이 뽑아내었고, 그 중에서 총 7개의 Query 를 만들어낼 수 있었습니다. 7개의 쿼리는 아래와 같습니다. 함께라는 것은 희망과 함께 우울과 함께 불안과 함께 죽음과 공존 자아와 함께 행복한 추억 이 때 전체 주제인 ‘함께’ 를 결합하여 주제에 벗어나지 않는 텍스트 중에서만 유사도를 고려할 수 있게 하였습니다. 이렇게 각각의 쿼리에 대하여 500개의 데이터 샘플들을 모을 수 있었습니다. 모델 선정 및 학습저희가 최종적으로 사용한 모델은 skt/ko-gpt-trinity-1.2B-v0.5 입니다. 시중에 공개되어 있는 사전학습 생성모델 중 가장 많은 parameter 를 가지고 있으며, GPT-3 논문에서 소개하듯이 Parameter 의 개수가 모델 성능으로 이어진다는 것에서 영감을 받아 최대한 큰 모델을 사용하고자 했습니다. 또한 MindsLab 에서 제공해준 모델과의 benchmark 를 직접적으로 확인할 수가 없어서 pilot test를 정성적으로 진행했을 때 ko-gpt-trinity 를 사용하는 것이 더 일관된 문장을 생성할 수 있겠다고 판단하게 되었습니다. 하지만, T4 의 한계는 여전히 남아있었습니다. Batch size 를 1로 하여도 항상 out-of-memory 문제가 발생하였습니다. 때문에 저희는 24개의 Decoder Layer 중 12개의 Decoder Layer 를 Freezing 시키고, Half Precision 을 사용함으로써 겨우 학습을 진행할 수 있었습니다. 또한 batch 크기를 최소한으로 했기 때문에 Accumulation step 을 활용하였습니다. 이렇게 각각의 Keyword 에 대해 Retrieval 로 가져온 500개의 Sample 들을 Fine-tuning 시켜서 총 7개의 수필작가 모델을 만들어낼 수 있었습니다. 수필 생성위에서 만들어낸 7개의 수필작가 모델로 하여금 글을 생성하게 했을 때 Prompt 로 주어지는 텍스트, 그리고 generate method 의 argument 에 따라서 매우 다른 결과들이 생성되었습니다. 또한 Beam search 를 사용했을 때 Repetition Problem 이 발생했기 때문에 Top-P Sampling 방식을 사용하여 Generate 를 진행했습니다. 이 때 사용한 코드는 아래와 같습니다. 12345678910gen_ids = model.generate(torch.tensor([input_ids]), max_length=1024, repetition_penalty=2.0, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, do_sample=True, top_k=30, top_p=0.95, use_cache=True) 하지만 Max Length 를 길게 잡았을 때 여전히 생성된 텍스트에 일관성이 존재하지 않는 현상이 발생하였습니다. 따라서 호흡을 짧게 가져가며 문장을 생성하였고, 생성된 문장을 다음 입력의 문장으로 하여 자연스러운 문맥을 이어갈 수 있도록 처리하였습니다. 이 때 생성된 문장을 length 단위로 계산하기 때문에 문장이 중간에 끊겨버리는 경우를 방지하기 위하여 문단 단위로 글을 생성하도록 하였습니다. 그렇게 사용한 최종 생성 코드는 아래와 같습니다. 1234567891011121314151617181920212223242526272829303132333435total = list(range(3000)) with tqdm(total=len(total)) as pbar: while len(gen_txt) &lt; 3000: if &quot;\\n&quot; in text: text = text[: text.index(&quot;\\n&quot;)] input_ids = tokenizer.encode(text) gen_ids = model.generate( torch.tensor([input_ids]), max_length=128, repetition_penalty=2.0, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, bos_token_id=tokenizer.bos_token_id, do_sample=True, top_k=30, top_p=0.95, use_cache=True, ) generated = tokenizer.decode(gen_ids[0, :].tolist()) if &quot;\\n&quot; in generated: gen_txt += generated[: generated.index(&quot;\\n&quot;)] if len(gen_txt) &gt; 1000: break elif len(gen_txt) &lt; 128: gen_txt += generated else: gen_txt += generated[len(text) :] pbar.update(len(generated)) splited_sentences = split_sentences(generated) if len(splited_sentences) &lt; 2: text = splited_sentences[-1] else: text = splited_sentences[-2] + &quot; &quot; + splited_sentences[-1] sleep(0.1) 그렇게 위와 같은 방식으로 7개의 수필작가 모델로 하여금 소주제에 해당하는 글을 작성하게 함으로써 보다 자연스럽게 이어지는 작품을 만들어낼 수 있었습니다. 최종 작품은 이곳에서 보실 수 있습니다. 마지막으로이렇게 무박 2일동안 진행되었던 프로젝트를 무사히 마칠 수 있었습니다. 부족한 점도 많고, 아쉬운 점도 많았지만 짧은 시간안에 다른 팀들과 차별화된 방법론으로 매우 가치있는 작품을 생성해낼 수 있어서 굉장히 좋은 경험이었다고 생각합니다. 이러한 대회를 제공해주신 성균관대학교와, 마인즈랩, 그리고 네이버 클로바팀에게 감사의 인사를 드리며 마치겠습니다. 감사합니다.","link":"/AI-Bookathon-%EB%8C%80%EC%83%81-%ED%9B%84%EA%B8%B0/"},{"title":"42Seoul 마지막 공통과제를 앞두고, 뼈문과 비전공자가 경험한 1년간의 회고록","text":"42Seoul 지원과 La Piscine(1개월 집중교육) 까지2019년 11월, 우연히 학교에서 이노베이션아카데미의 42Seoul 포스터를 보지 않았었다면, 지금 내 인생은, 개발자로의 길은 어떻게 되었을까 하는 생각을 가끔씩 하게 된다. 정말이지 답도 없고 길도 없고 막막한 순간들만 있진 않았을까…? 나에게 42Seoul은 참 좋은 동반자고 좋은 멘토였다. 나는 피아노를 전공하려고 했었던, 일반고등학교 문과 출신에 독어독문학을 전공한 뼛속까지 문과에 비전공자이다. 그런 내가 마지막 과제를 남겨두고 있는 지금, 지금까지의 과제들을 수행하면서 느꼈던 점들과 성장과정, 42Seoul에서의 생활들에 대해서 회고록을 작성해볼까 한다. 지금 생각해보면 참 까마득하다. 작년 11월, 아무것도 모르던 내가, 코딩을 업으로 삼을 것이라고 생각지도 못했던 내가 당당히 온라인 테스트에 합격하고 곧바로 체크인 미팅과 창의캠프, 그리고 한 달간의 La Piscine 까지, 본과정에 오기 위해 쉴새없이 달렸던 것 같다. 처음에는 그저 코딩이 유행하니까, 인공지능이 급성장하니까 호기심에 신청해보았다. 물론 그 호기심만으로 지원했던 것은 절대 아니다. 돈도 준다고 하지, 아이맥도 쓸 수 있다고 하지, 비전공자 생도, 졸업예정자가 아닌 일반 사람들도 지원가능하다 하지 조건이 너무 좋았다. 그렇게 나의 42 Life가 시작되었다. 이때까지만 하더라도 영국 엘리트 교육을 모방했네, 결국 헬조선화 되겠구나, 이런 부정적인 시선들의 입장도 들어왔었다. 한국에서, 특히 정부에서 주관하는데, 심지어 교육도 시켜주는 게 아니라 자기가 스스로 공부하는 시스템이기에 충분히 있을법한 이야기들이다. 그리고 나 역시 그런 입장들에 어느정도 동의했었다. 작년 12월, 이노베이션 아카데미 개소식에 참여할 수 있었다. 그 때 이민석 학장님의 말씀, ‘소프트웨어 개발자가 세상을 바꾼다.’ 이 한마디가 너무 좋았다. 단순하게 인공지능을 공부해보고 싶다던 비전공자 문과생이었던 내가, 세상을 바꿀 개발자가 되고 싶다는 생각을 하게 되기 까지는 그렇게 오랜 시간이 걸리지 않았다. 딱 저 한 마디, 대학입시에 합격했을 때보다 가슴뛰던 그 설렘은 정말이지 오랜만에 느껴보았던 것 같다. 그 한 마디가 42에 대한 온갖 부정적인 입장을 무너트려주었다. 그것이 내가 42Seoul에서 버틸 수 있었던 원동력이 되었던 것 같다. 온라인 테스트에 합격한 이후, La Piscine이 끝날때까지 참 많은 것이 변하고 바뀔 수 있는 시간이었던 것 같다. 특히 창의캠프를 갔다오고 나서 ‘왜 이렇게 퀄리티가 좋지? 3인실 호텔방을 그냥 숙소로 준다고? 정부기관은 역시 다르긴 다르구나’ 하면서 감탄했던 것, 첫 번째 과제를 마주하고 ‘아 xx,, shell이 뭐야 대체…’ 라며 욕을 했던 것, 첫 번째 시험에서 10분이 지나자 로그인 못 한 사람들을 불러모아서 “이렇게 저렇게 하면 될거에요~” 라고 알려주는 줄 알았으나 20분이 지나도 돌아오지 않던 사람들을 보면서 ‘뭐지…?’ 싶었는데 알고보니 집에 가라고 했던 충격적인 사실, 그 때 당시에는 매우 어색했으나 지금은 너무나도 당연해졌다. 이렇게 42 Seoul에, 42 Style에 나도 모르게 융화되고 적응되었다. 치열했던 1월의 La Piscine. 1월에 La Piscine을 할 수 있었던 건 나에게 매우 행운이었다. 코로나가 유행하기 전이었기 때문에 마스크를 착용하지 않았고, 매일 클러스터에 출석하며 눈치보지 않고 사람들과 대화할 수 있었으며, 오아시스에 모여 커피를 마시며 시험에 떨어졌다는 절망의 목소리도, 다양한 목소리들이 오고 갔다. 이것이 ‘한국에서 일어날 수 있는 일인가’ 라는 생각이 들었다. C를 공부한다는 것은 소프트웨어 개발자라면 모두가 거쳐가야 할 필수관문이라고 익히 들어왔다. 하지만 나는 어차피 인공지능에 관심이 있었고 그러기 위해서는 Python을 필수적으로 익혀야만 했기에 당장도 급한데 내가 42 과정에 참여하면서 C를 공부해도 되는것일까? 싶은 걱정이 들었었다. 물론 그 당시야, 아무것도 몰랐을 때였고 42Seoul 도 자리를 잡기위해 시간이 필요했었으니 정보도 많이 없었다. 그렇게 혼자 끙끙대며 고민했었는데, 지금에서는 당당하게 말할 수 있을 것 같다. “Python을 주력으로 사용할 것이라도, Web 개발자가 될 것이기에 다른 언어와 프레임워크만을 사용할 것이라도, C를 요즘 누가 쓰냐고 왜 공부하냐고 욕을하더라도 42Seoul에 조금이라도 관심있으면 ‘무조건’ 참여하세요” 라고 말이다. 물론 본과정에 합격하고 공통과제에 집중하게 되면 당연히 다른 언어 사용할 기회는 없다고 봐도 된다. 하지만 C를 통해 진행하는 42의 커리큘럼은 그 모든 걱정거리들을 잠재울 수 있을 정도로 체계적이며, 단순히 ‘C 언어’ 만 공부하고 체득하는 것이 아니라 소프트웨어 개발자로서의 태도와, 학습방법을 스스로 체득하고 적용할 수 있는 ‘능력’, 그리고 ‘커뮤니케이션 능력’을 기를 수 있다. 가장 핵심적인 42Seoul의 시스템은 바로 교수와 선생이 없는 ‘동료 학습’이지 않을까 싶다. 이 시스템 속에서 옆에 있는 동료가 서로의 선생님이자 교과서가 되는 것, 또한 그 지식을 공유하는 과정에서 발생하고 향상되는 커뮤니케이션 능력과 학습방법은 어느 기업을 가더라도 필수적으로 지녀야할 핵심가치이자 유용한 태도라고 생각된다. 이것을 얻을 수 있다는 것만으로도 주력으로 하게 될 언어와 기술스택에 조금 멀리 돌아가게 되더라도 충분히 42의 일원이 될 필요성은 충분하다고 생각한다. 본과정 합격, 그 이후 2020년 2월 19일. 본과정에 합격하여 방방 뛰면서 온 집을 휘젓고 다녔었다. 대학 합격할 때보다 더 좋았다.. ㅋㅋ 그렇게 2월 24일부터, 42Seoul 에서의 본과정 생활이 시작되었다. 처음의 목표는 공통과정을 6개월만에 끝내는 것이었다. 그것이 42Seoul 측에서 예상한 서클 돌파 기간이기에. 하지만 얼마 지나지 않아 그것은 매우 어려운 일이라는 것을 깨닫게 되었다. 나는 정말 아무것도 아는 게 없었기 때문에. 본과정을 처음 대하는 나의 태도는 매우 자만스러웠다. Piscine도 통과했겠다 두려울 게 없었던 모양이다. 가파른 러닝커브를 그려야겠다며, 과제 뭐 대충대충 하고 빨리 넘겨야지 라는 하찮은 생각이며, 나조차도 42를 대하는 태도가 정립되지 않았었기 때문에 여기에서 무엇을 배워서 얻어야겠다기 보다는 그저 빨리 블랙홀을 넘겨야지, 빨리 월급들어왔으면 좋겠다 등 일상적인 고민과 바람에 홀려 절대적인 시간의 척도를 계산하지 못했던 것 같기도 하다. 코로나와 재택학습, 그리고 성장지금와서 가장 기억에 많이 남는 과제는 무엇이었냐고 물어본다면, 나는 고민없이 ft_printf 과제를 가장 먼저 떠올릴 것이다. 이 과제는 정말 어려운 과제는 아니다. 물론 보너스에 도전한다면 난이도가 기하급수적으로 솟아오르긴 하지만. 하지만 나는 보너스도 안했으면서 이 과제에만 두 달이라는 시간이 걸렸다. 환경적인 문제도 있었다. 클러스터가 더 익숙해져 버린 나에게 코로나로 인해 집에서 과제를 한다는 것은 매우 어색했으며 힘든 일이었다. 물론 아이맥을 못쓴다는 이유도 크다. 또한 개인적으로 악재가 동시에 터졌기 때문에 마음이 정말 힘들었던 시간들을 보내야만 했다. ‘코로나 블루’ 라고 하는 우울감에 스스로에게 상처를 주고 갉아먹어가기도 했으며 모두가 그랬듯 제동이 걸린 삶이 너무나도 힘들었었기 때문이다. 이 두 달의 시간을 정말 잘 이겨내주어서 지금의 내가 있었던게 아닐까 싶다. 돌이켜봤을 때 이 두달의 시간을 통해 매우 가파른 성장을 할 수 있었던 것 같다. 실질적으로 ft_printf 라는 과제에서 배울 수 있는 새로운 지식은 별로 없다. 물론 입출력스트림/가변인자 등 놓쳐서는 안 될 중요한 부분들이 있긴 하지만 그게 전부인걸. 이 과제를 통과하기 위해서는 그런 지식들을 기반으로 한 코딩보다는 TDD 방식의 테스터케이스를 얼마나 잘 맞추느냐가 관건이 된다. 그런 면 때문에라도 나는 이 과제를 별로 좋아하지 않았고, 테스트하기를 꺼려했으며, 아 이런 것까지 맞춰야돼? 싶을 정도로 욕을 많이 했다. 또한 42의 코딩컨벤션 규약도 맞추느라 진이 빠졌다. 그렇게 더러운 하드코딩들과 필수파트도 구현을 못하면서 보너스를 하겠다는 마음가짐이 겹쳐서 이도저도 아닌 코드들만 계속해서 생산해내게 되었다. 결국 구조를 바꾸고 또 바꾸고, 이해하지 못할 것들은 갈아치워버리고 과감하게 지우고, TDD 방식으로 코딩을 하고 다 갈아엎었다가 처음부터 다시 구조를 설계하고. 이런 작업을 두 달동안 하루에 두 세번씩 매일같이 했다. 마지막엔 결국 내가 지쳐서 보너스를 하지 않고 하루만에 처음부터 다시 짠 코드로 제출하여 통과했지만. 내 방식이 올바른, 또는 42에서 지향하는 방식이 아닐지도 모르겠다. 하지만 이렇게 내 코드에 정을 가지고 욕심을 가지고, 더 가독성 좋은 혹은 더 짧은 코드를 만들어내던 경험이 지금의 끈기와 열정을 가지게 해준 동력이 된 것 같다. 이렇게 두 달을 삽질하고 나서 받게 되는 동료평가는 너무나 달콤했다. ft_printf 과제는 권장기간이 1주일밖에 되지 않기에, 같은 차수 동료들을 보면 해당 서클 과제들을 모두 통과해놓은 시기였다. 그래서 그런지 평가를 해주신 분들 모두 이 과제를 끝낸 사람들이었는데, 그런 분들에게 내 로직을 설명하고, 틀렸는지 맞았는지 같이 논의하고 내 코드를 뜯어보고, 예외처리는 모두 잘 되었는지, 코드는 어떻게 다이어트 시켰는지, 함수 추상화정도를 살펴보고 잘 구현했다는 평을 들었을 때 그렇게 기분이 좋을 수가 없었다. 이 경험을 바탕으로 나는 다른 과제들에도 임하게 되었고, 속도가 붙어서 대부분 2주~1달 정도로 과제들을 하나씩 해결할 수 있었다. 물론 ft_printf 과제 말고도 기억에 남고 후기를 남기고 싶은 과제들도 매우 많다. 그것들은 추후에 하나씩 다시 정리를 해볼 생각이다. 사회적 거리두기가 완화되고 다시 클러스터에 출입할 수 있게 될 때까지 약 3개월 동안 집에서의 학습을 쾌적하게 하기 위해 4k 모니터와 모니터암을 구매하고, 기계식 키보드를 구매하는 등, 이 기간동안 점점 개발자다운 생활을 하게 된 것 같다. 그리고 이 글을 작성하는 지금, 클러스터가 또 문이 닫혀버렸다. 올해는 개포동을 가지 못하겠고만..ㅎ 재택학습했다가, 클러스터에 갔다가, 또 다시 재택학습을 했다가.. 나 말고도 대부분 비슷했을 것으로 생각이 든다. 그렇게 본과정을 시작한지 약 10개월이 채 되지 않은 지금, 대망의 마지막 과제만을 남겨두고 있다. 더욱 42 스럽게42Seoul에서의 생활은 정말이지 자유로움 그 자체였다. 고등학교를 졸업하고 갓 대학에 입학한 새내기들이 처음으로 밤새 술을 마시고 아무걱정없이 놀고 하고 싶은 동아리를 지원하며 틈틈이 공부하는 것처럼 새로운 클러스터에서 이야기하고 놀고 공부하고, 너무 재미있는 생활들이었다. 그런 생활적인 부분들을 공유하고 싶다. _나는 어떻게 놀았고 어떤 동아리를 했고, 어떻게 공부하고 진로를 탐색해 나갔는지_. 가장 최근의 이야기를 먼저 해볼까 싶다. 가장 최근에 진행한 과제는 5서클의 Webserv 과제이다. Socket Server를 만들고 Custom Nginx 를 구현하는 것으로, HTTP/1.1 의 RFC 문서를 달달달 읽어야 하는 규모가 어마어마어마무시한 과제이다. 이 과제는 세명이서 진행하는 프로젝트로 처음으로 Github의 프로젝트 관리의 필요성을 느꼈던 과제였다. 앞에도 말했지만 이전서클과 비교했을 때, Webserv는 규모가 너무너무 크다. 위 사진처럼 학습시간을 확보해내지 못했다면 아마 팀플에 지쳐 나가떨어졌을 것이다. 왜냐면 난 아직도 아는 것이 많지 않은 뼈문과 비전공자 개발자이기 때문이다. 하지만, 그런 내가 해내버렸다. 이렇게 할 수 있었던 가장 큰 이유는 너무 감사하게도 열정많고 좋은 팀원분들을 만나게 된 것이지 않을까 싶다. 우선은 아는 것이 많지 않다는 것에 자신없어 하는 것이 아니라 치열하게 알기 위해 노력하고 많은 시간을 할애한 것. 이것이 팀플을 알차게 해낼 수 있었던 버팀목이지 않았을까 싶다. Webserv 과제를 시작하고 나서 가장 크게 달라진 것들은 다음과 같다. 처음으로 구글링만으로 해결되지 않는 지식의 갈망을 느껴서 책을 구매하기도 하고, 런타임 시 많은 시간이 걸려서 조금이라도 병목을 줄이기 위해 자료구조마다 시간복잡도를 고려하여 코딩을 하고, 다른 팀원들이 이해하기 쉽도록 더욱 가독성이 좋은 코드, 짧은 코드를 쓰기 위해 노력하고, 더 나은 소통을 위해 코딩컨벤션과 규약을 고민해보았다는 것. 또한 이제와서야 앞서 했던 프로젝트들을 왜 했는지 그 이유를 조금은 알 수 있을 것 같았고, 정말 체계적으로 커리큘럼이 이루어져있다는 것을 알게 되었다. 가장 크게 달라진 것들이라고 이야기를 했는데, 맞다. 나는 지금까진 구글링만으로 모든 지식을 커버했고, 그렇다보니 아는 것들은 있어도 체계가 잡혀있지 않았다. 또한 병목을 줄이기 위한 노력을 하지 않았고, 이정도면 디펜스하기에 충분하다 싶은 코드가 나왔을 때 더 발전시키기 위한 리팩토링을 전혀 하지 않았다. 그랬던 내가 이번 과제를 만나고 성격이 변한건지, 팀플의 힘인건지, 이것 역시 42의 교육 정책자들이 예상을 했던 것인지. 참 신기하다. Webserv 과제를 시작하기 전까지 나와 비슷하게 공부를 했던 분들은 내가 너무 대충했고, 공부를 안했고에 대해서 너무 걱정하지 않으셔도 될 것 같다. 5서클에 오면 어차피 하게 된다. 어차피 이전에 짰던 코드를 리팩토링 하게 된다. 42의 커리큘럼은 잔머리가 통하지 않았다. Webserv 과제에도 할 말이 너무너무 많다. 이것도 따로 블로그에 정리해야지..ㅋㅋㅋ 이렇게 공부했던 것과 더불어 진로에 대한 고민도 치열하게 했다. 멘토링을 받기도 하고 학장님께 조언을 드리기 위해 용기내어 DM을 보내기도 했다. 개인적인 이야기를 조금 보태자면, 나의 꿈은 ‘교수’가 되는 것이었다. 그렇기 때문에 내가 코딩을 시작하기 이전에도 나의 모든 커리어는 대학원에 맞추어져 있었다. 전산언어학을 전공하신 지도교수님 아래에서 유일한 학사 출신으로 정부 연구과제에 산학협력하여 참여하기도 하고, 지속적인 교수님들과의 컨택을 통해 대학원에 가는 것이 거의 확정과 다름이 없었다. 하지만 학장님과의 면담 이후로 이상주의적이었던 내가 보다 현실을 바라볼 수 있게 되었다. 그리고 결과적으로 대학원 진학을 포기했다. 내가 그간 느꼈었던 대학원에 대한 회의도 물론이거니와 기업에서의 연구가 학계보다 훨씬 더 진보해있다는 것, 내가 추구했던 가치들을 기업에서도 충분히 느낄 수 있다는 것 등이 이유가 될 것 같다. 보다 자세한 이야기를 하기에는 너무 글이 길어질 것 같아서,,, 이 글을 본 42Seoul 본과정생, 혹은 42를 새로 시작하려는 학생들 중에 나와 비슷하게 진로에 대한 고민을 하고계신 분들이라면 주저말고 학장님께 조언을 구해보길 강력하게 추천한다. 또한 42Seoul에서 나와 비슷한 관심사를 가지고 계신 분들과 스터디를 진행하고 공모전에 나가기도 했다. 정말 무슨 배짱이었는지, 슬랙에서 스터디원들을 모집하고 해외 논문이나 자연어처리에 대한 공부를 했었다. 코로나로 인해 더이상 진행할 수가 없어서 일단락되긴 하였지만, 이렇게 42Seoul은 자유롭게 사람들을 모집하고 공부를 할 수 있다!!! 이게 장점이라면 엄청난 장점이 아닐까 싶다. 42NLP 팀원들과 공모전에 나가기도 했다. 아쉽게 탈락해버렸지만(진짜?) 처음으로 42Seoul 카뎃분들과 진행한 사이드프로젝트였고, 이 과정에서 정말 배운 것이 많다. 그리고 무엇보다 재밌었다. 이렇게 나는 42Seoul 과제만 한 것이 아니라 내가 하고싶은 공부도 병행하고, 프로젝트도 하면서 목표에 한 발짝씩 다가설 수 있었던 것 같다. 이쯤되면 아니 얘는 뭐 공부만 했나? 라고 생각하실 분들이 계실 것 같다… ㅋㅋㅋㅋㅋ 하지만 42Seoul 내에는 공부뿐만 아니라 노는 동아리도 있다. 나는 42Band에서 키보드를 맡고 있으며, 코로나가 심해지기 전에는 매주 합주를 하며 즐거운 시간들을 보냈다. 악기를 처음 배워보는 분도, 노래를 처음 해보시는 분도 계셨지만 어느정도 합주가 이루어진다는 게 너무 신기했던 경험이다. 진짜,, 합주하는 시간만큼은 엄청난 힐링이었는데 코로나 진짜 ㅠㅠ 조금 완화되면 온라인 스트리밍으로 공연도 진행할 예정이다! 마치며42Seoul에서의 약 1년간의 경험은 나중에도 잊지 못할 추억으로 남을 것 같다. 빡세게 공부하는 것은 물론 너무 좋은 동료들과 이야기하는 것도 정말이지 즐거운 일이기 때문이다. 물론 아쉬운 일들도 정말 많다. 애초에 처음부터 제대로 공부를 했으면 어땠을까 혹은 더 많은 활동들을 했으면 어땠을까 싶기도 하다. 하지만 이너서클 돌파를 앞두고 있는 지금, 그런 아쉬움보다는 42에 대한 감사함이 더 크게 느껴진다. 이게 없었다면 나는 여전히 허우적거리고 있었을 것이며 이상과 현실속에서 터무니 없는 고민들만 하고 있었을 것 같다. 내 회고록이 예비 42분들, 또는 이미 본과정을 진행중이신 분들께 어떤 생활과 공부를 해야할 지, 또는 42를 선택하는 것에 있어서 많은 도움이 되었으면 좋겠다. 한 가지 당부드리고 싶은 것은, 42에서는 누구도 알려주지 않는다는 것이다. 본인이 스스로, 본인의 가치를 창출할 수 있도록 최선의 노력을 다했으면 좋겠다. 42Seoul에 대한 질문거리나 궁금한 점들이 있으시면 댓글로 남겨주시면 감사하겠습니다. 긴 글 읽어주셔서 정말 감사합니다.","link":"/42Seoul-%EB%A7%88%EC%A7%80%EB%A7%89-%EA%B3%B5%ED%86%B5%EA%B3%BC%EC%A0%9C%EB%A5%BC-%EC%95%9E%EB%91%90%EA%B3%A0-%EB%BC%88%EB%AC%B8%EA%B3%BC-%EB%B9%84%EC%A0%84%EA%B3%B5%EC%9E%90%EA%B0%80-%EA%B2%BD%ED%97%98%ED%95%9C-1%EB%85%84%EA%B0%84%EC%9D%98-%ED%9A%8C%EA%B3%A0%EB%A1%9D/"},{"title":"Brute Force Search","text":"브루트 포스 알고리즘 - Brute Force Search암호학에서의 Brute force attack이 아닌 알고리즘으로서의 브루트포스이다. 이 알고리즘의 핵심은 완전탐색, 가능한 모든 경우의 수를 탐색하면서 요구에 부합하는 결과를 가져오는 것이다. 장점은 모든 경우를 검사하므로 100%의 확률을 가졌지만, 모든 경우를 검사하는 만큼 효율성이 떨어질 수 있다. 문제해결 방법 주어진 문제를 선형구조로 구조화함. 구조화된 공간안에서 적절한 방법으로 해를 구성할 때까지 탐색한다. 구성된 해를 정리한다. 예제 및 알고리즘백준 블랙잭 문제정리카드의 개수 N(3 &lt;= N &lt;= 100)과 딜러가 지정한 카드의 합 M(10 &lt;= M &lt;= 300,000), 그리고 카드에 쓰여있는 수들이 입력된다. 12카드의 개수 N(3 &lt;= N &lt;= 100)과 딜러가 지정한 카드의 합 M(10 &lt;= M &lt;= 300,000),그리고 카드에 쓰여있는 수들이 입력된다. 1M을 넘지 않으며 M에 최대한 가까운 카드 3장의 합을 출력한다. 입력으로 들어온 카드가 5, 6, 7, 8, 9 이고 딜러가 지정한 블랙잭(합)이 21이라고 해보자. 들어온 카드를 배열로 생각하면 12345678sum1 = Card[0] + Card[1] + Card[2]sum2 = Card[0] + Card[1] + Card[3] ...sum10 = Card[2] + Card[3] + Card[4] 이런식으로 나타낼 수 있다. 여기에서 sum1 ~ sum10 중에 21과 가장 가까운 합을 구하는 것이므로, 3장의 카드를 더한 합을 모두 탐색하는 브루트 포스의 예제라고 할 수 있다. 1234567891011121314151617181920212223#include &lt;stdio.h&gt;int main() { int N, M; int tmp, sum = 0; int num[100] = {0, }; scanf(&quot;%d %d&quot;, &amp;N, &amp;M); for (int i = 0; i &lt; N; i++) scanf(&quot;%d&quot;, &amp;num[i]); for (int i = 0; i &lt; N; i++) { for (int j = i + 1; j &lt; N; j++) { for (int k = j + 1; k &lt; N; k++) { tmp = num[i] + num[j] + num[k]; if (tmp &gt; sum &amp;&amp; tmp &lt;= M) sum = tmp; } } } printf(&quot;%d\\n&quot;, sum); return (0);}","link":"/Brute-Force-Search/"},{"title":"CNN 첫걸음","text":"지금까지 배운 다층신경망(MLP) 구조는 각 뉴런들이 선형모델과 활성함수로 모두 연결된 (fully connected) 구조였다. Convolution 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다. Convolution 연산의 수학적 의미는 신호(signal)을 커널을 이용해 국소적으로 증폭 또는 감소시켜서 정보를 추출 또는 필터링하는 것이다. 커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용한다. 2차원 Convolution 연산 커널을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조 입력크기를 (H, W), 커널 크기를 $(K_H, K_W)$, 출력 크기를 $(O_H, O_W)$ 라고 하면 출력 크기는 다음과 같이 계산한다. O_H = H - K_H + 1\\\\ O_W = W - K_W + 1 채널이 여러개인 2차원 입력의 경우 2차원 Convolution을 채널 개수만큼 적용한다. 텐서를 직육면체 블록으로 이해하면 좀 더 이해하기 쉬움. Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다.","link":"/CNN-%EC%B2%AB%EA%B1%B8%EC%9D%8C/"},{"title":"CPython에서 -5 ~ 256 사이의 값을 가지는 정수타입 객체","text":"CPython에서는 흥미로운 사실이 발견된다. 변수에 값을 대입하는 흔한 식 a = 1 에서 = 연산자는 1이라는 값을 a에다가 할당하겠다는 뜻이다. 이것은 1이라는 값을 가진 a라는 객체를 생성하겠다는 뜻과 같다. 12345678910&gt;&gt;&gt; id(5)10914624&gt;&gt;&gt; a = 5&gt;&gt;&gt; id(a)10914624&gt;&gt;&gt; b = 3 + 2&gt;&gt;&gt; id(b)10914624 위의 코드에서 5라는 정수는 10914624 라는 객체 id를 가지고 있다. 하지만 a와 b 변수에 5라는 값을 할당했는데도 새로운 객체가 생성된 것이 아닌 같은 id를 가지게 된다. https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong docs에 따르면 CPython 내부 메모리에는 -5 ~ 256 사이의 정수 값에 해당하는 객체가 미리 생성되어 있다. 이 범위의 값을 새로운 변수에 할당하게 될 때, 새로 객체를 생성하는 것이 아니라 미리 생성된 객체를 참조하게 된다. 그래서 위의 예제와 같은 일이 일어나게 된다. 123456789101112131415&gt;&gt;&gt; a = 5&gt;&gt;&gt; a is 5True&gt;&gt;&gt; id(a)10914624&gt;&gt;&gt; id(5)10914624&gt;&gt;&gt; a = 257&gt;&gt;&gt; a is 257False&gt;&gt;&gt; id(a)139848341129200&gt;&gt;&gt; id(257)139848341128016 범위를 벗어난 257의 객체 id는 1139848341128016 257이란 값을 a라는 변수에 할당했을 때 a 객체의 id는 1139848341129200 두 객체의 id가 다른것을 확인할 수 있다. 즉 범위를 벗어난 정수값을 새로운 변수에 할당했을 때에만 새로운 객체를 생성한다는 것이다. 1a is b is 연산자는 a와 b 값을 비교하는 것이 아닌, 서로 같은 객체인지를 판별한다. 1a == b 만약 같은 값을 비교하고 싶다면\\== 연산자를 사용하여 값을 비교할 수 있다. 12345&gt;&gt;&gt; a = 257&gt;&gt;&gt; a is 257False&gt;&gt;&gt; a == 257True 따라서 -5 ~ 256 범위의 int형 정수를 비교할 때는 is 연산자와 == 연산자 사용에 주의를 기울일 필요가 있다.","link":"/CPython%EC%97%90%EC%84%9C-5-256-%EC%82%AC%EC%9D%B4%EC%9D%98-%EA%B0%92%EC%9D%84-%EA%B0%80%EC%A7%80%EB%8A%94-%EC%A0%95%EC%88%98%ED%83%80%EC%9E%85-%EA%B0%9D%EC%B2%B4/"},{"title":"Computer Vision Applications (Semantic Segmentation and Detection","text":"Semantic Segmentation 이미지 픽셀 별 분류 과제이다. Dense Classification, Perl Pixel 등으로 불리기도 한다. Fully Convolutional Network output이 1000개의 채널이라고 하면, Dense Layer를 없애고 Fully Convolutional Network 로 변경하려는 것임. Fully Connected (Dense) Layer를 사용하는 것과 결과적으로 똑같음. 파라미터도 완전히 똑같음. 이러한 과정을 convolutionalization 이라고 한다. Transforming fully connected layers into convolution layers enables a classification net to output a heat map. While FCN can run with inputs of any size, the output dimensions(special dimension) are typically reduced by subsampling. So we need a way to connect the coarse output to the dense pixels. Deconvolution (conv transpose) 직관적으로 보았을 때 Convolution의 역 연산이다. special dimension을 키워주는 역할을 하게 된다. 엄밀히 말하면 convolution의 역 연산은 존재할 수 없다. 원래의 값으로 완전히 복원할 수는 없기 때문이다. 하지만 이렇게 생각하면 좋은 이유는 네트워크 구조를 짤 때 파라미터 크기와 네트워크 입출력을 계산할 때 역으로 생각하면 편하기 때문이다. DetectionSemantic Segmentation 이랑 비슷하지만, 퓨어픽셀을 이용한 히트맵을 찾는 것이 아니라 객체들의 bounding box를 찾는 과제로 만든 것이다. R-CNN R-CNN takes an input image, extracts around 2,000 region proposals (using Selective search), compute features for each proposal (using AlexNet), and then classifies with lenear SVMs. SPPNet RCNN의 가장 큰 문제는 이미지에서 2000개의 바운딩박스를 뽑았을 때, 2000개의 이미지 혹은 패치를 모두 CNN에 통과시켜야 한다. 즉 하나의 이미지를 처리하는데 CPU에서는 1분의 시간이 소요되었었다. 이미지 전체에 대한 컨볼루션 피쳐맵을 만들고, 뽑힌 바운딩박스 위치에 해당하는 컨볼루션 피쳐맵의 텐서만 뽑아온 것으로 CNN에 통과시킴으로써 훨씬 빨라지며, 한 번의 CNN으로 결과를 얻을 수 있다. Fast R-CNN 기본 컨셉은 SPPNet과 굉장히 유사하지만, 뒷단의 Neural Net(RoI)을 통해서 바운딩박스 리그레션과 분류를 진행했다. Faster R-CNN 이미지를 통해 바운딩박스를 뽑아내는 Region Proposal 역시 학습을 시키자는 것임. Region Proposal Network 이미지가 있으면 이미지 속 특정 영역(패치)이 바운딩박스로서 의미가 있는지 없는지(즉 영역에 물체가 있는지)를 판단하는 것이다. anchor boxes가 필요한데, 이것은 미리 정해놓은 바운딩박스의 크기이다. 내가 이 영역에 어떤 크기의 물체가 있을 것 같다라는 예상을 전제로 만들어 놓은 것임. Fully Conv 레이어가 사용됨. YOLO extremely fast object detection algorithm It simultaneously predicts multiple bounding boxes and class probabilities. No explicit bounding box sampling (compared with Faster R-CNN) 이미지 안에 찾고싶은 물체의 중앙이 그리드 안에 들어가면, 그 그리드 셋이 해당 물체의 바운딩 박스와 해당 물체가 무엇인지 같이 예측을 해야 한다. 이 때 B개의 바운딩 박스를 예측하게 된다. 이후 바운딩박스의 refinement(x/y/w/h)를 예측하고, 각각의 그리드셋이 속하는 중점의 오브젝트가 어떤 클래스인지 예측한다. 이 두 가지 정보를 취합하게 되면 박스와 박스가 어떤 클래스인지 예측할 수 있다.","link":"/Computer-Vision-Applications-Semantic-Segmentation-and-Detection/"},{"title":"CV &amp; NLP 선택가이드 - 업스테이지 서대원, 박선규","text":"CV 도메인에 대하여 (박선규)흔히 볼 수 있는 컴퓨터비전 태스크에 대한 사진 Taskonomy (CVPR 2018 Best Paper) 컴퓨터 비전에 관련된 26개의 태스크를 하나하나 정의하고, 한 태스크로의 모델을 학습시킨 후 전이학습에 대한 성능을 기준으로 두 태스크에 대한 관계(유사도)를 분석한 논문임. 태스크에 대한 연관관계보다는 태스크가 이렇게 많았구나..라는 인사이트를 얻게 됨. 이렇게 태스크가 많은데도, 학회를 가면 항상 새로운 것을 보게 됨. 연구분야가 굉장히 넓고 다양하다. 이미지에 관련된 것은 Convolution에 굉장한 의존성을 가지고 있다. 즉, Detection과 Segmentation 정도만 배우더라도 다른 컴퓨터비전 태스크를 접할 때 큰 어려움 없이 접할 수 있다. OCR ViT 컴퓨터 비전의 대부분 태스크는 일반적으로 CNN을 기반으로 이루어지는데, 최근에는 Vision Transformer를 많이 사용한다. NLP에서의 Trnasformer는 데이터가 많아질 수록 성능이 높아지는데, 이미지 역시 데이터가 많아지니까 Vision Transformer의 퍼포먼스가 높아졌다. 그러나 아직은 CNN 기반 모델들이 서비스에 들어갈 확률이 높다. Alias-Free GAN ViT가 사람의 개입없이 데이터만으로 성능을 높여보자는 시도였다면, Alias-Free GAN은 몇 가지 아이디어만으로 원래의 GAN 문제를 해결하였다. 결론컴퓨터비전이라고 하면 어떤 이미지나 시각정보를 다루는 방법론에 대한 연구이며, 시각 정보에 포함된 것들이 생각보다 매우 다양하다. 간단하게는 이미지부터 텍스트가 들어간 이미지 등, 생각보다 굉장히 다양한 서브태스크를 포함하는 굉장히 방대한 분야이며, 그 안에서 흥미가 있을 태스크를 찾아보는 재미가 있다. 또한 비전 관련된 커리큘럼을 선택하게 되면 Convolution 이라는 Operation과 CNN에 대해 굉장히 자세하게 배우게 될 것인데, 이것이 실 서비스에서 쓰이는 것들 중에는 이만큼 효율적인 것이 없기 때문에 부스트캠프에서 배우는 비전이란 지식이 굉장히 쓸모가 있을 것이다! NLP 도메인에 대하여 (서대원)NLP에 대한 정의 사람의 Document 를 해석하는 것에 밀접해있다. Document는 텍스트일 수도 있고 사람의 음성일 수도 있다. Document에서 의미를 뽑아내거나 혹은 단어 혹은 문장을 구별하거나 정보를 뽑아내는 모든 것을 포함한다. NLU(understanding), NLG(generation)에 대한 분야가 있다. Search 구글 검색창에는 전통적인 TF-IDF 가 이용되는 것이 아니라 언어모델이 들어가서 찾으려는 정보가 없어도 비슷한 정보들이 검색된다. Voice Assistant영화 HER에 나오는 인공비서를 만드는 것이 자연어처리 개발자들의 최종 낭만이지 않을까. 음성 인식을 먼저 하고 인식된 결과로 자연어처리를 하여 음성 합성까지 하는 태스크이다. Translation번역은 원래의 목적이 한 언어를 다른 언어로 변환하는 것인데, 해당 데이터가 다른 태스크를 학습하는 데 도움이 되기도 한다. Mono Lingual 태스크를 할 때 외국어 데이터로 Augmentation 할 수 있다. Applications can be decomposed into LNP Tasks다양한 NLP Task를 깊게 들어가다보면 결국에는 아래의 태스크들로 압축될 수 있다. 다양한 태스크를은 결국에 분류로 귀결이 된다. (단어 분류, 문장 분류, MRC나 QNA등도 결국에는 분류이다.) 숫자로 결과가 나오는 것이 아니라면 기본적으로 분류 태스크임. Trend 1: Transfer Learning with Laaaaaaaaaarge Models첫 번째 트렌드는 언어모델을 사용하는 것이고, 그 언어모델의 크기가 괴애애애애애앵장히 크다는 것이다. 대부분 언어모델들을 백본으로 삼아서 파인튜닝을 통해 학습을 하게 된다. GPT-2 가 나왔을 때만 해도 파라미터수 겁나 크다 생각했는데, GPT-3가 나왔을 때는 아.. 거대모델이 트렌드구나 생각할 수 있음. Trend 2: Retrieval-based ModelLarge Model의 경우 도서관의 모든책을 읽고 다 암기해! 와 똑같은 매커니즘이다. 하지만 Retrieval-based는 이런 정보들을 모두 하나의 모델에 넣는 것보다는 정보들을 각 모델에 나누고 책을 찾아서 답을 찾는 것처럼, Retrieval 된 Document 에서 정답을 찾는 것처럼 동작된다. DPR 같은 것들을 예시로 들 수 있다. Trend 3: MultimodalityText-to-Image(DALL﹒E: Creating Images from Text, OpenAI), Image-to-Text(Image Captioning) 결국에는 진행을 하다보면 다양한 형식의 데이터들이 들어오고, 데이터들이 연관된 Task를 학습하게 된다. 따라서 보통 사진/음성 등과 텍스트를 한 번에 학습시킬 상황이 발생할 수도 있음. Voice Assistant를 학습시킬 때 음성데이터도 텍스트와 같이 학습함. 네~와 네?는 다르듯이. 따라서 NLU 중 음성텍스트 이해와 관련된 분야도 생기고 있다. ML Engineers Implement ML Systems 데이터를 크롤링하고 전처리 (데이터 버저닝 능력) 하며 모델 딴에서는 신경망 관련된 아키텍처의 이해가 필요하고, 이해를 바탕으로 새로운 논문이 나왔을 때 논문 수치를 재현하는 능력, 그리고 그 능력을 바탕으로 새로운 태스크에서 높은 성능을 내는 능력이 필요하다. 다음은 MLOps 처럼 코드와 모델을 관리하는 능력이 필요하며, 마지막으로 모델을 서빙하기 위한 API를 만들거나 실제 Production에 들어갔을 때 해당 모델의 Log를 분석하고 관리할 수 있는 능력이 필요함. 위에 것들은 차근차근 언젠간 다 알게 되기 때문에 주니어 레벨에서는 자료구조나 알고리즘의 기본기를 탄탄히 하고 ML Model에 대한 기본기를 탄탄히 하면 나중에는 위의 것들이 필요하다는 것만 인지하고 있으면, 나머지 것들은 나중에 자연스럽게 습득이 된다. 처음에는 NLP와 CV중 하나의 도메인을 깊게 파면서 추후에는 아 다른 데이터까지 다루어야 한다는 것을 인지하고 있을 것! 사전 질문 Q&amp;AQ: CV, NLP 분야 일을 하면서 가장 보람있었던 순간: A: CV OCR 분야로 인턴십을 하면서 무료 폰트들에서 글자를 뽑아내어 폰트 분류기를 만들어보자는 일을 했음. 해당 프로젝트라는 초라한(?) 계기로 홍콩으로 프로젝트를 갔다온 것이 뿌듯한 기억으로 남아있음. A: NLP 본인은 문과였을 때부터 불평등 해소에 관심이 많았음. 정치를 통한 해소도 있겠지만 기술발전으로 불평등을 많이 해소할 수 있을 것이라 생각했음. 단적인 예로 학교를 다니다가 모 커피숍에서 나이가 있으신 분들은 다 줄을 서계시고 나이가 어린 사람들은 전부 사이렌 오더로 기다리고 있었음. 이것이 정보격차라고 생각하게 되었고, 이것을 어떻게 해소할 지 고민하면서 ‘디지털 푸어’들을 위하여 핸드폰에 음성명령으로 주문할 수 있게끔 하는 인터페이스가 있으면 누구나 다 잘 사용할 수 있겠구나 라는 생각이 들었음. 사이렌 오더도 그렇고 키오스크도 그렇고 정보 격차를 많이 일으키는 현상 중에 하나일텐데, NLP를 통해 해소할 수 있지 않을까라는 동기로 NLP에 관심이 있게 되었고, NLP를 하다보니까 Multi Modality를 해야겠구나라는 생각이 들면서 관심사가 확장되고 있다. Q: 바이블같은 책, 혹은 재밌게 읽은 책 등. A: CV 책은 잘 읽지 않는 편임.. A: NLP 책은 잘 모르겠는데, 요즘 NLP 모델들이 트랜스포머 구조로 대동단결하고 있기 때문에 Attention is all you need를 여러번 읽어보는 것이 좋은 것 같다. 반복해서 읽을수록 나의 이해가 부족했구나라는 생각이 많이 들게 된다. 그리고 또 한 번 읽고나면 까먹기 때문에.. Attention is all you need를 여러번 읽으면 도움이 많이 될 것이다. Q: 최근에 가장 기억에 남는 논문이나 기술 A: CV Multi Modality 논문에서 뉴런을 분석해보니까 글자를 읽는 영역을 따로 학습하지 않았는데 내부 뉴런에 반응하는 뉴런이 있더라! 하는 것. 그리고 Window Measurable 에서 사람이 말하는 ‘지능’에 대하여 고찰을 한 (프랑수아 쥴레) 논문을 인상깊게 보았음. Q: 이미 경험해본 분야를 해야할 지 막 관심이 생기는 다른 분야를 선택해야 할 지 고민이 된다. (CV 프로젝트 해봤는데 NLP 해야되는가?) A: NLP CV나 NLP 모두 잡포지션이 매우 많고 유망하기 때문에 그냥 더 재밌는 분야 선택하는게 좋을 것 같다. 사실 또 재밌어하는 분야고 최종적으로 만들고 싶은 어플리케이션의 형태에 있는 분야여야지 본인이 열심히 하게 된다. 흥미가 있으면 이미 경험해본 정도를 금방 추월할 수 있기 때문에 좋아하는 분야 선택하는 것을 추천한다. A: CV 본질적으로는 같은 답변인데, 본인이 생각했을 때 더 잘 할 수 있는 분야를 선택하는 것이 맞는 것 같다. 하지만 한 분야에서 깊이를 늘리는 것도 매우 좋은 접근이고, 그 분야에서 다양한 프로젝트를 해보는 것도 좋은 시도이지만, 본인의 경우 다양한 것을 해보고 싶어서 원래 공부하던 RL을 버리고 CV와 NLP를 공부했었다.하지만 뭘 선택하든 현업에 가면 더 배워야하기 때문에 본인이 잘할 수 있는 것을 선택하는 게 취업시장에서 어떻게든 유리하게 작용할 것이다. Q: 비전공자가 취업에 좀 더 쉽게 접근할 수 있는 분야라 하면 어느쪽을 골라야하는지, 또 업계 전반적인 구직현황은? A: CV 좀 더 쉽게 접근할 수 있는 것이라고 하면 본인이 조금이라도 더 흥미를 느끼는 분야일 것이다.. 본인이 조금 더 쉽게 이해할 수 있어보이는 쪽이 좋아보임. A: NLP 취업에 대해서는 Job Market이 굉장히 좋고 어떤 분야에서든 성과를 좀 내면 비전공자라도 둘 다 괜찮다. 본인의 경우 CV가 조금 더 어려웠던 이유는 삼각함수나 퓨리에 변환 등 이미지 처리 부분에서 심리적인 장벽이 있었기 때문에 조금 학습이 더뎌서 어렵게 느꼈었는데, 결국에는 CV든 NLP든 어려운 것은 다 해야된다. AI와 ML에 많이 포커스를 두는 사람들이 많은데, Engineering에도 많은 포커스를 두는 것이 좋다. 즉 좋은 ‘개발자’가 되는 것이 중요함. AI가 조금 부족하더라도 코딩능력과 커뮤니케이션 능력이 좋다면 데려가려는 기업이 많을 것이라고 생각한다. Q: CV와 NLP에 대하여 수강하게 될 때 각 분야에서 가장 중요하게 봐야할 부분은? A: NLP NLP에 대해서 설명을 드리면 커리큘럼을 봤을 때 상당히 좋은 커리큘럼이라는 생각이 들었다. 클루를 통하여 NLP의 태스크에 대하여 감을 잡고, 이러한 태스크를 기반으로 어떤 어플리케이션을 만들 수 있을까 고민하는 것이 중요하다. 가장 중요한 것은 언어모델에 대한 이해와 언어모델이 어떻게 학습이 되고 왜 잘되고 어떤 언어모델이 어떤 태스크에서 잘 되는지를 중점적으로 생각해보는 것이 좋음. A: CV CV에서는 사실 컴퓨터비전 내에서도 다양한 서브태스크가 많고 NLP나 음성에 대한 태스크도 있는데, 각 태스크마다 엔지니어링 요소들이 들어가는 것들이 있음. OCR을 했을 때 퍼포먼스를 위하여 Task Specific 보다는 네트워크 최적화 등의 테크닉이 필요할 수 있음. 그래서 이런 것들 위주로 생각해보는 게 좋고, 다른 태스크를 하게 되더라도 벽느끼지 않도록 공통적인 부분을 많이 챙기는 것이 좋음. Q: NLP의 경우 각 언어마다 모델링을 할 때 많은 차이가 나는지? A: NLP 차이가 날 수 있다. 한국어의 경우 형태소를 분석한 뒤 하는 것이 효율이 좋지만, 요즘 트렌드는 언어 의존도가 떨어지는 쪽이다. 그래서 요즘엔 동일한 토크나이징을 사용하는 등 의존도가 많이 사라졌다. Q: 기존 인력 대비 차별점을 가지기 위해 어떠한 스킬을 계발하는 것이 좋을지. A: CV 인력이 포화상태로 가고 있다는 생각이 잘 들지 않는다. 결국 해야할 것은 기본기가 잘 갖추는 것이다. 이 조건만 맞추기에도 면접은 매우 어려운 프로세스이고, 기본기가 충실한 사람이 잘 없다. 한가지 태스크를 바닥까지 파본 경험 혹은 매우 다양한 것들을 프로젝트로 접해봤다는 넓이도 차별점이 될 수 있다. 차별점도 매우 다양할 수 있다. Q: 제조업관련 비전 기술을 성장하면 IT 기업에서 fit이 맞지 않을 수 있다는 두려움이 있음. A: CV 특정 태스크를 가지고 프로젝트를 진행할 때 해당 태스크만을 위한 디테일들에 집중하게 되면 이러한 결과가 나올 수 있다. 다른 분야에서도 잘 쓰이는 테크닉인지 혹은 좀 다양한 분야에서도 유용하게 쓸 수 있는 기술들인지 생각하면서 개발하면 좋다. Q: CS전공지식, 연구트렌드파악, 논문구현 및 서빙능력 위주로 공부하려는데 좋은 방향일지. A: 이것으로 가는게 맞다. 다 맞는 말임. 좋은 방향을 설정한 것 같고, 각 태스크 별로 뭘 해야하는지 세부적인 계획을 짜는게 더 좋을 것이다. Q: 음성을 공부해보고 싶은데 접점이 맞는 NLP를 선택하고자 하는데, Engineer와 Researcher 중에 무엇이 어울릴지 판단하는데 도움이 될 방법은? A: NLP 엔지니어와 연구원 중 무엇이 맞을지는 해보기 전에는 알 수 없다. 회사 사람들 중에도 둘 다 해본 사람들이 많다. 개인적으로는 둘 다 해보고 되는 곳을 가는게 좋지 않나.. 본인도 그랬음. 리서쳐 같은 경우 데이터가 고정되어 있고 데이터에서 성능을 어떻게 끌어올릴지 고민하는 경향이 큰 것 같고 모델링 관점의 실험을 통해 퍼포먼스를 높이는데, 엔지니어는 데이터의 형태가 고정되어 있지 않고 리서치 페이퍼를 참고하면서 모델들을 바닥에서부터 개발하기 보다는 베이스라인을 삼고 개발하고 서빙하는 것까지의 과정으로 진행되는 것 같다. A: CV 둘 중에 선택하라 그러면 본인은 엔지니어를 선택했는데, 되게 심플하게 한 쪽은 논문을 읽고 쓰는 쪽, 다른 한쪽은 서비스를 만드는 쪽이라고 생각하고 판단했음. 그렇지만 완전히 분리되는 분야는 아닌 것 같다. 둘 중에 뭘 하든 둘 다 해야됨.","link":"/CV-NLP-%EC%84%A0%ED%83%9D%EA%B0%80%EC%9D%B4%EB%93%9C-%EC%97%85%EC%8A%A4%ED%85%8C%EC%9D%B4%EC%A7%80-%EC%84%9C%EB%8C%80%EC%9B%90-%EB%B0%95%EC%84%A0%EA%B7%9C/"},{"title":"Context-Dependent-Pre-Trained-Deep-Neural-Networks-for-Large-Vocabulary-Speech-Recognition","text":"Large-Vocab 음성인식을 위한 문맥의존적 사전학습 심층 신경망 - 음성인식에서 딥러닝이 고전 모델을 이긴 첫 사례 기존에 잘 작동하는 모델들(특히 CD-GMM-HMM)은 적은 수의 레이어만을 가지고 있었기 때문에 역전파 알고리즘과 오토인코더 학습만으로 좋은 결과를 얻을 수 없었기에 실제 사용에 있어서 ASR(Automatic Speech Recognition)의 성능은 인간 수준에 미치지 못했다. CD-DNN-HMM은 CD-GMM-HMM에서 HMM구조와 심층신경망을 결합한 형태로, 비지도학습 방식의 사전학습 HMM 모델의 정답을 DNN의 Training Label로 활용하는 준지도학습 방식의 딥러닝모델이다. 사전학습 때에는 심층 신뢰망(BDN)을 모방하여 오토인코더로 비선형적인 패턴을 학습하며 이후에는 BDN의 구조를 버리고 라벨링된 데이터를 가지고 Fine-Tuning을 진행하며 학습을 진행한다. 적은 데이터만으로도 적은 Training Error를 기록하며 CD-GNN-HMM보다 훨씬 나은 성능을 입증했으며 딥러닝 모델로 시퀀셜한 데이터를 정교하게 학습할 수 있음을 입증했다. 어떤 문제가 있는지, 그 문제를 풀기위한 기존의방법, 기존 방법의 한계와 이 논문의 방법은 어떻게 문제를 해결했는지 Abstract해당 논문에서는 대규모 단어집합의 음성인식(LVSR)을 위해 새로운 CD(Context-Dependent) 모델을 제안한다. 해당 모델은 DNN-HMM(Deep Neural Network - Hidden Markov Model) 구조로 되어있고, Senone(음성 인식의 term)의 분포를 출력하도록 훈련한다. 또한 CD-DNN-HMM을 LVSR에 적용하는 절차를 설명하고 고전적인 GMM-HMM(Gaussian Mixture Model) 구조의 성능을 크게 능가하는 것을 입증한다. Introduction최근 MMI(Maximum Mutual Information) Estimation, MCE(Minimum Classification Error) Training, MPE(Minimum Phone Error) Training, large-margin techniques, HMM(Hidden Markov Model), CRF(Conditional Random Fields, 새로운 음향모델) 등 주목할만한 발전이 있었지만, 그럼에도 불구하고 실제 사용에 있어서 ASR(Automatic Speech Recognition)의 성능은 인간 수준에 미치지 못했다. 최근 촘촘하게 직결되어 있고 많은 hidden layer를 가진 신뢰망(Belief Nets)에 주요한 발전이 있었다. 그 결과 심층신뢰망은 데이터에서 복잡한 통계적 패턴을 포착할 수 있는 비선형적인 특징 검출기의 계층을 학습할 수 있었다. 심층신뢰망의 학습 알고리즘은 우선 각각 레이어의 가중치를 순전히 비지도(unsupervised)적인 방법으로 개별적으로 초기화하고 라벨링된 데이터로 전체 네트워크를 미세조정한다. 이러한 준지도(semi-supervised)적인 접근은 음성, 오디오, 텍스트, 이미지 데이터의 분류 등을 포함한 수많은 어플리케이션에 효과적이라는 것이 입증되었다. 이러한 발전은 사전학습된 신경망과 ASR을 위한 다른 딥러닝 테크닉들을 기반으로 한 음향모델(acoustic model) 개발에 흥미를 불러일으켰다. 예를들어 문맥독립적인 사전학습된 심층신경망 HMM 하이브리드 구조는 최근에 전화인식 분야에 제안되었고 매우 경쟁력있는 성능을 달성했다. 심층신경망의 가중치를 초기활 때 사전학습을 사용하는 것은 문헌에서 논의된 두 가지의 잠재적인 이점이 있다. 사전훈련은 테스트케이스가 반복되지 않을 정도로 방대한 데이터셋이 있을 때에도 더 많은 데이터에 대해 일반화의 오류가 줄어들지 않는 효과를 가진 데이터 의존적인 독특한 종류의 정규화 방식으로 보자는 근거가 제시되었다. 입력들의 분포에 대한 정보를 사용함으로써 정규화를 통해 비교적 적은 라벨링 데이터로도 매우 표현력이 뛰어난 모델을 학습시킬 수 있다. 또한 다른이들도 추후의 최적화(전통적으로 SGD)를 목적으로 한 사전훈련과 일치한다는 증거를 보고했다. 결론적으로 사전훈련된 신경망은 사전훈련되지 않은 신경망보다 낮은 training error를 달성한다. 이러한 효과는 특히 깊은 오토인코더에서 두드러진다. 심층신뢰망은 최초의 사전훈련방식으로 연구된 방법이다. 깊은 오토인코더들이 사전훈련 심층신뢰망으로 효율적으로 학습될 수 있다는 것이 밝혀지고, 더 깊은 신경망에 대한 관심이 되살아났다. 많은 연구자들 역시 사전훈련이 많은 문제들과 모델 아키텍쳐, 심지어 거대한 데이터셋으로 훈련된 하나의 싱글레이어를 가진 신경망의 경우에도 도움이 될 것이라고 보고했다. 우리는 비지도학습 방식의 사전훈련 테크닉들을 많은 히든 레이어를 가진 신경망을 학습하는데 편리하고 강력한 방법이라고 보았다. 이러한 경유로 본 논문에서는 사전훈련된 심층신경망(DNN)과 문맥의존적(CD) 히든 마르코프 모델 사이의 하이브리드 구조인 새로운 음향모델을 제안한다. 본 모델에 사용된 사전학습 알고리즘은 심층신뢰망(DBN)이지만, dynamic Bayes net과 구별하고, 사전훈련이 끝난 후에 심층신뢰망을 버리고 오직 인식가중치를 유지하고 훈련하는 것과 구별하기위해 DNN-HMM이란 용어로 축약할 것이다. CD-DNN-HMM은 심층신경망의 힘과 CD-HMM의 시퀀셜한 모델링 능력을 결합한다. 즉, 모델의 주요 요소를 설명하고 파라미터를 학습하는 과정을 설명하며 CD-GMM-HMM의 성능을 뛰어넘는 것을 입증할 것이다. A. Previous Work Using Neural Network Acoustic ModelsB. Introduction to the DNN-HMM ApproachII. DEEP BELIEF NETWORK심층신경망은 여러 확률적인 히든유닛을 가지고 있는 확률적 생성모델이다. 이것은 비지도학습 알고리즘에서 효과적이며 빠르고, 정확하고 bottom-up 추론을 할 수 있다. 비지도학습의 사전학습 이후에는 up-down 알고리즘으로 DBN 가중치를 최적화하는 미세조정(fine-tuning)을 진행한다. 이 논문에서는 기존 feed-forward 신경망이 아니라 비지도학습 방식의 사전학습 알고리즘의 결과로 나타난 DBN 가중치를 사용하며 미세조정시 역전파 알고리즘을 사용한다. 사전학습","link":"/Context-Dependent-Pre-Trained-Deep-Neural-Networks-for-Large-Vocabulary-Speech-Recognition/"},{"title":"Convolution 소개","text":"파라미터 수 손으로 직접 계산해보기 알렉스넷의 파라미터 수 구하기 input = 224 224 3filter = 11 11 3fisrt param = $11113482 ~= 35k$ Modern CNNAlexNet네트워크가 두 개로 나뉘어져 있다. 당시 GPU가 부족했기 때문임. 두 장의 GPU로 학습하고 합쳤다. 인풋에 $11 * 11$ 필터를 사용했는데, 파라미터 관점에서 좋진 않다. 상대적으로 많은 파라미터가 필요하기 때문. 이후 5개의 Convolution 레이어와 3개의 Dense 레이어를 사용한다. 최근 200-300개 네트워크를 가진 신경망에 비하면 Light한 편이다. Key Ideas Rectified Linear Unit(ReLU) 활성화 함수를 사용했다. $ReLU = max(0, x)$ 리니어 모델이 갖는 좋은 성질들을 가질 수 있게 한다. 리니어 모델들의 성질을 가지고 있기 때문에 학습하는 SGD나 Gradient Descent로 학습을 용이하게 한다. 활성화함수를 사용할 때 이전에 많이 활용하던 Sigmoid나 tanh 같은 경우 값이 커지면 슬로프가 줄어들게 된다. 슬로프가 결국 기울기니까, 뉴런의 값이 많이 크면(0에서 벗어나면) Gradient Slope는 0에 가까워진다. 이 때 Vanishing Gradient 문제가 발생할 수 있는데, ReLU를 사용하면 해당 문제를 해소할 수 있음 GPU Implementation (2 GPUs) Local Response Normalization(어떠한 입력 공간에서 Response가 많이 나오는 부분을 죽이는 것임. 최근엔 많이 사용되지 않음), Overlapping pooling Data Augmentation Dropout 지금보면 LRN을 제외하고는 당연하게 모두가 사용하는 테크닉들이 주요 아이디어로 사용되었다. 하지만 당시 2015년에는 당연하지 않았다. 일반적으로 가장 잘 되는 기준을 당시에 잡아준 것이라고 볼 수 있음. VGGNet $3 * 3$ Convolution 필터를 사용하였다. 크기를 생각해봤을 때 필터의 크기가 커질수록 얻는 이점은 하나의 컨볼루션 필터를 찍었을 때 고려되는 인풋의 크기가 커진다는 것이다. (Receptive Field) $55$​ 필터 하나와 $33$​ 필터 두 개를 사용할 때 Receptive Field 차원에서는 똑같다. 하지만 파라미터의 개수는 400k vs. 294k 정도로 약 1.5배 차이가 난다. $3 * 3$ 레이어 두 개의 파라미터가 많을 것 같지만, 같은 리셉티브 필드를 사용한다는 전제로 봤을 때 필터의 크기가 작을수록 적은 파라미터를 가질 수 있다. $1*1$ FC레이어를 사용하였다. Dropout VGG16, VGG19 (레이어의 개수에 따라) GoogLeNet$1*1$ Convolution이 Dimension(차원) Reduction 효과가 있다. Convolution Feature Map이 witdh, height를 가질 때 special dimension이 아니라 Tensor의 Depth 방향으로 차원을 줄이는 것을 통해 파라미터 숫자를 줄일 수 있다. AlexNet -&gt; VGGNet 에서 배웠던 인사이트는 같은 리셉티브 필드에서는 필터 크기가 작은 것이 좋다는 것을 알아내었고, LeNet에서는 전체 파라미터를 어떻게 줄일 수 있을지를 알 수 있다. 네트워크 안에 네트워크가 있는 구조를 NIN(Network in network) 구조라고 한다. Inception Blocks $33$ Convolution을 하기 전에 $11 Conv$가 들어가는데 이것이 중요한 역할을 한다. Inception Block에서 여러 Response들을 Concat 하는 효과도 있지만 $1*1$ 이 들어감으로서 파라미터 수가 줄어들게 된다. $1*1$ Conv는 채널 방향으로 차원을 줄이게 된다. 파라미터의 수는 $3 3 128 128 = 147,456$ vs. $1 1 128 32 \\ X \\ 3 3 32 * 128 = 40,960$ 이다. 입력과 출력의 사이즈는 똑같음, 즉 Receptive Field 차원에서 같은데 파라미터의 수가 약 1/3로 줄었다. $1*1$ Conv가 파라미터를 30% 가량 줄일 수 있다! Which CNN Architecture has the least number of parameters? AlexNet (8-layers) = (60M) VGGNet (19-layers) = (110M) GoogLeNet (22-layers) = (4M) ResNet General Performance: Train Error가 줄어듦에도 불구하고 Test Error가 Train Error와 나는 차이를 말함. Overffiting: Train Error가 줄어드는데 Test Error가 큰 현상. 하지만 Train Error가 더 작은데도 불구하고 Test Error가 더 크면, 학습이 되지 않는 것임. 기존에는 네트워크가 커지는데 학습이 더 잘 되지 않는 상황이 존재했음. Key Ideas Add an identity map (skip connection) = Residual Connection 입력이 왔을 때 출력값이 나오는데, 이 때 입력 x 를 한 단짜리 Convolution에 더해주는 것임. 학습하고자 하는 것은 Residual 차이만 학습하는 것임. 차이만 학습하길 원하는 것이 Residual Connection을 사용한 ResNet임. $f(x) \\rightarrow x + f(x)$ 레지듀얼을 사용하지 않으면 네트워크가 많아도 학습이 잘 되지 않는다. 일반적으로 Simple Shortcut을 많이 사용함. Batch Norm 이 Convolution 뒤에 일어난다. 하지만 네트워크를 작성하다보면 Batch Norm 을 ReLU 뒤에 넣거나 안넣는 것이 결과가 좋을 때가 있다고 한다. Bottleneck architecture $3*3$​ Conv를 하기 전에 Input 채널을 줄이는 구조이다. ResNet으로 갈 수록 성능은 증가하고 파라미터 사이즈는 줄어들게 된다. 핵심: $11$으로 채널을 줄이고 줄인 채널에서 $33 혹은 5*5$로 Receptive Fields를 맞춤으로써 파라미터를 줄여나가는 것이 핵심 전략이다. DenseNetResNet은 결국 Convolution을 통해 나오는 값을 더해주는데, DenseNet의 아이디어는 더하지 말고 (왜냐면 성질이 섞이니까) Concat 하자는 것이다. DenseNet uses concatenation instead of addition Concat의 단점은 채널이 계속해서 커진다는 것인데, 이렇게 되면 Convolution Feature Map Channel도 같이 커지니까 파라미터도 커진다. 어떻게 채널을 줄이냐? $1*1$ Conv를 하면 됨. Transition Block $BatchNorm \\rightarrow 11 Conv \\rightarrow 22 AvgPooling$ Summary VGG: repeated $3*3$ blocks GoogLeNet: $1*1$ Convolution ResNet: skip-connection DenseNet: Concatenation","link":"/Convolution-%EC%86%8C%EA%B0%9C/"},{"title":"Data Centric 특강 - 최성철 교수님","text":"Data Centric더이상 성능을 미세하게 올리는 것에 집착하는 AI 엔지니어링은 요구되지 않는다. 과연 어떤 능력이 더 중요할 것인가? 수학을 임성빈 교수님만큼 해야 AI를 할 수 있는가? Nope 대신에 ML/AI Engineer들은 이제는, 코딩을 잘해야 한다. 네트워크에 대한 지식이나, CS에 대한 지식들이 더 많은 Area에서 기회를 줄 것임. 실전 AI에서는 매우 복잡한 단계를 거쳐서 프로젝트가 진행된다. Issues Data 실제 ML 프로젝트에서는 양질의 데이터 확보가 관건임. Production time 데이터(프로젝트가 끝난 후 Serving 단계에서의 데이터) 와 Experiment 데이터가 다를 때 발생하는 문제들 데이터 생성에 관한 문제 User generated data: inputs, clicks for recommendation System generated data: logs, metadata, prediction Data Flywheel (데이터 선순환): 사용자들 참여로 데이터 개선 구글 포토를 사용하면 가족들의 얼굴을 계속 같은 사람인지 물어보게 하여 사람들이 직접 태깅하게 하는 것. 데이터가 스스로 라벨링을 하게 하는 작업. Data augmentation: 데이터를 임의로 추가 확보 Data drift: 시간이 지나면서 데이터는 계속 바뀔 것임. 어떻게 production 레벨과 experiment 레벨의 데이터 간극을 맞출지가 중요한 문제이다. 2주나 4주 등 일정 간격을 정한 뒤 새로운 데이터로 새로운 모델을 뽑아냄. 이 때 새로운 모델을 뽑아낼 때는 hyper parameter tuning이 되어있을 것임. 이것을 가지고 Dynamic 하게 학습을 시키는 등 대책이 있으면 좋음. Data Feedback Loop 사용자로부터 오는 데이터를 자동화하여 모델에 피딩해주는 것이 중요함. Model Algorithms Metrics Hyper parameter tuning 앞으로 알아야 할 것들. MLOps 도구들 당연히 데이터베이스!! SQL Cloud - AWS, GCP, Azure Spark (+ Hadoop) Linux + Docker + 쿠버네티스 Scheduling 도구 (KubeFlow, MLFlow, AirFlow) 위의 것들을 써보는 것을 추천하지만, 전문가가 되라는 것은 아님. 최근엔 이러한 역량들을 전부 요구하기 때문이다. 하나의 시스템으로서의 ML/DL 개발 우리는 보통 주피터노트북을 키는 것부터 시작하는데, 모든 일이 주피터노트북에서 일어나지 않는다. 주피터노트북은 그저 실험을 위한 도구일 뿐. 개발에는 Code - Test - Monitor - Deploy 단계가 반드시 포함된다. 그리고 해당 단계에서의 도구들을 사용할 줄 아는 역량이 필요하다. 정리 앞으로는 알고리즘 연구자보다 ML/DL 엔지니어의 필요성이 더 증대 단순히 ML/DL 코드 작성을 넘어서야 함 자동화, 데이터연계, 실험결과에 기반한 설득, 시스템화 좋은 기획자적인 요소들이 필요.","link":"/Data-Centric-%ED%8A%B9%EA%B0%95-%EC%B5%9C%EC%84%B1%EC%B2%A0-%EA%B5%90%EC%88%98%EB%8B%98/"},{"title":"F.Liszt, 12 Transcendental Etude S.139 No. 10 in F minor &#39;Appassionata&#39;","text":"Franz Liszt - 12 Transcendental Etude S.139 no.10 in F minor “Appassionata” 15살의 리스트가 초월적인 기교와 화려함을 자랑하고 싶어서, 본인을 위해 작곡한 12개의 초절기교 연습곡 중 10번 작품으로 부제는 ‘열정’이다. 12개의 초절기교 연습곡들에는 은유적인 부제가 붙어 있는데, 이 작품은 2번과 함께 부제가 없는 곡이었다가 후세에 사람들에 의해 붙여졌다. ‘열정’이란 말 그대로 Allegro Agitato Molto의 빠른 속도를 자랑하는 이 곡의 테마는, 곡이 시작할 때 나타나는 왼손과 오른손의 교차로 표현되는데, 이 테마의 변주로 고조되는 분위기를 인식하면 더욱 재밌는 감상이 될 수 있다. 2016.03 Piano in Yonsei 12회 정기연주회","link":"/F-Liszt-12-Transcendental-Etude-S-139-No-10-in-F-minor-Appassionata/"},{"title":"F.Liszt, Legend No.2, S.175","text":"Franz Liszt - Legend no.2, S.175부제 - 물 위를 걷는 파올라의 성 프란체스코 리스트의 두 개의 전설 중 두 번째 작품으로, 첫 번째 전설 ‘새들에게 설교하는 성 프란체스코’에 비해 훨씬 화려하고 드라마적이다. 이 작품의 배경은, 메시나 해협을 건너는 뱃사공이 성인 프란체스코에게 “당신이 정말로 성인이라면 물 위로 걸어가시오”라고 말한 것에서 시작한다. 그러자 성 프란체스코는 자신의 겉옷을 물위에 펼친 뒤 그 일부를 돛으로 올려 세우고, 지팡이로 이를 고정시켜 반대편 물가로 안전하게 걸어서 건너갔다는 이야기이다. 잔잔한 물결에서 시작하여 거센 파도를 지닌 바다를 연상시키는 화음과 진행은, 물 위를 건너는 기적을 드라마틱하고 더욱 강렬하게 증폭시킨 뒤, 화려하고 성스러운 고양감으로 결말을 맺는다. 2016.09. Piano in Yonsei 13회 정기연주회","link":"/F-Liszt-Legend-No-2-S-175/"},{"title":"Fazil Say - Paganini Variation 도입부","text":"Fazil Say는 여러 유명 작품들을 많이 편곡했는데 그 중에서 터키행진곡과 이 곡이다. 현대음악적인 테크닉들이 매우 많이 등장하며, 현대음악답게.. 뒷부분은 내 취향이 아니다. 이해하기 넘 어려워 리스트의 파가니니 연습곡 6번(주제와변주)을 재즈느낌으로 편곡했는데 참 아름답게 잘 살린 것 같다. 원곡은 입시곡이었어서 맨 첫 시작 긁는 것만 들어도 PTSD 오는데 이건 좀 재밌다. 이게 편곡의 순기능인가 ㅎㅎ; 내가친거!","link":"/Fazil-Say-Paganini-Variation-%EB%8F%84%EC%9E%85%EB%B6%80/"},{"title":"Generative Models 소개","text":"What I cannot create, I do not understand. - Richard Feynman 생성 모델을 학습한다는 것에 대해서 가장 처음에 생각하는 것은 그럴듯한 문장 혹은 이미지를 만드는 것이라고 생각한다. 하지만 이것이 전부가 아니라, 생성모델은 그것보다 많은 것을 포함하는 개념이다. Generation: 강아지와 같은 이미지를 만드는 것도 생성모델이 하는 일이지만, Density estimation 어떤 이미지가 들어왔을 때 확률값 하나가 튀어나와서 이미지가 고양이 같은지 강아지 같은지 강아지가 아닌 것 같은지 구분하고 싶은 것임. 이상행동감지로 활용될 수 있다. 엄밀히 생성모델은 Discrimity 모델을 포함하고 있다. 즉 생성해내는 것만 아니라 구분하는 것까지 포함하고 있다. 이러한 모델을 보통 explicit model 이라고 한다. 확률값을 얻을 수 있는 모델을 뜻함. Unsupervised representation learning 이라고 강아지가 있으면 강아지는 귀가 두개고 꼬리가 있고 등의 특성이 있을텐데 이것을 feature learning 이라고 한다. 이 feature learning이 생성모델이 할 수 있는 것으로 표현하기도 함. Basic Discrete Distributions Bernoulli distribution 이것을 포함하는 확률분포에는 숫자가 하나 필요함. 동전을 던졌을 때 앞 뒤가 나오는 것. 카테고리면 n개가 필요한 것. 주사위를 던졌을 때 n개가 필요한 것이 아니라 n - 1개가 필요함. Categorical distribution RGB 이미지 하나를 픽셀로 표현할 때 (r, g, b) ~ p(R, G, B) 는 256 256 256 가지가 있다. 그렇다면 이것을 표현하는 파라미터는 256 256 (256 - 1) 개가 필요함.","link":"/Generative-Models-%EC%86%8C%EA%B0%9C/"},{"title":"IP 주소 및 서브넷마스크","text":"IP Address - 네트워크 계층에서 사용되는 주소. IP 헤더에 포함된 데이터 주소. IPv4: 32bit(2^32), 8bit씩 4개의 octet IPv6: 128bit(2^128), 16bit씩 8개의 field Subnet Mask - IP주소에 대한 네트워크 아이디와 호스트 아이디를 구분하기 위해 사용. IP AddressSubnet MaskNetwork IDHost IDIP 주소 개수13.13.10.1255.0.0.013..13.10.12^2413.13.10.1255.255.0.013.13..10.12^1613.13.10.1255.255.255.013.13.10..12^813.13.10.1255.255.255.25513.13.10.1-2^00.0.0.00.0.0.0-0.0.0.02^32 Unitcast Address A Class: 첫번째 필드의 2진수 값 중 맨 앞이 0이라는 공통비트라면 A Class로 정의. (00000000 ~ 01111111) B Class: 첫번째 필드의 2진수 값 중 맨 앞이 10이라는 공통비트라면 B Class로 정의. C Class: 첫번째 필드의 2진수 값 중 맨 앞이 110이라는 공통비트라면 C Class로 정의. D Class: 첫번째 필드의 2진수 값 중 맨 앞이 1110이라는 공통비트라면 D Class로 정의. E Class: 첫번째 필드의 2진수 값 중 맨 앞이 1111이라는 공통비트라면 E Class로 정의. Network Name: 호스트 아이디 전체가 0인 주소 ex) 192.168.1.1/24 -&gt; 192.168.1.0 Subnet Broadcast Address: 호스트 아이디 전체가 1인 주소 ex) 192.168.1.1/24 -&gt; 192.168.1.255 설정가능한 IP Address: 네트워크 아이디와 서브넷 브로드캐스트 주소를 제외한 IP 주소 ex) 192.168.1.1/24 -&gt; (192.168.1.1 ~ 192.168.1.254) 총 (2^8 - 2)=254개 Public IP Address: ISP업체로부터 임대로 받아 사용하는 주소 Private IP Address A Class: 10.0.0.0/8 (10.0.0.1 ~ 10.255.255.255) B Class: 172.16.0.0/12 (172.16.0.1 ~ 172.31.255.255) C Class 192.168.0.0/16 (192.168.0.0 ~ 192.168.255.255)","link":"/IP-%EC%A3%BC%EC%86%8C-%EB%B0%8F-%EC%84%9C%EB%B8%8C%EB%84%B7%EB%A7%88%EC%8A%A4%ED%81%AC/"},{"title":"Optimization 소개","text":"“Language is the source of misunderstandings” - Antoine de Saint-Exupery (1900-1944) 용어에 대한 정리를 하고 넘어가야 이후에 오해가 생기지 않는다. 용어 통일이 가장 중요하기 때문에 용어에 대한 명확한 컨셉을 잡아볼 것임. IntroductionGradient Descent First-order iterative optimization algorithm for finding a local minimum of a differentiable function. 찾고자 하는 파라미터에 대해서 손실함수를 미분한 편미분값을 가지고 학습을 하겠다는 것임. Local minimum: 손실함수를 미분했을 때 극소적으로 좋은 로컬 미니멈을 찾는 것을 목적으로 함. Important Concepts in Optimization Generalization (일반화) 대부분의 경우 일반화 성능을 높이는 것이 목적이다. 일반화 성능을 높이기만 하면 좋은 것인가? 일반적으로 학습을 시키게 되면 Iteration이 지나면서 Training Error를 줄이게 되는데, Training Error가 0이 되었다고 해서 우리가 항상 원하는 최적값에 도달했다는 보장이 없다. 일반적으로 Training error가 주어지지만, 어느정도 시간이 지나고 나면 Test Error가 커짐, 즉 학습에 사용하지 않은 데이터에 대해서는 성능이 오히려 떨어지게 된다. 즉, 일반적으로 일반화는 테스트에러와 트레이닝 에러의 갭을 줄이는 것을 의미한다. 만약 우리 네트워크가 안좋아서 학습데이터에 대한 성능이 안좋으면, 일반화의 퍼포먼스가 좋다고 해서 테스트 데이터의 성능이 좋다고 말할 수 없다. 일반화의 성능은 테스트와 트레이닝 에러의 갭을 말하기 때문이다. Under-fitting vs. over-fitting 학습데이터에 대해서는 잘 동작을 하지만 테스트데이터에 대해서 잘 동작하지 않는 것을 OverFitting이라고 한다. Cross validation 일반적으로 데이터가 분리되어 있음 (Train, Test, Validate) 보통은 학습 데이터로 학습시킨 모델이 학습에 사용되지 않은 밸리데이션 데이터를 기준으로 학습이 잘 되었는지 확인하는 과정이다. 하지만, 트레인과 테스트를 반반으로 했을 때, 트레인을 80 테스트를 20으로 했을 때 등 Split 범위에 따라서 학습의 퍼포먼스가 달라질 수 있다. 이러한 것을 해결하고자 하는 것이 Cross Validation이다. (혹은 K-Fold Validation) 네트워크를 학습할 때 수많은 파라미터가 존재한다. 또한 Hyper Parameter들에 대한 기준이 없다. 따라서 Cross-validation을 통해 최적의 Parameter Sets 을 찾고, 파라미터를 보정한 상태에서 모든 학습데이터를 학습에 사용해야 더 많은 데이터를 사용할 수 있다. Test Data는 어떠한 방법으로든 학습에 사용되어서는 안된다. 즉, 학습에는 Train과 Validate 데이터만을 이용해야 한다. Bias-variance tradeoff 사격을 할 때 항상 같은 곳에만 찍히면 (원점이 아니더라도) 좋은 것이다. 전체 모델을 어느정도 평준화 시키면 최적값에 도달할 가능성이 많기 때문이다. 이것을 Low Variance라고 한다. (출력이 얼마나 일관적으로 나오는 지) 이것에 반해 Variance가 크면 비슷한 입력이 들어와도 출력이 많이 달라지는 것을 의미한다. Bias라는 것은 출력에 따른 평균적으로 True Target에 접근하는 정도를 의미하며, Bias가 크면 Mean에서 많이 벗어나는 것을 의미한다. 학습 데이터에 Noise가 껴있다고 가정했을 때 내가 이 노이즈가 껴있는 타겟 데이터를 미니마이즈하는 것에 대하여 세 파트로 나뉘어진다. 내가 미니마이즈하는 것은 하나의 값(cost)이지만, 그 값은 세 파트로 나뉘어져 있어서 한 파트를 줄이면 다른 파트가 커질 수밖에 없는 Trade-off 관계이다. \\begin{aligned} \\mathbb{E}\\left[(t-\\hat{f})^{2}\\right] &=\\mathbb{E}\\left[(t-f+f-\\hat{f})^{2}\\right] \\\\ &=\\ldots \\\\ &=\\mathbb{E}\\left[\\left(f-\\mathbb{E}[\\hat{f}]^{2}\\right)^{2}\\right]+\\mathbb{E}\\left[(\\mathbb{E}[\\hat{f}]-\\hat{f})^{2}\\right]+\\mathbb{E}[\\epsilon] \\end{aligned}위 식에서 cost를 minimize한다는 것은 bias와 variance, noise를 minimize 한다는 것을 의미한다. Bootstrapping 신발끈을 말하는 것이며, 그 뜻은 신발끈을 들어올려 하늘을 날겠다라는 허황된 뜻임. 딥러닝에서는 학습 데이터가 100개가 있다고 하면, 100개 중 80개를 사용하겠다 등 일부만 뽑아서 사용한다는 것을 의미한다. 이렇게 했을 때 하나의 입력에 대해서 각 모델들의 출력이 달라질 수 있다. 이 때 각 모델들의 예측값들이 얼마나 일치를 이루는지 보고 전체 모델들의 특성을 알고자 하는 것이다. 학습데이터가 고정되어 있을 때 subsampling을 통해 여러 학습데이터를 만들고 그것을 이용하여 여러 모델 혹은 metric을 만들겠다는 것임. Baggin and boosting Bagging(Bootstrapping aggregating) 학습데이터를 여러 개를 만들어서 (Booststrapping) 여러 모델을 만들고 그 모델의 output들을 가지고 평균을 내겠다는 뜻임. 학습데이터가 고정되어 있으면 이것을 모두 이용하여 하나의 결과를 내는 것이 좋을 것 같지만 사실은 그렇지 않다. N개의 모델을 만들고, N개의 모델의 출력값의 평균이나 Voting을 통해 나오는 결과를 사용하는 것이 보통 성능이 더 좋다. Boosting: 학습데이터를 시퀀셜하게 바라봐서 간단한 모델을 만들고, 모델에 학습데이터를 다 넣어본다. 모델에서 80개는 잘 예측을 했지만 20개는 잘 안되었다. 이 때 모델을 하나를 더 만드는데 전에 안되었던 20개에 데이터를 잘 예측하는 모델을 만든다. 이렇게 모델들을 만들고 합친 이후에 하나의 struct learner를 만들고, 그 weight를 찾는 방식을 말한다. Practical Gradient Descent MethodsStochastic gradient Descent 하나의 샘플을 이용 Mini-batch Gradient Descent Subset of data 를 사용 Batch Gradient Descent whole data 를 사용 Batch-size Matters 배치사이즈는 굉장히 중요하다. 엄청나게 큰 배치를 사용하게 되면 Sharp Minimizers에 도달한다. 하지만 작은 배치를 사용하면 Flat Minimizers에 도달한다. 배치사이즈를 작게 사용하는 것이 일반적으로 좋다. sharp보단 flat이 더 좋다는 것임. 우리의 목적은 Train function에서 잘 동작하는 것이 아니라 Testing function에 잘 동작하는 것이다. 이 때 Flat Minimum은 Testing 값이 잘 나오지만, Sharp Minimum의 경우 한 번도 트레인에 사용되지 않은 데이터에서는 잘 동작하지 않는다. Gradient Descent Methods구현할 필요는 전혀 없음! 딥러닝 프레임워크에서는 한 줄 컷이다. Stochastic Gradient Descent 가장 큰 단점은 Lr, stepsize를 잡는 것이 가장 어렵다는 것이다. 너무 크면 학습이 안되고, 너무 작으면 학습을 오래 시켜도 학습이 안된다. $W_{t + 1} \\leftarrow W_t - \\eta g_t$ $\\eta$ : Learning rate or step size $g_t$: Gradient Momentum (관성) SGD의 단점을 극복하기 위해 제시된 Optimization 테크닉이다. 한 번 Gradient가 한 방향으로 흐른 이후 다음 Gradient 가 조금 다른 방향으로 흘렀어도, 이전 방향으로 흘렀던 정보를 활용해보자. a_{t+1} \\leftarrow \\beta a_t + g_t \\\\ W_{t+1} \\leftarrow W_t - \\eta a_{t+1} $a_{t+1}$ : Accumulation $\\beta$ : Momentum 모멘텀과 현재 기울기를 합친 Accumulation으로 Update를 한다. 장점은 한 번 흘러간 기울기를 포함하여 업데이트를 하기 때문에 어느정도 학습이 잘 이루어지게 된다. Nesterov Accelerated Gradient 이것도 역시 a라고 하는 Accumulate 가 업데이트를 하는 것인데, 기울기를 계산할 때 Lookahead gradient를 계산하게 된다. 모멘텀은 이전 기울기 정보로 계산하는데, 이 친구는 a라고 하는 현재 정보가 있다면 그 방향으로 한 번 가보고 기울기를 계산한 다음에 그 기울기를 가지고 Accumulation을 계산한다. \\begin{aligned} a_{t+1} & \\leftarrow \\beta a_{t}+ \\nabla \\mathcal{L} (W_{t}-\\eta \\beta a_{t})\\\\ W_{t+1} & \\leftarrow W_{t}-\\eta a_{t+1} \\end{aligned} 일반적으로 모멘텀을 관성으로 해석한다면, Local Minimum으로 Conversion 하지 못하는 현상이 생길 수 있는데 이에 반해 NAG는 Local Minimum을 지나서 기울기를 계산하는 것이 아니라 한 번 지나간 그 점에서 기울기를 계산하는 것이기 때문에 Local Minimum이 한 쪽 아래로 흘러갈 수 있다는 장점이 있다. 즉, 극값에 조금 더 빠르게 접근할 수 있다는 장점이 있다. Adagrad 신경망의 파라미터가 변했는지 안변했는지를 보게 되고 많이 변한 파라미터들은 적게 변화시키고 조금 변한 파라미터는 많이 변화시키기 위해 고안된 Adaptive Learning 이다. 즉 파라미터들이 얼마나 변했는지를 기록한 정보가 필요하다. $W_{t+1}=W_{t}-\\frac{\\eta}{\\sqrt{G_{t}}+\\epsilon} g_{t}$​​​ $G_t$: sum of gradient squares 가장 큰 문제는 $G_t$ 라는 것이 계속해서 커지기 때문에 결국에는 무한대로 가게 되면 분모가 무한대니까 W의 업데이트가 안되게 된다. 즉 뒤로 갈 수록 학습이 점점 되지 않는 문제가 생긴다. 이것을 해결하기 위해 고안된 방법이 Adam 등이다. Adadeltha \\begin{aligned} G_{t} &=\\gamma G_{t-1}+(1-\\gamma) g_{t}^{2} \\\\ W_{t+1} &=W_{t}-\\frac{\\sqrt{H_{t-1}+\\epsilon}}{\\sqrt{G_{t}+\\epsilon}} g_{t} \\\\ H_{t} &=\\gamma H_{t-1}+(1-\\gamma)\\left(\\Delta W_{t}\\right)^{2} \\end{aligned} Adagrad가 가지는 $G_t$가 커지는 것을 막기 위해 고안된 방법 중 하나이다. 타임스텝 t가 주어졌을 때 $G_t$ 를 윈도우 사이즈 만큼의 파라미터, 시간에 따른 기울기 변화를 보겠다는 것임. 이것 역시 문제가 있음. 윈도우 사이즈를 100으로 잡았을 때 100회 정보에 대한 값을 가지고 있어야 한다. 즉 GPU가 터질 수도 있다. 이것을 $H_t$ 를통해 해결할 수 있다. Adadelta의 가장 큰 특징은 Learning rate가 없다는 것인데, 그 뜻은 우리가 무언가를 시도해볼 것이 적다는 뜻이다. 따라서 잘 쓰이지 않는다. RMSprop 논문을 통해서 제안된 방법이 아니다. 아이디어는 간단함. \\begin{aligned} G_{t} &=\\gamma G_{t-1}+(1-\\gamma) g_{t}^{2} \\\\ W_{t+1} &=W_{t}-\\frac{\\eta}{\\sqrt{G_{t}+\\epsilon}} g_{t} \\end{aligned} Adagrad 처럼 Gradient Squares를 그냥 더하는 것이 아니라 분모에 넣고, $\\eta$ 라는 stepsize가 들어간다. Adam 일반적으로 가장 무난하게 사용하는 것이 바로 Adam이다. \\begin{aligned} m_{t} &=\\beta_{1} m_{t=1}+\\left(1-\\beta_{1}\\right) g_{t} \\\\ v_{t} &=\\beta_{2} v_{t-1}+\\left(1-\\beta_{2}\\right) g_{t}^{2} \\\\ W_{t+1} &=W_{t}-\\frac{\\eta}{\\sqrt{v_{t}+\\epsilon}} \\frac{\\sqrt{1-\\beta_{2}^{t}}}{1-\\beta_{1}^{t}} m_{t} \\end{aligned} $m_t$ : momentum $v_t$ : EMA of gradient squares Gradient 크기에 따라서 Adaptive하게 Learning rate를 바꾸는 것과 이전 Gradient 정보에 해당하는 모멘텀을 잘 합친 방법이다. Regularization학습에 잘 되는 것을 방해하기 위해 규제하는 것임. 학습을 방해함으로써 얻기 위한 목적을 잘 생각해야 한다. 즉, 학습데이터가 잘 되기 위해 사용하는 것이 아니라 방해함으로써 테스트데이터에도 잘 동작하도록 만드는 것이다. Early stopping 학습을 일찍 멈추는 것. 하지만 학습을 멈출 때 테스트 데이터를 사용하면 Cheating이기 때문에 Validation Data를 사용한다. Loss 가 어느 순간부터 커질 수 있는 데 그 시점에 빠르게 학습을 멈추자는 것임. Parameter norm penalty 신경망 파라미터가 너무 커지지 않게 하는 것임. 각 파라미터들을 제곱한 다음에 더하면 어떤 숫자가 나올텐데, 그 숫자를 같이 줄이는 것이다. 이왕이면 네트워크 웨이트의 숫자가 작을 수록 좋다. ‘크기’에 관점에서. 물리적이거나 해석적인 관점에서의 의미는 뉴럴넷이 만드는 함수의 공간속에서 함수를 최대한 부드러운 함수로 보자는 것이다. 부드러운 함수일수록 일반화 성능이 높다는 것일 것이다라는 가정이 전제됨. Data augmentation 딥러닝에서 가장 중요한 것 중 하나가 데이터이다. 데이터가 적으면 딥러닝보다 전통적인 ML(XG Boost, Random Forest 등)이 훨씬 성능이 높다. 하지만 데이터가 커질수록 머신러닝 방법론들보다 딥러닝 방법론이 훨씬 성능이 좋다. 하지만 데이터는 한정적이기 때문에 Augmentation을 통해 어떠한 방법으로든 데이터를 늘리는 것이다. Noise robustness 왜 잘되는지는 아직 의문이 있다. Data Augmentation이랑 비슷할 수도 있지만, 데이터에 노이즈를 집어넣는 방식이다. 굳이 차이를 두자면 입력에만 노이즈를 두는 것이 아니라 weight에도 noise를 주는 것이다. Label smoothing 데이터 두 개를 뽑아서 Train 단계의 데이터 두 개를 섞어주는 것이다. 일반적으로 분류 문제에서 Descision Boundary를 찾는 것이 목적인데, Descision Boundary 를 조금 더 부드럽게 만들어주는 효과가 있다. 라벨을 섞고 이미지(학습데이터)도 섞는 방법임. 왜 잘되는지가 설명되기 보다는 그냥 하면 잘 된다. Dropout 신경망의 weight를 0으로 바꾸는 것이다. 각각의 뉴런들이 더욱 Robust한 Feature를 얻을 수 있다고 해석을 한다. Batch normalization 논란이 많은 논문이다. 적용하려는 레이어에 Statistics를 정규화 시키는 것이다. 천개의 파라미터가 있는 히든레이어라면, 천개의 파라미터의 각각의 값들에 대한 statistics에 대해 평균을 빼주고 표준편차를 나눠주는 방식이다. 활용하면 성능이 많이 올라간다.","link":"/Optimization-%EC%86%8C%EA%B0%9C/"},{"title":"PStage MRC 1강 - MRC Intro","text":"Introduction to MRCMachine Reading Comprehension (기계독해)는 주어진 지문(Context)를 이해하고, 주어진 질의(Query/Question)의 답변을 추론하는 문제이다. 예를들어 서울특별시에 대한 지문에서 서울의 GDP는 세계 몇 위야? 라는 질문을 했을 때, 세계 4위 라는 답변을 하는 과제이다. 주로 질문(쿼리)가 들어오면 문서에서 답을 낼 수 있는 시스템(구글, 인공지능 스피커)에서 사용되며, 관련 문서를 찾고 -&gt; 정답을 찾는 방식으로 구성된다. MRC의 종류들은 아래와 같다. 1) Extractive Answer Datasets 질의에 대한 답이 항상 주어진 지문의 segment(or span)으로 존재하는 경우. 이런 방식을 채택하는 데이터셋으로는 SQuAD, KorQuAD, NewsQA 등이 있다. 2) Descriptive/Narrative Answer Datasets 답이 지문 내에 존재하지 않고, 질의를 보고 생성된 sentence(or free-form)의 형태. MS MARCO, Narrative QA 등의 데이터셋이 이러한 방식을 채택한다. 3) Multiple-choice Datasets 최근에는 잘 사용되지 않지만, 질의에 대한 답을 여러 개의 answer candidates 중 하나로 고르는 형태임. MRC Datasets들이 언제부터 생성되었는지 확정적으로 정해지진 않았지만, MCTest(2013)을 시초로 보고, 해당 형태는 수능 객관식문제와 비슷한 형태를 띄고 있다. 이후에 점점 지문 내에 답이 존재하지 않더라도 검색을 통해 관련된 답변을 찾아올 수 있는 형태로 데이터셋이 구축되고 있다. MRC의 ChallengesMRC를 하면서 어려운 과제들을 직면할 수 있다. 첫 번째로, 단어들의 구성이 유사하지 않지만 동일한 의미의 문장을 이해해야 하는 경우이다. Question에 있는 단어들이 Phrase에 대체로 비슷하게 존재하면 굉장히 쉽게 답을 찾을 수 있지만, 비슷한 단어가 없는 등의 경우엔 굉장히 답을 고르기 어려울 수 있다. 또한 지시대명사(it, the, him, her 등)를 찾는 것이 굉장히 어려운 과제이다. 두 번째는 주어진 지문에서 질문에 대한 답을 찾을 수 없는 경우가 있으며, 마지막으로 Multi-hop reasoning(여러 개의 document에서 질의에 대한 supporting fact를 찾아서 답을 찾아야 하는 경우)이다. MRC의 평가방법MRC에서는 Exact Match / F1 score, ROUGE-L / BLEU 로 평가를 진행할 수 있다. EM: 예측한 답과 ground-truth이 정확히 일치하는 샘플의 비율 F1 score: 예측한 답과 ground-truth 사이의 token overlap을 F1으로 계산 ROUGE-L Score: 예측한 값과 ground-truth 사이의 overlap recall BLEU (Bilingual Evaluation Understudy): 예측한 답과 ground-truth 사이의 precision Unicode &amp; Tokenization유니코드Unicode는 전 세계의 모든 문자를 일관되게 표현하고 다룰 수 있도록 만들어진 문자셋으로, 각 문자마다 숫자 하나에 매핑한다. 유니코드와 조금 다른 방식으로 인코딩 &amp; UTF-8이 있는데, 인코딩이란 문자를 컴퓨터에서 저장 및 처리할 수 있게 이진수로 바꾸는 것이며, UTF-8은 현재 가장 많이 쓰는 인코딩 방식으로 문자 타입에 따라 다른 길이의 바이트를 할당하는 방식이다. 123456789101112131415161718# ord: 문자를 유니코드 code point로 변환ord('A')&gt; 65ord('가')&gt; 44032hex(ord('A'))&gt; '0x41'hex(ord('가'))&gt; '0xac00'# chr: Code point를 문자로 변환chr(44032)&gt; '가'chr(65)&gt; 'A'chr(0xAC00)&gt; '가'chr(0x41)&gt; 'A' 한국어는 유니코드에서 한자 다음으로 많은 코드를 차지하고 있다. 한글은 초성, 중성, 종성을 분리할 수 있기 때문에 각 성 하나하나가 유니코드에 매핑되어 있으나, 초성/중성/종성이 전부 결합되어있는 완성형 글자 하나하나도 모두 유니코드에 매핑되어 있기 때문에 가짓수가 굉장히 많아지기 때문이다. 토크나이징가장 쉽게 토크나이징을 할 수 있는 방법은 띄어쓰기 기준으로 split하는 것이나, 효과가 좋지 않아서 주로 subword 토크나이징 방식을 사용한다. 자주 쓰이는 글자 조합은 한 단위로 취급하고, 자주 쓰이지 않는 조합은 subword로 쪼갠다. BERT 토크나이저에서 “##”은 디코딩할 때 해당 토큰을 앞 토큰에 띄어쓰기 없이 붙인다는 것을 뜻한다. 12tokenizer.tokenize('아버지 가방에 들어가신다')&gt; ['아버지', '가', '##방', '##에', '들어', '##가', '##신', '##다'] 그 다음으로 볼 것은 BPE(Byte-Pair Encoding) 토크나이저다. 데이터 압축용으로 제안된 알고리즘이며, NLP에서 활발하게 사용되고 있다. BPE 알고리즘은 아래의 방식을 따른다. 가장 자주 나오는 글자 단위 Bigram(or Byte pair)를 다른 글자로 치환한다. 치환된 글자를 저장해둔다. 1~2번을 반복한다. Looking into the DatasetKorQuAD 살펴보기LG CNS에서 언어지능 연구를 위해 공개한 질의응답/기계독해 데이터셋으로 한국어 위키피디아 1,550개의 문서에 대한 10,649건의 하위 문서와 크라우드 소싱으로 제작한 63,592개의 질의응답 쌍으로 구성되어 있음. (Train 60,407 / Dev 5,774 / Test 3,898) 현재 KorQuAD v1.0과 v2.0이 공개되어 있다. 2.0은 보다 긴 분량의 문서가 포함되어 있고, 단순 자연어 문장 뿐 아니라 복잡한 표와 리스트 등을 포함하는 HTML 형태로 표현되어 있어서 문서 전체 구조에 대한 이해가 필요하다. KorQuAD는 SQuAD v1.0의 데이터 수집 방식을 벤치마크하여 표준성을 확보하였다. 대상 문서를 수집한 뒤 전처리를 거친 후 질문/답변 약 7만쌍을 생성하였고, 양질의 질의응답 쌍을 작업자들이 생성할 수 있도록 상세한 가이드라인을 제시하였으며 2차 태깅과정에서 사람이 직접 답해보면서 Human Performance를 측정하였다. KorQuAD는 huggingface datasets 라이브러리로 편하게 불러올 수 있으며, huggingface datasets 라이브러리를 사용하면 memory-mapped, cached 되어 있는 데이터셋을 불러올 수 있어서 메모리 공간 부족이나 전처리 과정 반복의 번거로움을 피할 수 있다. 12from datasets import load_datasetdataset = load_dataset('squad_kor_v1', split='train') # squad_kor_v2 KorQuAD 의 답변 유형은 대상, 인물, 시간, 장소` 등 쉽게 특정할 수 있는 것이 대부분을 차지하고 있으며, 영문 표준 데이터와 특성이 유사하다.","link":"/PStage-MRC-1%EA%B0%95-MRC-Intro/"},{"title":"PStage MRC 2강 - Extraction-based MRC","text":"Extraction-based MRCExtraction-based MRC의 문제 정의질문의 답변이 항상 주어진 지문 내에 span으로 존재한다고 가정한다. 평가방법은 크게 EM과 f1이 존재한다. f1은 조금 더 soft한 metric으로 보통 EM보다 점수가 높게 형성된다. EM score는 정답과 조금이라도 다르면 0점을 부여하고, F1은 정답과의 overlap 비율을 계산하여 부분점수를 부여한다. 모델의 input은 Context, Question 두 개의 벡터가 임베딩의 형태로 들어가게 되며 start index와 end index이 존재하는 벡터가 output이 된다. Pre-processingTokenization텍스트를 작은 단위(token)로 나누어야 한다. 띄어쓰기, 형태소, subword 등 여러 단위의 토큰 기준이 사용되는데, 최근엔 OOV(out of vocab) 문제를 해결해주고 정보학적 이점을 가진 BPE가 주로 사용되며, WordPiece Tokenizer 역시 BPE 방법론 중 하나이다. input으로 들어갈 때 SEP 라는 special 토큰으로 context와 질문을 구분하게 된다. Attention Mask입력 시퀀스 중에서 attention 연산 시 무시할 토큰을 표시한다. 보통 0은 무시, 1은 연산에 포함하며, PAD와 같은 의미를 가지지 않은 special token을 무시하기 위해 사용한다. Token Type IDS입력이 두 개 이상의 시퀀스(ex; 질문 &amp; 지문)로 구성되어 있을 때 각각에게 ID를 부여하여 모델이 구분하여 해석하도록 유도하게 된다. MRC에선 질문이 항상 첫 번째 문장으로 구성되기 때문에 질문을 0으로, 지문을 1로 구성하고 padding 값들에 0을 부여하여 지문 내에서만 정답을 찾을 수 있도록 한다. 모델 출력값정답은 문서 내에 존재하는 연속된 단어 토큰(span)이므로, 시작 인덱스와 종료 인덱스를 알면 정답을 맞출 수 있다. Extraction-based에선 답안을 생성하기 보다, 시작위치와 끝위치를 예측하도록 학습하여 Token Classification Task로 치환하여 학습을 진행한다. Fine-tuningQuestion + Paragraph 가 SEP 토큰으로 구분되어 있는 워드 임베딩이 input으로 들어온 후 Start/End Span을 output으로 내보내야 한다. 이 output span은 각 토큰이 답의 시작 토큰일 확률, 각 토큰이 답의 끝 토큰일 확률로 구성되며, 실제 답의 start/end position과 CELoss를 취하게 된다. 12logits = self.qa_outputs(sequence_output)start_logits, end_logits = logits.split(1, dim=-1) 1234loss_fc = CrossEntropyLoss(ignore_index=ignored_index)start_loss = loss_fct(start_logits, start_position)end_loss = loss_fct(end_logits, end_position)total_loss = (start_loss + end_loss) / 2 Post-processing불가능한 답 제거아래와 같은 경우에는 candidate list에서 후보를 제거해야한다. End position이 start position보다 앞에 있는 경우 예측한 위치가 context를 벗어난 경우 (ex - question 위치에서 답이 등장한 경우) 미리 설정한 max_answer_length보다 길이가 긴 경우 최적의 답 찾기 Start/end position prediction 에서 score(logits)가 가장 높은 n개를 찾는다. 불가능한 start/end 조합을 제거한다. 가능한 조합들을 score의 합이 큰 순서대로 정렬한다. Score가 가장 큰 조합을 최종 예측으로 선정한다. Top-k가 필요한 경우 차례대로 내보낸다.","link":"/PStage-MRC-2%EA%B0%95-Extraction-based-MRC/"},{"title":"PStage MRC 3강 - Generation-based MRC","text":"Generation-based MRCGeneration-based MRC 문제 정의Extraction-based mrc와 다르게 주어진 질문의 답이 지문 내에 존재하지 않을 수도 있기 때문에 기본적으로 답변을 ‘생성’하는 문제로 분류한다. input은 Extraction-based mrc와 동일하지만, generation-based mrc에서는 fine-tuning 시 정답 text까지 생성하는 seq2seq 모델로 분류할 수 있다. Generation-based MRC vs. Extraction-base MRC MRC 모델 구조 Seq-to-Seq PLM 구조 (Generation) vs. PLM + Classifier 구조 (Extraction) Loss 계산을 위한 답의 형태 / Prediction의 형태 Free-form text 형태 (Generation) vs. 지문 내 답의 위치 (Extraction) Extraction-based MRC는 f1 계산을 위해 text로의 별도의 전환 과정이 필요하다. Pre-processingTokenizeExtraction과 같이 토큰화를 진행한 뒤 input_ids(또는 input_token_ids)를 생성한다. Generation 모델에서는 PAD 토큰은 사용되지만, CLS와 SEP 토큰의 경우 사용할 수는 있지만 보통 자연어를 이용한 텍스트 포맷으로 대신하는 경우가 많다. ex) CLS -&gt; Question, SEP -&gt; Context 입력표현Extraction 모델과 달리 어텐션 마스크만 사용한다. 대부분 decoder가 사용된 모델에서는 입력시퀀스를 구분하지 않기 때문이다. 출력표현토큰의 시작과 끝 위치를 맞추는 것이 아닌 정답 텍스트를 표현하기만 하면 된다. 모델의 출력을 선형 레이어에 넣고, sequence_length 내 각 위치마다 들어가야할 단어를 하나씩 선택하면서 전체 길이만큼 반복하며 시퀀스를 생성한다. ModelBART seq2seq 문제의 pretraining을 위한 denoising autoencoder를 사용한다. 인코더는 BERT와 같은 bi-directional 구조이며, 디코더는 GPT와 같은 uni-directional(autoregressive)이다. 텍스트에 노이즈를 주고 원래 텍스트를 복구하는 문제를 푸는 것으로 pretraining을 진행한다. Post-processingDecoding보통 스페셜 토큰을 사용하여 문장의 시작점부터 search를 진행한다. Greedy Search 빠르지만 처음에 선택한 답이 잘못된 선택일 수 있다. Exhaustive Search 모든 가능성을 고려하기 때문에 vocab이나 문장 길이가 조금만 길어도 불가능에 가까워진다. Beam Search Exhaustive Search를 하되 각 타임스텝마다 가장 가능성이 높은 Top-k만 유지한다.","link":"/PStage-MRC-3%EA%B0%95-Generation-based-MRC/"},{"title":"PStage MRC 4-5강 - Passage Retrieval - Sparse Embedding, Dense Embedding","text":"Introduction to Passage RetrievalPassage RetrievalPassage Retrieval은 질문에 맞는 문서를 찾는 것을 의미한다. 데이터베이스에 따라서 형태가 달라질 수는 있지만, 일반적으로 웹 상에 존재하는 모든 문서를 대상으로 한다. 예를들어 토트넘이라는 쿼리에 대해서 토트넘의 역사, 우승기록, 손흥민 등에 대한 문서를 가져오는 시스템을 의미한다. Passage Retrieval with MRCPassage Retrieval을 MRC와 연결지어 생각했을 때, Open-domain Question Answering이 가능해진다. Passage Retrieval과 MRC를 이어서 2-Stage로 만들어버리는 것을 의미하는데, 쿼리가 들어왔을 때 데이터베이스에서 적절한 문서를 가져온 후, MRC 모델을 통해 가져온 문서에서 답을 찾는 프로세스이다. 이 때 중요한 것은 정답이 있을 만한 문서를 적절하게 찾아서 가져오는 과정이며, Query와 Passage를 미리 임베딩한 뒤 유사도로 랭킹을 매겨놓고, 유사도가 가장 높은 Passage를 선택한다. Passage Embedding and Sparse EmbeddingPassage Embedding SpacePassage Embedding의 벡터 공간을 의미하며, 벡터화된 Passage를 이용하여 Passage 간 유사도도 계산할 수 있다. Sparse Embedding0이 아닌 숫자가 상당히 적게 분포되어 있는 벡터를 Sparse Vector라고 하는데, 보통 원핫임베딩의 경우 Vector의 차원이 굉장히 커지게 된다. 따라서 Sparse한 Embedding을 구성하는 것이 유리한데, 그 방법으로는 BoW(Bag of Words)를 구성 -&gt; n-gram (하지만 n이 커질수록 제곱으로 vocab 크기가 커진다는 단점이 있음.) unigram: it, was, the, best, of, times bigram: it was, was the, the best, best of, of times Term Value를 결정하는 방법 Term이 document에 등장하는지 (binary) Term이 몇 번 등장하는지 (term frequency) 등 (TF-IDF) 즉 vocab의 크기에 따라서 Embedding 차원이 결정되는데, 등장하는 단어가 많을수록, n-gram에서 n이 커질수록 차원이 커지게 된다. 이러한 BoW 방법론은 Term overlap을 정확하게 잡아내야 할 때 굉장히 유용하지만, 의미(semantic)가 비슷한데 다른 단어인 경우에는 아예 비교가 불가능하다 TF-IDFTF-IDF란?Term Frequency - Inverse Document Frequency 란 단어의 등장빈도 뿐만 아니라 단어가 제공하는 정보의 양을 곱하여 계산하며, 어떤 지문에선 자주 등장하지만 통합 문서에서는 많이 등장하지 않았으면 그 지문에서 많이 등장한 단어들에 점수를 더 주게 된다. Term Frequency (TF)해당 문서 내 단어의 등장 빈도를 의미한다. TF를 계산하는 것은 매우 간단한데, 특정 단어가 해당 문서에 몇 번 등장하는지를 계산하게 된다. 이후에 Normalize를 한 이후 비율로 변환하여 사용하게 된다. Inverse Document Frequency (IDF)단어가 제공하는 정보의 양으로, 특정 Term이 등장한 Document 개수를 총 Document 개수로 나눈 이후에 log를 씌워서 계산한다. Combine TF &amp; IDFTF와 IDF를 곱하여 최종 점수를 계산하며, ‘a’나 ‘the’ 같은 매우 자주 나오는 관사들은 TF는 높지만 IDF가 0에 가깝기 때문에 매우 낮은 점수를 기록할 것이며, 자주 등장하지 않는 고유명사 들에는 IDF가 커지면서 전체적인 TF-IDF 값이 증가할 것이다. TF-IDF를 이용해 유사도 구하기TF-IDF에서는 cosine 유사도를 사용하며, 결국엔 cosine distance로 볼 수 있다. BM25TF-IDF보다 많이 쓰이는 BoW와 유사한 sparse embedding으로, TF-IDF에서 문서의 길이까지 고려하여 점수를 매긴다. 검색엔진이나 추천 시스템에서 굉장히 많이 사용되는 방법론이다. Introduction to Dense EmbeddingPassage EmbeddingPassage Embedding은 구절(passage)을 벡터로 변환한 것으로, TF-IDF와 같은 Sparse Embedding의 경우에는 벡터의 크기는 매우 크지만 실제로 0이 아닌 숫자가 상당히 적게 존재한다. 또한 이러한 BoW 방법론 같은 경우에는 특정 단어가 있어야 0이 아닌 값이 생기기 때문에 실제로 대부분의 값들이 0으로 존재한다. Limitations of sparse embeddingSparse Embedding의 차원이 큰 것은 compressed format으로 해결할 순 있지만… 가장 큰 단점은 유사성을 고려하지 못한다는 것이다. Dense EmbeddingSparse Embedding의 한계를 해결하기 위해 Dense Embedding이 등장하고 최근에는 훨씬 많이 사용되고 있다. Sparse embedding에 비해 더 작은 차원의 고밀도 벡터로 이루어져있고, 대부분의 요소가 0이 아닌 값들을 가지게 된다. 또한 각 차원이 특정 Term에 대응되지 않는다는 특징이 있다. Retrieval: Sparse vs. DenseSparse Embedding은 단어의 존재 유무를 알아맞추기 상당히 유용하지만 의미를 해석하기가 어렵고, 따라서 Dense 처럼 의미가 같더라도 다른 단어로 표현된 것을 탐지할 수 있는 임베딩을 사용하게 된다. 하지만 중요한 Term들이 정확히 일치해야 하는 경우에 Sparse 임베딩의 성능이 확실히 뛰어나기 때문에 Sparse 혼자만으로는 할 수 있는 것이 적기 때문에 Sparse와 Dense를 동시에 사용하거나, Dense만으로 구성하는 경우가 많아졌다. Training Dense EncoderBERT와 같은 Pre-trained Language Model(PLM)이면 Dense Encoder가 될 수 있으며, 그 외 다양한 Neural Net 구조도 가능하다. Passage를 Embedding 하게 될 때는 CLS 토큰의 output을 사용하게 된다. 이후엔 Question Encoder와 Passage Encoder 에서 각각 나온 Dense Embedding을 dot product 함으로써 유사도를 구하게 된다. 학습 시에는 두 인코더를 Fine Tuning하게 된다. Dense Encoder 학습 목표와 학습 데이터 학습목표: 연관된 Question과 Passage Dense Embedding 간의 거리를 좁히는 것 (내적값을 높이는 것)으로 higher한 similarity를 찾는 것을 목표로 한다. Challenge: 그렇다면, 연관된 Question과 Passage를 어떻게 찾을 것인가? 이것은 기존 MRC 데이터셋을 활용하여 해결할 수 있다. -&gt; 연관된 Question과 Passage 간의 dense embedding 거리를 좁히며 (positive), 연관되지 않은 것들에 대해서는 거리를 멀게 하는 방식으로 학습을 진행한다. (negative) Negative Sampling Corpus 내에서 랜덤하게 뽑으며, 좀 더 헷갈리는 negative 샘플들 (TF-IDF 스코어는 높지만, 답을 포함하지 않는 것들)을 뽑아낸다. Objective function Positive passage에 대한 Negative Log Likelihood Loss를 사용한다. 성능을 측정하는 방법으로는 Tok-k retrieval accuracy: retrieve 된 passage 중에 답을 포함하는 passage 비율로 측정한다. Passage Retrieval with Dense EncoderFrom dense encoding to retrievalPassage와 Query를 각각 임베딩한 후, 쿼리로부터 가까운 순서대로 Pasage의 순서를 매긴다. From retrieval to odqaRetriever를 통해 찾아낸 Passage를 활용하여, MRC 모델로 답을 찾는다. Dense Encoding을 개선하는 방법 학습방법 개선 (DPR) 인코더 모델 개선 (BERT보다 크고 정확한 Pretrained 모델) 데이터 개선 (더 많은 데이터와 전처리 등)","link":"/PStage-MRC-4-5%EA%B0%95-Passage-Retrieval-Sparse-Embedding-Dense-Embedding/"},{"title":"PStage MRC 6강 - Scaling up with FAISS","text":"Passage Retrieval and Similarity Search저번 강의에서 임베딩을 통해 리트리벌을 진행하려고 할 때 질문과 지문 쪽 각각 인코더가 존재했다는 것을 배웠다. 질문은 그때그때 임베딩을 하며, 지문은 미리 임베딩을 해놓지만 새로운 것이 들어올 때 추가하여 적절한 임베딩을 반환하게 된다. Nearest Neighbor Search에서는 Passage 개수가 늘어날 수록 Top-k 때 Dot product 연산이 부담스러워질 수 있는데, 이러한 과정에서 Similarity Search를 아는 것이 중요하다. MIPS(Maximum Inner Product Search)기본적으로 Nearest Neighbor Search보다, Inner product Search가 더 많이 사용되고 있다. Nearest Neighbor와 같은 L2 유클리디언 거리를 계산하는 것보다 Dot Product의 Maximum을 찾는 문제로 돌리는 것이 더 수월하기 때문이다. 가장 가까운 벡터를 찾겠다는 것은 Maximum Inner Product를 찾겠다는 것으로 이해하면 된다. 하지만 개념 정리나 상상을 할 때는 Nearest Neighbor Search를 떠올리는 것이 더 쉽긴 한다. MIPS는 주어진 질문(query) 벡터 q에 대해 Passage 벡터 v들 중 가장 질문과 관련된 벡터를 찾는 문제이며, 관련성은 내적이 가장 큰 값으로 찾는다. 하지만 실제 검색해야할 데이터는 위키피디아에만 5백만개이며, 수십억, 수십조까지 커질 수 있기 때문에 사실상 모든 문서 임베딩을 Bruteforce로 찾는 것은 불가능하다. Tradeoffs of similarity search1) Search Speed - 쿼리당 유사한 벡터 k개를 찾는 시간. 많은 벡터를 가지고 있을수록 당연히 시간이 오래걸린다. Pruning 을 사용하면 속도 개선 가능 2) Memory Usage - 벡터를 어디에서 가져올 것인지. RAM에 올려놓으면 가장 빠르지만 RAM은 비싸다. Compression 으로 메모리 압축 가능 3) Accuracy - Bruteforce 검색과 얼마나 유사한지. 속도를 증가시키려면 정확도를 어느정도 희생해야 한다. Exhaustive Search로 정확도 개선 가능 Approximating Similarity SearchCompression - Scalar Quantization (SQ)Vector를 압축하여 어느정도의 정보 손실을 감안하면서 메모리 사용량을 낮추는 방법이다. 보통 32bit floating point를 사용하는데, 8bit의 unsigned integer로 압축하여 사용한다. Pruning - Inverted File (IVF)클러스터링 기법으로, 일정 벡터들을 군집화하여 Search Space를 줄인 후 검색속도를 개선하는 방법이다. 군집화 이후 쿼리가 들어왔을 때 쿼리와 가장 비슷한 군집만을 방문하도록 하는 것이다. 군집화 방법으로는 k-means clustering을 가장 많이 사용한다. Inverted file (IVF) 라고 불리는 이유는 각 클러스터에 있는 포인트들을 인덱스로 가지고 있기 때문이다. 각 클러스터의 아이디와 클러스터 벡터들이 연결되어 있는 형태로 데이터 구조가 형성된다. Introduction to FAISSFAISS는 large scale 데이터에 대해서 효율적으로 유사도 검색과 군집화를 도와주는 페이스북의 오픈소스 라이브러리이다. FAISS를 사용하기 위해서는 클러스터링된 벡터들을 확보해야 한다. 클러스터를 랜덤하게 지정하면 비효율적이기 때문에 적절한 군집화를 위해 데이터셋을 활용해야 하며, Quantization을 할 때도 scale할 비율을 계산해야 하기 때문에 학습데이터를 활용해야 한다. 따라서 Train과 Adding 단계가 필요하다. Scaling up with FAISSIVF 인덱스 만들기 (SQ 방식) 1234567nlist = 100 # 클러스터 개수quantizer = faiss.IndexFlatL2(d)index = faiss.IndexIVFFlat(quantizer, d, nlist) # Inverted File 만들기index.train(xb) # 클러스터 학습index.add(xb) # 클러스터에 벡터 추가D, I = index.search(xq, k) # 검색 IVF 인덱스 만들기 (PQ 방식) 벡터 압축 기법으로, 전체 벡터를 저장하지 않고 압축된 벡터만을 저장하여 메모리 사용량을 줄일 수 있음. 12345678nlist = 100 # 클러스터 개수m = 8 # subquantizer 개수quantizer = faiss.IndexFlatL2(d)index = faiss.IndexIVFPQ(quantizer, d, nlist, m, 8) # 각각의 sub-vector가 8 bits로 인코딩됨index.train(xb) # 클러스터 학습index.add(xb) # 클러스터에 벡터 추가D, I = index.search(xq, k) # 검색","link":"/PStage-MRC-6%EA%B0%95-Scaling-up-with-FAISS/"},{"title":"PStage MRC 7강 - Linking MRC and Retrieval","text":"Introduction to Open-domain Question Answering (ODQA)MRC는 지문이 주어진 상황에서 질의응답을 하는 과제이다. 이에 반해 ODQA는 비슷한 형태이지만, 지문 부분이 주어지는 것이 아니라 위키 혹은 웹 전체가 주어지게 되기 때문에 매우 많은 지문을 봐야하는 Large Scale 과제가 된다. ODQA라는 문제는 꽤 예전부터 다루었던 문제이며, short answer with support의 형태를 목표로 했었다. 1) Question processing 질문으로부터 키워드를 선택하여 답변의 타입을 선택할 수 있도록 했다. 2) Passage Retrieval 기존 IR 방법을 활용하여 연관된 document를 뽑고, passage 단위로 자른 후 선별한다. 3) Answer processing 주어진 질문과 passage들 내에서 답을 선택하는 휴리스틱한 분류문제로 진행했다. ODQA는 꽤 역사가 길며, 지금까지는 위와 같은 방식으로 진행되고 있었는데, 2011년 IBM Watson을 통해 발전하게 된다. Retriever-Reader ApproachODQA에서 가장 많이 쓰이는 approach로, DB에서 관련있는 문서를 검색하는 Retriever과 검색된 문서에서 질문에 해당하는 답을 찾아내는 Reader로 이루어진다. 학습 단계Retriever TF-IDF, BM25 -&gt; 학습없음 Dense -&gt; 학습필요 Reader SQuAD와 같은 MRC 데이터셋으로 학습 학습 데이터를 추가하기 위해서 Distant supervision 활용 Distant supervision질문-답변만 있는 데이터셋에서 MRc 학습 데이터 만드는 방법. Supporting document가 필요함. 위키피디아에서 Retriever를 이용하여 관련성 높은 문서를 검색한다. 너무 짧거나 긴 문서, 질문의 고유명사를 포함하지 않는 등 부적합한 문서를 제거한다. answer가 exact match로 들어있지 않은 문서를 제거한다. 남은 문서 중에 질문과 (사용 단어 기준) 연관성이 가장 높은 단락을 supporting evidence로 사용한다. Inference Retriever가 질문과 가장 관련성이 높은 k개의 문서 출력 Reader는 k개의 문서를 읽고 답변 예측 Reader가 예측한 답변 중 가장 score가 높은 것을 최종 답으로 사용 Issues and Recent ApproachesDifferent granularities of text at indexing time위키피디아에서 각 passage의 단위를 문서, 단락, 또는 문장으로 정의할지 정해야 함 Retriever 단계에서 몇 개(top-k)의 문서를 넘길지 정해야 함 Granularity에 따라 k가 다를 수밖에 없음. Single-passage training vs. Multi-passage trainingk개의 passage들을 따로따로 보는 것이 아니라 retrieval이 가져온 문서 전체를 하나의 passage로 취급하여 reader모델이 그 안에서 answer span 하나를 찾도록 하는 것임. 다만 문서가 너무 길어지므로 GPU에 더 많은 메로리를 할당해야하고, 처리해야하는 연산량이 많아진다.","link":"/PStage-MRC-7%EA%B0%95-Linking-MRC-and-Retrieval/"},{"title":"Paper Review를 위한 깃허브 협업 페이지 관리하기","text":"팀프로젝트를 진행하면서 여러 목적성을 가진 채널이 필요해졌고, 그 중 하나가 논문리뷰한 자료들을 어떻게 관리할지였다. 팀원들 모두 퀄리티가 너무 좋게 준비를 잘 해오셔서 시각적인 요소들을 사용하여 자료를 공유할 수 있으면 좋겠다는 생각을 했고, 여러 이야기 끝에 깃허브 페이지를 사용해보기로 했다. Jekyll or Hexo?깃허브 페이지를 관리할 때 주로 Jekyll과 Hexo를 사용한다. 처음에는 Jekyll로 페이지를 꾸며보았지만 동적 사이트 생성방식인 Jekyll은 반영시간이 매우 길고 관리가 귀찮기 때문에 Hexo로 옮기게 되었다. Previous Install Node.js Mac의 경우 brew install node 로 설치할 수 있다. 설치가 잘 되었는지 확인하기 위해서는 cmd(window) 혹은 터미널(mac)에서 npm version 을 쳐보자! git 환경 세팅docs 등 로컬에서 관리할 폴더만 하나 만들어두고, PaperReview 레포지토리를 클론받습니다. git clone https://github.com/Boostcamp-AI-Tech-1-15/PaperReview.git PaperReview 레포지토리를 클론받고 현재 develop 브랜치에 관련 파일들을 전부 올려두었는데, cd PaperReview git checkout develop 위의 명령어를 통해 develop 브랜치로 이동한후 자신이 관리하고 싶은 로컬 디렉토리에 관련된 정보들을 모두 copy 하면 됨. 현재 npm 으로 모듈들을 설치해놓았고, 버전관리 시스템이 구축되어있지 않기 때문에 충돌을 피하기 위해 로컬 디렉토리로 카피해서 옮겨주는 것을 권장한다! ex) 로컬에 docs 라는 폴더를 만든 후 docs 디렉토리로 이동 후 cp -rf ../PaperReview/* . 그냥 ctrl+C,V 혹은 드래그앤드롭 해도 된다. (숨김파일 체크한 후 모두 복사해와야함!!!) 로컬에서 저장할 폴더는 .git 이없어도 가능. (git remote가 없는 상태에서도 가능) 모든 파일이 복사가 끝났으면 npm list 를 통해 아래와 같은 모듈이 전부 잘 있는지 체크한다. (없다면 저한테 연락을..) 포스트 생성하기현재 초기세팅은 진행해놓았기 때문에 post 할 파일만 마크다운으로 생성하고 깃에 배포만 하면 된다. 포스트 생성하는법 hexo new post &quot;파일이름&quot; 위의 명령어를 치게 되면 ~/docs/source/_posts/ 경로에 내가 생성한 이름의 md 파일이 생성됨. 이 때 내가 생성한 파일 이름이 라우터로 들어간다. test라는 이름으로 생성했을 경우 url = https://boostcamp-ai-tech-1-15.github.io/PaperReview/test Test.md 파일을 열어보면 위와 같은 상태일텐데, date, category, tags, title 정도만 수정해주면 된다. category 작성법: string 과 array 형식으로 넣을 수 있다. 단일 스트링의 경우 카테고리 하나만 등록됨. [A, B] 배열 형태로 집어넣었을 경우 12A.. B 형태로 순차적인 child 계층으로 카테고리가 만들어짐. 예를들어 CV의 Detection 카테고리에 넣고 싶다면, [Computer Vision, Object Detection] 이라고 입력하시면 된다. Tags 작성법: Category 와 똑같음. 카테고리와 날짜, 제목, 태그까지 전부 작성이 끝났으면 그 아래에 마크다운으로 작성한 글을 그대로 복사붙여넣기 하면 된다. 이 때 Header 순(#, ##, ###) 으로 TOC(Table of Contents)가 작동하게 되니 참고바랍니다! Warning포스트를 만들 때 layout에 썸네일을 추가할 수 있고, 마크다운 글 안에 이미지나 gif를 추가할 수 있다. 썸네일 추가는 자동으로 추가되는 layout에 thumbnail: 이미지경로 로 추가할 수 있다. 마크다운 글 안에서 이미지 추가는 마크다운 문법인 ![html에 보여질 이미지 설명](이미지경로) 로 추가할 수 있다. 이미지는 source/image 폴더에 추가해주시면 된다. 이 때 주의할 것은 썸네일과 이미지 경로가 다릅니다. 이걸 해결해보려고 하다가 시간이 너무 지체되는 바람에.. 일단은 포기했습니다. ㅜㅜ 썸네일 이미지 경로: /image/이미지 포스팅 글에서의 이미지 경로: /PaperReview/image/이미지 ex) source/image에 test.jpg 를 넣었다고 했을 때 썸네일 경로: thumbnail: /image/test.jpg 포스팅 글에서의 이미지 경로: /PaperReview/image/test.jpg 같은 파일이라고 하더라도 링크가 걸리는 상대경로가 다르기 때문에 꼭 주의 요망!! 깃허브 페이지에 배포하기여기까지 작성이 끝났다면 로컬에서 hexo s 혹은 hexo server 명령어를 통해 서버를 실행한 뒤 localhost:4000 으로 접속해서 배포 전에 미리 확인할 수 있다. (생략 가능) 배포는 정말 간단하게도 hexo deploy --generate 명령만 실행하면 알아서 PaperReview 레포지토리의 gh-pages 브랜치 로 add/commit/push 가 일어난다. 이후 몇 분 뒤 저희 깃허브 페이지로 접속하면 반영되어있는 것을 확인할 수 있다!","link":"/Paper-Review%EB%A5%BC-%EC%9C%84%ED%95%9C-%EA%B9%83%ED%97%88%EB%B8%8C-%ED%98%91%EC%97%85-%ED%8E%98%EC%9D%B4%EC%A7%80-%EA%B4%80%EB%A6%AC%ED%95%98%EA%B8%B0/"},{"title":"RNN 소개","text":"Sequential Model 일상에서 다루는 대부분의 데이터가 시퀀셜 데이터다. (말, 영상 등) 시퀀셜 데이터를 다루는 가장 큰 어려움은, 우리가 가장 얻고 싶은 것은 어떤 영역에 어떤 정보가 있다와 같은 것인데, 시퀀셜 데이터는 길이가 얼마나 될지 모르는 것이다. 즉 데이터가 몇개의 차원으로 되어있을지 모른다는 것인데, 이것은 시퀀셜 모델은 몇 개의 데이터가 들어오든 동작할 수 있어야 한다. 이전에 어떤 데이터가 들어왔을 때 다음에 어떤 데이터가 들어올 지 예측하는 것이 가장 기본적인 모형이다. 과거에 고려해야 하는 정보량이 계속 늘어난다는 것이 가장 큰 어려움이다. 시퀀셜 모델을 가장 간단히 만들 수 있는 방법은, Fix the past timespan 과거의 데이터를 몇 개만 보겠다고 정해두는 것이다. Autoregressive model Markov model (first-order autoregressive model) 가장 큰 특징은 가정을 할 때 바로 이전 과거에만 dependent 한다는 것이다. 이러한 마르코프 모델은 과거의 많은 정보를 버릴 수밖에 없다는 단점이 있다. Latent autoregressive model 위의 모델들은 과거의 많은 정보를 사용할 수 없기에 대안으로 나온 것이 Latent autoregressive model이다. 중간에 Hidden state가 들어가 있는데, 이것은 과거의 정보를 요약하고 있다고 생각하면 됨. 즉 바로 직전 과거에 의존하는 것이 아닌 hidden state에 의존하는 구조이다. Recurrent Neural Network자기 자신으로 돌아오는 구조가 하나 있다. 나의 h_t 에서의 t에서의 hidden state는 x_t에 의존하는 것이 아니라 t - 1에서 얻어진 cell state에 의존하는 구조임. Recurrent 구조를 시간순으로 풀게 되면 입력이 많은 FC Layer로 표현할 수 있다. Short-term dependencies 과거에 얻어진 정보들이 전부 취합되어서 미래에 고려해야 하는데 RNN 자체는 하나의 Fixed Rule로 하나로 취합하기 때문에 과거의 정보가 현재까지 살아남기가 힘들어진다. 즉 불과 몇 스텝 전의 정보는 잘 고려가 되는데 한참 전의 정보는 잘 고려가 되지 않는다. 활성화함수가 sigmoid 인 경우 Vanishing(기울기소실) 문제, ReLU일 때는 exploding gradient(기울기폭발) 문제가 발생한다. Varnila RNN LSTM (Long Short Term Memory) cell state는 내부에서만 흘러가고 t + 1개의 정보를 summarize 해준다. output이 아래로도 흐르는데, 이 output은 previous hidden state로 t+1로도 흐르게 된다. Forget gate 중간에 흘러가는 cell state가 핵심이다. 흘러가는 컨베이어 벨트처럼, 어떤 물건을 올리고 빼고 결정하는 것이 Cell State로 Forget Gate의 역할이다. Decide which information to throw away. $f_{t}=\\sigma\\left(W_{f} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{f}\\right)$ Input gate $\\begin{aligned}i_{t} &amp;=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\\tilde{C}_{t} &amp;=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right)\\end{aligned}$​ Decide which information to store in the cell state. Update cell Update the cell state. $\\begin{aligned}i_{t} &amp;=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\C_{t} &amp;=f_{t} C_{t-1}+i_{t} \\tilde{C}_{t}\\end{aligned}$ Output gate Decide which information to store in the cell state. $\\begin{aligned}i_{t} &amp;=\\sigma\\left(W_{i} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{i}\\right) \\\\\\tilde{C}_{t} &amp;=\\tanh \\left(W_{C} \\cdot\\left[h_{t-1}, x_{t}\\right]+b_{C}\\right)\\end{aligned}$ GRU (Gated Recurrent Unit) 게이트가 두 개임. (reset gate &amp; update gate) cell state가 없고 hidden state만 있음. LSTM보다 GRU의 성능이 높은 경우가 있음. 하지만 Transformer 등장 이후엔 둘 다 잘 쓰지 않음.","link":"/RNN-%EC%86%8C%EA%B0%9C/"},{"title":"Papago NMT API를 이용하여 자동번역 카카오챗봇 만들기","text":"안녕하세요. 네이버의 Papago NMT API를 이용하여 자동번역 카카오톡 챗봇을 구현해 보겠습니다. 사전 준비카카오톡 OBT 신청 개발 환경Goorm IDE, Flask, Python, Kakaotalk, Naver API 1. Naver Developers App 생성https://developers.naver.com/products/nmt/ 우선 위 링크에 들어가서 오픈 API이용 신청을 클릭합니다. 이후 사용 API가 Papago 번역이 맞는지 확인하시고, 비로그인 오픈 API서비스 환경에서 Web설정을 선택하시고 웹서비스 url은 우선 http://naver.com 등 아무 url이나 적어줍니다. 추후에 스킬서버 url과 같은 url을 입력해주시면 됩니다. (사실 이 기능이 뭔지 잘 모르겠습니다.) 생성이 완료 되었다면, 어플리케이션 정보에 가셔서 Client ID와 Client Secret을 메모장에 복사붙여넣기 해줍니다. 2. Goorm IDE API 서버 생성2020/05/30 - [Programming] - Teachable Machine Model과 카카오 챗봇 연동시키기 Goorm IDE에 로그인하시고, 새 컨테이너를 flask stack으로 생성해 줍니다. 위 링크와 동일하게 진행하시고, app.py와 papago.py를 만들어줍니다. app.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import flaskfrom papago import papagoapp = flask.Flask(__name__)@app.route(&quot;/param&quot;, methods=[&quot;POST&quot;])def param(): req = flask.request.get_json() global lang_from lang_from = req['value']['origin'] global lang_to lang_to = req['value']['origin'] print(req)@app.route(&quot;/translate&quot;, methods=[&quot;POST&quot;])def translate(): req = flask.request.get_json() txt = req['userRequest']['utterance'] req['contexts'][0]['lifespan'] = 1 lang_from = req['contexts'][0]['params']['대상언어']['value'] lang_to = req['contexts'][0]['params']['번역언어']['value'] ret = papago(txt, lang_from, lang_to) print(req) res = { &quot;version&quot;: &quot;2.0&quot;, &quot;template&quot;: { &quot;outputs&quot;: [ { &quot;simpleText&quot;: { &quot;text&quot;: ret } } ] } } print(res) return flask.jsonify(res)if __name__ == &quot;__main__&quot;: app.run(host='0.0.0.0') papago.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import urllib.requestimport jsondef select_lang(lang_from, lang_to): if lang_from == '한국어': source = 'ko' elif lang_from == '영어': source = 'en' elif lang_from == '독일어': source = 'de' elif lang_from == '러시아어': source = 'ru' elif lang_from == '프랑스어': source = 'fr' else: source = 'en' if lang_to == '한국어': target = 'ko' elif lang_to == '영어': target = 'en' elif lang_to == '독일어': target = 'de' elif lang_to == '러시아어': target = 'ru' elif lang_to == '프랑스어': target = 'fr' else: target = 'ko' return (source, target)def papago(txt, lang_from, lang_to): client_id = &quot;YOUR_CLIENT_ID&quot; # 개발자센터에서 발급받은 Client ID 값 client_secret = &quot;YOUR_CLIENT_SECRET&quot; # 개발자센터에서 발급받은 Client Secret 값 encText = urllib.parse.quote(txt) source, target = select_lang(lang_from, lang_to) data = &quot;source=&quot; + source + &quot;&amp;target=&quot; + target + &quot;&amp;text=&quot; + encText url = &quot;https://openapi.naver.com/v1/papago/n2mt&quot; request = urllib.request.Request(url) request.add_header(&quot;X-Naver-Client-Id&quot;, client_id) request.add_header(&quot;X-Naver-Client-Secret&quot;, client_secret) response = urllib.request.urlopen(request, data=data.encode(&quot;utf-8&quot;)) rescode = response.getcode() if (rescode != 200): print(&quot;Error Code:&quot; + rescode) return 'Cannot translate' response_body = response.read().decode('utf-8') res = json.loads(response_body) return (res['message']['result']['translatedText']) papago.py에서 client_id와 client_secret 부분에 메모장에 복사해두었던 naver app의 아이디와 시크릿코드를 입력합니다. 코드진행은 이렇습니다. 유저가 카카오톡으로 발화를 보냄 발화가 POST요청으로 들어오면 request를 파싱하고, papago 함수를 실행 encText에 대해서 source언어를 target언어로 번역 후 번역된 message를 return return value를 response의 text로 보냄. app.py에서 request를 받을 때 나타나는 [‘context’]는 아래 카카오 오픈빌더 부분에서 다루도록 하겠습니다. 여기까지 진행이 되셨다면, 포트포워딩 후 명령어를 메모장에 복사 붙여넣기 해줍니다. 3. 카카오 챗봇 설정2020/05/30 - [Programming] - Teachable Machine Model과 카카오 챗봇 연동시키기 위의 내용과 동일하게 채널을 생성해 주고, 오픈빌더에 접속합니다. 스킬탭으로 가서 번역 스킬을 만들어줍니다. 포트포워딩 후 복사해놨던 명령어를 스킬서버 url에 붙여넣기 하고 저장을 눌러줍니다. 이번 챗봇은 시나리오 연결을 살짝 복잡하게 꾸며봤습니다. 저는 웰컴블록과 폴백블록을 통해서 번역언어를 선택하도록 할 것이고, 번역언어 선택과 번역탈출, 번역 세 개의 시나리오로 구성해보았습니다. source 언어와 target 언어를 파라미터로 입력받고 그 두 개의 파라미터를 스킬서버로 보낼 것입니다. 이 때 context의 역할이 중요해집니다. 위와 같이 번역언어를 선택하는 블록에서 대상언어와 번역언어라는 두 개의 파라미터를 만들어준 후에 봇 응답을 받아주고, 저 응답이 나오는 동시에 스킬을 진행할 것입니다. 이를 위해서는 컨텍스트를 연결해주어야 합니다. 컨텍스트를 연결해주면 바로연결 버튼 등을 통해 블록을 연결해주지 않더라도 맥락이 실행되면 자동으로 다음 블록이 실행되게 됩니다. 따라서 파라미터와 같이 새로운 스킬블록으로 넘겨줄 수 있는 것이죠. 코드에서 lifespan을 1로 할당해주는 이유는 컨텍스트는 0회-10회의 입력밖에 받지 못하기 때문입니다. 우측 상단에 점 세 개 표시를 누르게 되면 컨텍스트 설정 버튼이 나타납니다. 번역언어 선택 블록에서 output 컨텍스트에 translate라는 맥락을 만들어줍니다. 번역 블록에서는 봇 응답을 스킬데이터 사용으로 설정해주고, input 컨텍스트에 번역언어 선택 블록에서 만든 translate라는 맥락을 연결시켜 줍니다. 발화가 없는 번역 블록이 실행되면 context의 lifespan이 계속 1로 초기화되기 때문에, 탈출 블록이 없으면 api가 계속해서 호출됩니다. 따라서 번역탈출 블록에 패턴 발화를 입력해주고, 봇 응답에 다시 번역언어 선택 블록으로 돌아갈 수 있게 구성해 주었습니다. 따라서 본 봇은 웰컴 블록 실행 -&gt; 번역언어선택 블록 -&gt; 번역 블록 (무한루프) 탈출 블록이 실행되었다면 번역언어선택 블록 -&gt; 번역 블록 (반복) 위와 같은 방식으로 동작하게 됩니다. 여기까지 완료되었다면, 배포를 하신 후에 생성한 채널과 대화를 시작합니다. 잘 작동하는 것을 확인할 수 있습니다 ^^","link":"/Papago-NMT-API%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%98%EC%97%AC-%EC%9E%90%EB%8F%99%EB%B2%88%EC%97%AD-%EC%B9%B4%EC%B9%B4%EC%98%A4%EC%B1%97%EB%B4%87-%EB%A7%8C%EB%93%A4%EA%B8%B0/"},{"title":"RNN 첫걸음","text":"시퀀스 데이터 소리, 문자열, 주가 등처럼 순차적으로 나타나는 데이터를 시퀀스 데이터로 분류한다. 독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다. P(X_1, ..., X_t) = P(X_t \\mid X_1, ..., X_{t-1})*P(X_1,...,X_{t-1})\\\\=\\prod_{s=1}^{t}P\\left(X_{S} \\mid X_{S-1}, \\ldots, X_{1}\\right) 이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다. 시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다. $H_T = Net_\\Theta(H_{t-1}, X_{t-1})$​ 시퀀스 길이가 길어지면 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요하다. (truncated BPTT) 이러한 문제들 때문에 Vanila RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있다. 이를 해결하기 위해 등장한 것이 LSTM과 GRU이다.","link":"/RNN-%EC%B2%AB%EA%B1%B8%EC%9D%8C/"},{"title":"Teachable Machine Model과 카카오 챗봇 연동시키기","text":"안녕하세요. 어린이도 쉽게 만드는 인공지능, 구글의 Teachable Machine에서 추출한 모델을 가지고 goormide, keras, flask를 이용하여 API를 만들어서 카카오 챗봇과 연동시켜 보겠습니다. 개와 고양이를 분류하는 모델을 만들고 유저가 챗봇에서 개 사진을 보내면 “개”라는 텍스트를, 고양이 사진을 보내면 “고양이”라는 텍스트를 출력해 보겠습니다. 주의사항챗봇을 만들기 위해서는 사전에 카카오 i 오픈빌더에 오픈베타 신청을 하셔야 됩니다. 사전준비 및 모델 생성이미지를 직접 웹캠으로 추출해도 되지만, 편의를 위해 크롬의 확장 프로그램 ‘Image Downloader’를 이용하여 개와 고양이의 이미지를 한꺼번에 다운로드 받겠습니다. [##_Image|kage@J4WVC/btqEwgQ2TzB/oKt2N26rqKRLK2FN2aZa81/img.png|alignCenter|data-origin-width=”1915” data-origin-height=”1731” data-ke-mobilestyle=”widthContent”|크롬 확장프로그램 - Image Downloader||_##] 이미지 다운로드가 완료되었다면 다운로드된 폴더에서 관련 없는 사진을 삭제해 준 다음에 https://www.teachablemachine.withgoogle.com 티처블 머신 사이트에서 Image Project를 생성해줍니다. 1~2. Upload버튼을 누른 후 다운로드 받은 폴더를 하나씩 드래그 앤 드롭 해줍니다. 3. Train 버튼을 눌러 학습을 시킵니다. 4. Webcam을 File로 바꾼 후 아무 사진이나 하나를 드래그 앤 드롭해서 테스트를 해줍니다. Dog 100%로 이미지 분류 모델이 손쉽게 완성되었습니다! 이후 Export Model을 클릭하고 Tensorflow로 Keras Model을 다운로드 해줍니다. 이렇게 간단한 방법으로 딥러닝을 이용한 이미지 분류 모델이 생성되었습니다! 카카오 챗봇과 연동가능한 API 만들기goormide를 이용하여 Flask 스택의 새 컨테이너를 만들어줍니다. 컨테이너가 생성되었다면 컨테이너를 실행시킵니다. 상단의 파일-&gt;가져오기-&gt;파일을 클릭하여 다운로드 받은 후 압축을 푼 티처블 머신 모델을 업로드해줍니다. 실행시킨 컨테이너의 app.py에 아래 코드를 복사-붙여넣기 합니다. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import tensorflow.kerasimport flaskimport urllib.requestfrom PIL import Image, ImageOpsimport numpy as npapp = flask.Flask(__name__)model = Nonedef load_model(): global model model = tensorflow.keras.models.load_model('keras_model.h5')@app.route(&quot;/api/predict&quot;, methods=[&quot;POST&quot;])def api_predict(): # UserRequest 중 발화를 req에 parsing. req = flask.request.get_json() req = req['userRequest']['utterance'] # 이미지 전처리 - 발화가 jpg, png 확장자일 때만 실행 if 'jpg' in req or 'png' in req: np.set_printoptions(suppress=True) data = np.ndarray(shape=(1, 224, 224, 3), dtype=np.float32) urllib.request.urlretrieve(req, 'img') image = Image.open('img').convert('RGB') size = (224, 224) image = ImageOps.fit(image, size, Image.ANTIALIAS) image_array = np.asarray(image) normalized_image_array = (image_array.astype(np.float32) / 127.0) - 1 data[0] = normalized_image_array prediction = model.predict(data) output = np.argmax(prediction, axis=-1) if (output == 0): msg = &quot;개입니다!&quot; else: msg = &quot;고양이입니다!&quot; # basic card format res = { &quot;version&quot;: &quot;2.0&quot;, &quot;template&quot;: { &quot;outputs&quot;: [ { &quot;basicCard&quot;: { &quot;title&quot;: msg, &quot;description&quot;: &quot;&quot;, &quot;thumbnail&quot;: { &quot;imageUrl&quot;: req }, &quot;buttons&quot;: [ { &quot;action&quot;: &quot;webLink&quot;, &quot;label&quot;: &quot;사진보기&quot;, &quot;webLinkUrl&quot;: req } ] } } ] } } else: # simple text format res = { &quot;version&quot;: &quot;2.0&quot;, &quot;template&quot;: { &quot;outputs&quot;: [ { &quot;simpleText&quot;: { &quot;text&quot;: &quot;사진을 보내주세요&quot; } } ] } } print(res) return flask.jsonify(res)if __name__ == &quot;__main__&quot;: print(&quot;* Loading Keras model and Flask starting server...&quot;) print(&quot;please wait until server has fully started&quot;) load_model() app.run(host='0.0.0.0') 플라스크는 기본으로 5000번 포트를 사용합니다. 상단에 컨테이너 -&gt; 포트포워딩설정 후 내부포트 5000번을 등록시킵니다. (실제 접근은 외부포트 53513으로 합니다.) 이 때 나오는 명령어 부분을 복사해서 메모장에 적어놓습니다. 여기까지 완료되었다면, 터미널에 python app.py 명령어로 서버를 작동시킵니다. 아래와 같은 메세지가 출력된다면 서버가 정상적으로 실행된 것입니다. (종료는 Ctrl+C) 이제 api서버가 완성되었습니다! 카카오 챗봇 생성카카오톡 채널 관리자센터에서 테스트용 채널을 하나 개설해줍니다. https://i.kakao.com/login 사전신청이 완료되었다면 챗봇을 하나 만들어줍니다. 미리 만든 채널과 연결을 시켜주면 카카오톡에서 검색이 가능해집니다. 오픈빌더의 장점은 손쉽게 다양한 유저 시나리오를 구성할 수 있다는 것입니다. 자세한 내용은 도움말 탭을 참고하세요. 우선 스킬 탭으로 와서 스킬을 하나 생성해줍니다. Flask 서버에서 “/api/predict”에 동작하게 해놨으니 URL에는 아까 복사해놓은 명령어(ip주소:포트) 뒤에 /api/predict를 붙여줍니다. 이렇게 스킬을 저장한 뒤, 이미지를 손쉽게 input으로 받기 위해서 폴백블록을 사용해줄 것입니다. 폴백블록은 사용자가 봇이 이해하기 힘든 말을 했을 때 “다시 말씀해 주세요”, “이해하지 못했어요” 등의 메세지를 처리하는 블록입니다. 위와 같이 우측 중단에 만든 스킬을 연결시켜주고, 하단 봇 응답에 “스킬데이터 사용”을 활성화 해주고 저장시킵니다. 이제 우측 상단에 있는 봇테스트 버튼을 누르고 폴백 블록이 잘 동작하는지 확인해봅니다. 위와 같은 메세지가 잘 출력이 되었다면, 배포 탭에 들어가서 배포를 해줍니다. 배포가 완료된 뒤 만든 채널을 찾아서 대화를 시작합니다. Basic Card Format으로 응답이 잘 되는 것을 확인할 수 있습니다. response의 경우 카카오 오픈빌더 도움말에서 상세하게 설명하고 있으니 적절한 포맷을 찾아 응답을 해주면 될 것 같습니다.","link":"/Teachable-Machine-Model%EA%B3%BC-%EC%B9%B4%EC%B9%B4%EC%98%A4-%EC%B1%97%EB%B4%87-%EC%97%B0%EB%8F%99%EC%8B%9C%ED%82%A4%EA%B8%B0/"},{"title":"Tensorflow에서 Value Error: Unknown Layer: Functional 문제","text":"문제 원인 및 발생 경위Tensorflow 로 구현된 사전학습 모델을 사용하게 되면 심심치 않게 해당 오류를 발견할 수 있다. 이것은 로컬 혹은 컨테이너 환경에서의 Tensorflow 버전이 사전학습 모델의 Tensorflow 버전과 일치하지 않아서 생기는 문제이다. 왜냐하면 Tensorflow 2.3 버전에서 model을 저장할 때의 json 구조가 달라졌기 때문이다. 기존의 (Tensorflow 2.2 버전 이전) 모델 저장 구조12{&quot;class_name&quot;: &quot;Model&quot;, &quot;config&quot;: {&quot;name&quot;: &quot;model_1&quot;, &quot;layers&quot;: [{&quot;name&quot;: &quot;input_1&quot;, &quot;class_name&quot;: &quot;InputLayer&quot; ... 변경 이후 (Tensorflow 2.3 버전 이후) 모델 저장 구조12{&quot;class_name&quot;: &quot;Functional&quot;, &quot;config&quot;: {&quot;name&quot;: &quot;functional_1&quot;, &quot;layers&quot;: [{&quot;class_name&quot;: &quot;InputLayer&quot; ... - 참고 smecsm.tistory.com/180 필자는 Tensorflow 2.3 버전이 Release 되기 전에 학습되어 공개된 사전학습 모델들을 사용하거나 새로 학습을 시킨 모델을 load하려는 상황에서 Value Error: Unknown Layer: Functional 에러를 자주 보았는데, 이것을 해결하기 위해 상당히 많은 시행착오를 겪었다. 해결 방안1. 로컬 혹은 컨테이너의 tensorflow 버전을 2.3.0 이상으로 upgrade 한다.주로 구글링 하면 쉽게 보이는 해답들은 pip 를 사용하여 호환이 되는 버전으로 upgrade 를 하라고 나와있다. 1pip install --upgrade tensorflow==2.3.0 하지만 Raspberry Pi와 용량 혹은 CPU 버전이 낮은 컨테이너에서는 아무리 위의 명령을 실행시켜보아도 어떤 이유에선지 Tensorflow 버전이 2.0.0 이상으로 Upgrade 되지 않는 문제가 있어서 다른 해결방법을 찾아보아야 했다. 2. Tensorflow 버전을 downgrade 하여 재학습 시킨다.1번 방안으로 해결되지 않을 경우 tensorflow 버전을 downgrade 하여 재학습시키는 방법밖에는 없다. Google의 Colab 에서는 손쉽게 tensorflow 버전을 조절할 수 있다. (현재 colab의 default tensorflow version 은 2.4.1 이다.) 1.xx 버전으로 다운그레이드 할 경우 1%tensorflow 1.x 2.x 버전 중에서 2.3.0 이하 버전들을 사용해야 하는 경우 12!pip install --upgrade tensorflow==2.0.0!pip install --upgrade tensorflow-gpu==2.0.0 Colab이 아니어도 로컬 혹은 컨테이너 환경에서 마찬가지로 pip install —upgrade tensorflow==VERSION 명령을 통해 tensorflow 버전을 조절할 수 있다. 보통 사전학습 모델을 공유하는 github의 repo들을 보면 train 코드까지 첨부하는 경우가 많다. 학습하려는 데이터의 규모가 크지 않거나 학습에 필요한 시간을 소비할 수 있는 분들은 해당 저장소를 clone 한 다음 제시되어 있는 dataset으로 똑같이 train 시키는 것이 가장 마음 편할 것이다. 하지만 학습에 시간을 투자할 수 없다면…? 마지막 방법은 하나밖에 없다. 3. Tensorflow 를 사용하지 않는다.규모가 크거나 성능이 좋거나 혹은 SOTA 사전학습 모델들을 보면 보통 pytorch로 이루어진 모델들도 함께 공유가 되어있다. pytorch를 사용하도록 하자.. 만약에 pytorch나 keras로 된 모델도 없을 경우에는 tensorflow를 downgrade 하여 학습에 시간을 투자하는 것 말고는 방도가 없는 것 같다. (만약 다른 방안이 있다면 공유 부탁드리겠습니다.) 끝.","link":"/Tensorflow%EC%97%90%EC%84%9C-Value-Error-Unknown-Layer-Functional-%EB%AC%B8%EC%A0%9C/"},{"title":"Transformer 소개","text":"What makes sequential modeling a hard problem to handle? 문장은 항상 길이가 달라질 수도 있고 문장 어순을 바꾸는 등 문법에 항상 완벽하게 대응하는 문장을 만들지 않듯이 중간에 뭐 하나가 빠져있을 수 있음. 또한 permuted sequence라고 뭐 하나가 밀리거나 하는 등의 문제가 있을 수 있어서, 중간에 무언가가 바뀐 시퀀셜 데이터가 들어간다면 모델링이 굉장히 어려워진다. 이것을 해결하기 위해 Transformer가 등장하였고, self-attention 이라는 구조를 사용하게 된다. Attention is all you need - Transformer is the first sequence transduction model based entirely on attention. RNN이라는 구조에서 (하나의 입력이 들어가고, 다른 입력이 들어갈 때 이전 뉴런에서 가지고 있던 cell state가 다음 뉴런으로 들어가는 재귀적 구조) Transformer에는 재귀적인 구조가 없고, attention 구조를 활용했다는 것이 가장 큰 변화이다. From a bird’s-eye view, this is what the Transformer does for machine translation tasks. 트랜스포머 방법론은 시퀀셜한 데이터를 처리하고 인코딩하는 문제이기 때문에 NMT에만 적용되지 않고, 이미지 분류, detection, 이미지분류, Dall-e(문장에 맞는 이미지 생성) 등 여러 태스크에서 활용되고 있다. 어떠한 문장이 주어졌을 때 (불어문장) 그것을 다른 문장으로 바꾸는 것을 하려고 하는 것임. 시퀀셜 데이터를 넣었을 때 시퀀셜 데이터가 나오게 하는 것. 입력 시퀀스와 출력시퀀스의 단어 숫자가 다를 수 있고, 입력 시퀀스와 출력 시퀀스의 도메인이 다를 수 있는 하나의 모델구조임. 원래 RNN에서는 세 개의 단어가 들어가면 세 번의 작동을 했는데, 트랜스포머에서는 세 개든 백 개든 한 번에 인코딩을 할 수 있게 된다. 이렇듯 self-attention 구조에서는 n개의 단어를 한 번에 처리할 수 있다. Six identical (but not shared) encoders and decoders are stacked. 동일한 구조를 갖지만 네트워크 파라미터가 다르게 학습되는 인코더와 디코더가 학습되어 있다. 그렇다면 n개의 단어가 어떻게 인코더에서 한 번에 처리되는가? 그리고 인코더와 디코더 사이에 어떤 정보들을 주고받는가? 마지막으로 어떻게 디코더가 generation 할 수 있는가? 인코더가 n개의 단어를 한 번에 처리할 수 있는 이유 n개의 벡터가 한 번에 들어가게 됨. 하나의 인코더는 Self-attention과 Feed Forward Neural Network 구조를 한 단 씩 거치는 구조로 되어있다. 그리고 출력되는 n개의 출력값이 두 번째 인코더의 input으로 들어가도록 stacked 되어 있다. The Self-Attention in both encoder and decoder is the cornerstone of Transformer. 셀프어텐션은 트랜스포머가 왜 잘되게 되었는지를 나타낼 수 있다. 뒷단의 피드포워드 네트워크는 MLP와 사실 동일하기 때문이다. Ex) NMT문제를 푼다고 가정을 하고, 3개의 단어만 들어온다고 가정을 해보자. 트랜스포머는 세 개의 단어가 주어지면, 세 개의 벡터를 각각 찾아주는 것이다. 여기서 중요한 점은 벡터에서 벡터로 가는게 하나의 피드포워드로 볼 수 있지만, 여기서 중요한 셀프어텐션은 하나의 벡터 x1-&gt;z1 과정에서 단순히 x1의 정보만을 활용하는 것이 아니라 x2, x3의 정보도 활용하는 것이다. 즉 n개의 단어를 만들 때 n개의 정보를 모두 활용하게 되고 피드포워드 네트워크는 디펜던시가 없이 그냥 변환해주는 것에 불과하다. Self-Attention at a high level. 하나의 문장에 있는 단어를 설명할 때는 단어를 그 자체만으로 이해하면 되는 것이 아니라 그 문장속에서 단어가 어떠한 interaction을 가지는지 알아야 한다. 트랜스포머는 위 사진의 it이 어떠한 단어들과 관계를 가지는지 계산하는 것이다. 기본적으로 Self-attention은 세 개의 벡터를 만들게 된다. (세 개의 뉴럴넷이 있다고 생각하면 됨) 그 벡터는 Q, K, V이며, 각각을 쿼리벡터 키벡터 밸류벡터라고 부른다. Query, Key, Value vectors are computed per each word (=embedding). 하나의 입력이 주어졌을 때 하나마다 세 개의 벡터를 이루게 되고 이 세 개의 벡터를 통해 x1이라고 불리는 첫번째 단어에 대한 임베딩 단어를 새로운 벡터로 바꿔줄 수 있다. 각 단어마다 Q, K, V 벡터들을 만들었는데, 맨 처음에 하는 것은 Score 벡터를 만드는 것이다. 이 스코어 벡터를 계산할 때 인코딩을 하고자 하는 쿼리벡터와 나머지 n개의 키벡터를 구하고, 그 두 벡터를 내적한다. 즉 이 두 개의 벡터(쿼리, 키벡터)가 얼마나 align이 잘 되어있는지 보고 나머지 n개의 단어와 얼마나 유사도가 있는지 계산하게 된다. 내가 인코딩하고자 하는 쿼리벡터와 나머지 벡터들의 키 벡터를 전부 구한다음에 내적. 내적을 한 것은 결국에 R 번째 단어와 나머지 단어 사이에 얼마나 interaction 해야하는지를 알아서 학습하게 하는 것이고 이것이 결국 attention (특정 태스크를 수행할 때 특정 timestep에 어떤 입력들을 더 주의깊게 봐야할 지) 이다. 스코어벡터가 나오면 normalize하는데, key 벡터가 몇차원으로 할 지는 하이퍼파라미터고 키벡터 차원의 sqrt를 취해서 그것으로 나눠주게 된다. 스코어밸류 자체가 어떠한 range 안에 들어가게 하기 위해서 query 혹은 key 벡터 차원의 sqrt로 나눠주는 것이다. 그리고 score 벡터가 확률이 되게 하기 위해 softmax를 취해준다. 이렇게 attention weight를 구할 수 있음. 어텐션 웨이트는 각각 단어가 다른 단어, 혹은 자신 단어와 얼마나 Interaction을 해야하는가이고 이것은 스칼라로 나온다. 하지만 그 값이 어떤 값이 될지가 중요한데, 그것은 밸류벡터가 결정한다. 임베딩벡터가 주어지면 각각의 임베딩벡터마다 쿼리키밸류 벡터를 만들었고 그러고나서 나오는 키벡터와 밸류벡터의 내적으로 스코어벡터를 만들고, 소프트맥스를 취해 스칼라를 만든 이후 최종적으로 사용할 값은 각각의 단어 임베딩에서 나오는 밸류벡터들의 웨이트의 합이 되는 것임. 즉 밸류벡터들의 웨이트를 구하는 과정이 각 단어에서 나오는 쿼리벡터와 키벡터의 내적을 정규화하고 소프트맥스를 취하고 나온 attention을 밸류벡터의 웨이트 섬을 한 것이 임베딩벡터의 인코딩된 벡터가 되는 것임. 여기서 주의할 점은 Query 벡터와 Key 벡터의 차원은 항상 같아야한다. 하지만 Value 벡터의 차원은 달라도 된다. 밸류벡터는 결국 웨이티드 섬만 하면 되기 때문. Multi-headed attention 은 앞에서 했던 어텐션을 여러 번 하는 것이다. (Q, K, V)를 n개 만드는 것이 멀티헤드어텐션임. 이렇게 함으로써 얻을 수 있는 것은 n 개의 어텐션을 반복하게 되면, n개의 인코딩 벡터가 나오게 된다. 여기서 중요한 것은 인코더가 하나만 있는데 인코딩된 벡터가 다음으로 넘어갈 때 필요한 것은 입출력 차원이 맞아야한다. 임베딩된 디멘젼과 인코딩되서 셀프어텐션으로 나오는 벡터가 항상 같은 차원이어야 한다. Positional Encoding 입력에 특정 벡터값을 더해주는 bias이다. 필요한이유: 트랜스포머는 n개의 단어를 sequential 하게 넣어줬다고 하지만, 실질적으로 sequential한 정보가 사실은 포함되어 있지 않다. abcd, bcda, cdba 등의 각각 단어들이 인코딩되는 값은 달라질 수가 없다. order에 의존적이지 않기 때문이다. 실제로 문장을 만들 때 어떤 단어가 먼저 나왔는지는 중요하지 않다. 따라서 포지셔널 인코딩이 필요하게 된다. Self-attention으로 n개의 단어가 주어지면 n개의 단어가 나타나는데, i번째 단어를 인코딩할 때 나머지 n개의 모든 입력 정보를 활용하여 각각의 단어들에 대하여 인코딩 벡터를 찾는다. 즉 encoder 안에서는 self-attention, residual-connection, normalize, feed-forward 의 순서로 각각의 인코딩된 벡터에 대해서 독립적으로 동일한 뉴런네트워크가 동작하게 된다. Encoder는 주어진 단어를 표현하는 것이었고 디코더는 그것을 가지고 무언가를 생성하는 페이즈다. Decoder에서는 어떤 벡터가 인코더에서 전해졌는지가 중요하다. Transformer는 결국에 Key와 Value를 보내게 된다. i번째 단어의 쿼리벡터와 나머지 단어들의 키벡터를 곱해서 어텐션을 만들고 거기에 밸류벡터의 웨잍섬을 하는데, input을 디코더에서 출력하고자 하는 단어들에 대해서 만드려면, Key벡터와 Value벡터가 필요하고, 가장 상위 단어들의 레이러를 만들게 된다. 디코더에 들어가는 단어들로 만들어지는 쿼리벡터와 인코더(입력)으로 주어지는 벡터를 가지고 최종값을 만들어내는 것이다. 그리고 최종 출력은 autoregressive 하나의 단어씩 만들게 된다. In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence which is done by masking future positions before the softmax step. The Encoder-Decoder Attention layer works just like multi-headed self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values from the encoder stack. The final layer converts the stack of decoder outputs to the distribution over words. Vision Transformer (ViT)트랜스포머는 이미지에도 활용되고 있는데 이미지 분류를 할 때 인코더만 활용하고 인코더 벡터를 트랜스포머에 집어넣는 식으로 동작한다. 이미지를 특정 영역으로 나누고 각각 영역에 sub patch들을 linear layer에 넣어서 시퀀셜한 데이터처럼 활용한다. 즉 Transformer는 단순 NMT, NLP 에만 활용되는 것이 아니라 비전에도 활용되고 있다. DALL-E텍스트-&gt;이미지 트랜스포머의 디코더만 활용을 했고 이미지도 그리드로 나눠서 시퀀스로 활용했고 문장 역시 시퀀스로 활용했다. GPT-3 를 활용했다고 함.","link":"/Transformer-%EC%86%8C%EA%B0%9C/"},{"title":"VSCode Terminal Customizing","text":"서버에서 vscode 터미널 많이들 사용하고 계시죠?!! 캠퍼여러분들께 도움이되는 터미널 커스텀에 대해서 소개해드릴까 합니다! vscode의 default terminal은 bash로, 커스터마이징의 끝은 순정이라고는 하지만.. zsh의 강력함을 한 번 맛보면 bash로 돌아가기가 정말 힘듭니다. 그러면.. zsh는 뭐가다른데?! 이미지 한장으로 보여드릴게요. 결과는 아래와 같습니다. 딱봐도 엄청나죠..!!!? 위의 것들은 oh-my-zsh과 powerlevel10k, 그리고 zsh-autosuggestions 과 zsh-syntax-highlighting 이라는 플러그인을 적용한 모습입니다. 여러분들도 이대로만 따라하시면 위와같은 스마트한 기능과 멋을 가진 매력적인 터미널을 얻을 수 있습니다! 자 그렇다면 지금부터 본격적인 세팅 들어갑니다. 레쓰기릿!! Linux 환경을 전제로 둔 방법입니다. MacOS나 Window사용자분들은 주의해주세요! 목차 1. Prev Installation2. install oh-my-zsh3. install zsh plugins4. install powerlevel10k &amp; config p10k5. color customizing Prev Installationcurl 만 설치되어있다면 모든 것을 할 수 있습니다. curl 이 깔려있지 않다면, 터미널을 열고 1apt-get install curl 위의 명령어를 타이핑하여 curl 설치를 먼저 진행해주세요! 저는 이미 깔려있기 때문에, 처음 설치하는 분들은 다른 메세지가 출력될 거에요! 아무튼 저렇게 쫘르르르르륵 나오고 설치가 완료되었다는 메세지가 나타나면 성공! Install oh-my-zsh위에서 설치한 curl을 가지고 oh-my-zsh 를 설치하는 단계입니다. 1sh -c &quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)&quot; oh-my-zsh 는 오픈소스 프로젝트로, zsh을 자유롭게 customizing 할 수 있는 프레임워크입니다. 위와 같은 메세지가 나타나면 성공! 자, 그럼 저희는 한 번의 복붙으로 oh-my-zsh에서 지원하는 여러 테마를 사용할 수 있게 되었어요!! agnoster 와 같은 이쁜 테마도 있긴 하지만, 저희는 조금 더 강력하고 자유로운 커스텀이 쉬운 powerlevel10k 라는 녀석을 사용해보도록 할거에요! 잠깐, 그전에!! 먼저 유용한 zsh plugins 부터 설치하고 갈게요! Install zsh pluginsVim에 대한 유명한 일화를 아시나요..? (대충 수십년간 빔을 사용한 개발자에게 왜 빔만 쓰냐고 물어봤더니, ‘나가는 법을 몰라서’라고 대답하는 짤) Vim을 메인으로 사용하는 개발자들은 대부분 굉장한 자부심은 물론, Customizing에 대한 엄청난 실력을 가지고 있는데요! 그래서 그분들이 shell 기반에서 편리하게 사용할 수 있는 플러그인들을 엄청나게 많이 만들어놓았다는 사실…!! 그 중에 두 가지 유명한 플러그인을 소개해드릴까 합니다! Auto Suggestionszsh-autosuggestions는 이전에 한번이라도 타이핑했던 command line이 있으면, 흐릿한 글씨로 이전에 타이핑했던 command를 제안해주는 기능입니다. 설치방법은 아래와 같습니다. 1git clone https://github.com/zsh-users/zsh-autosuggestions ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-autosuggestions 설치가 완료되었다면, ~/.oh-my-zsh/custom/plugins 라는 경로에 zsh-autosuggestions 라는 폴더가 생겨있을거에요! 이것을 zsh 에 반영을 시켜주어야 합니다. oh-my-zsh를 설치하면 자동으로 root 디렉토리에 .zshrc 라는 파일이 생기는데요, vscode를 root 디렉토리로 여신 다음, .zshrc 파일을 클릭하여 에디터로 띄워줍니다. 쭈우우욱 내리다보면 plugins=(git) 이라고 되어있는 라인을 찾으실 수 있을텐데요! 이 부분을 위와같이 plugins=(git zsh-autosuggestions) 로 바꿔주기만 하면 끝입니다! 바로 이어서 syntax highlighting도 설치해볼까요?! Syntax Highlight syntax highligh 기능은 적절한 리눅스 command라던지, 이전에 설정했던 올바른 alias가 typing 되었다면 색을 바꿔주는 녀석을 말합니다! default 색상은 올바른 command면 녹색, 틀린 command면 빨간색으로 표시가 돼요! 설치방법은 아래와 같습니다. 1git clone https://github.com/zsh-users/zsh-syntax-highlighting.git ${ZSH_CUSTOM:-~/.oh-my-zsh/custom}/plugins/zsh-syntax-highlighting 이 친구도 크게 다를게 없습니다. auto suggestions 처럼 위와 같이 git clone을 이용하여 플러그인을 받아줍니다. 설치가 되었다면 역시나 ~/.oh-my-zsh/custom/plugins 라는 경로에 zsh-syntax-highlighting 이라는 폴더가 생겨있을거에요. 위와 똑같이 zshrc 에서 plugins=(git zsh-autosuggestions)를 찾은다음 plugins=(git zsh-autosuggestions zsh-syntax-highlighting) 으로 수정해주시면 됩니다. 이러면 이제 설치가 끝난 것이나 다름없는데요, 최종적으로 zsh에 반영하기 위해서는 1source ~/.zshrc 위처럼 source command를 이용하여 수정한 zshrc의 내용을 zsh에 반영시켜주어야 합니다! source 까지 끝났다면 ls 나 pwd 같은 명령어를 두세번씩 쳐보세요. 어때요, 조금 쓸만해졌죠?! 자 아직 끝이 아닙니다!! 거의 다왔어요. 조금만 힘내세요! Install Powerlevel10k &amp; Config p10koh-my-zsh에서 powerlevel10k를 설치하는 명령어는 아래와 같습니다. 1git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k 터미널에서 위의 명령어를 친 후, 설치가 되었다면 또다시 .zshrc 파일을 수정해줘야겠죠?! ZSH_THEME=robbyrussell 부분을 찾아서 위의 사진처럼 ZSH_THEME=&quot;powerlevel10k/powerlevel10k&quot; 로 변경해주시면 끝이에요! 그다음에 1source ~/.zshrc source를 통해 zshrc를 zsh에 반영해주면..!!? 짜잔..!!! 설정마법사가 등장했어요. 이제 위에서 나오는 질문들에 대해서 주어진 보기와 같은 알파벳들을 타이핑하면 해당 설정들이 반영된 나만의 터미널이 완성이 된답니다~~~!!! 저 같은 경우엔 one line, compact, 12hour format .. 등등과 같은 옵션을 선택하여 위와 같은 결과를 얻게 되었답니다!! :) Color Customizing마지막으로 VSCode에서의 터미널 Color Customizing에 대한 것입니다! 저같은 경우는 mojave 라는 vscode 테마를 사용하고 있어서 따로 터미널 색을 지정해주진 않았지만, 기본 default theme를 사용중이신 분들께 유용한 링크를 하나 소개해드리려고 해요! 바로, vscode base16 term 이라는 곳인데요, 여기에서 하나씩 클릭해보면서 원하는 색 조합을 선택한 뒤, Copy to clipboard 를 클릭하시면 vscode color setting에 대응하는 json 형태의 key, value가 클립보드에 복사가돼요! 이것을 vscode의 settings.json 파일의 맨 아래에다가 복사를 해주면 끝입니다!! 참고로 settings.json 의 위치는 사용자에 따라서 다를 수 있어요! Vscode에서 ctrl , 를 입력하신 뒤 설정창에서 setting 을 검색하시면 위와 같이 settings.json에서 편집 이라는 링크가 나오는데, 이것을 누르시면 자동으로 연결된 settings.json 파일이 생성되거나, 이미 있다면 기존 settings.json 파일이 열립니다! 그럼 이상으로 terminal customizing 을 마무리하겠습니다!! 긴 글 읽어주셔서 감사하고, 이쁜 터미널로 즐거운 코딩 하시길 바라겠습니다! :)","link":"/VSCode-Terminal-Customizing/"},{"title":"Vscode에서 Python Convention 지정하기","text":"vscode에 python extention 설치하기 가상환경에 autopep8 패키지 설치 conda install autopep8 (anaconda) pip install autopep8 (pip) 가상환경에 pylint 패키지 설치 conda install pylint (anaconda) pip install pylint (pip) vscode에서 cmd , (mac) ctrl , (window) 를 누르고 Settings 탭에 진입. Settings 검색창에 FormatOnSave 검색 후 체크박스에 체크. 위의 방법으로 되지 않는다면, vscode의 settings.json 파일 확인 후 아래 인자들이 작성되어있는지 확인, 되어있지 않다면 복붙하고 저장 python.formatting.autopep8Path 는 직접 확인하여 작성해야함. Mac(m1)의 경우 Conda 설치시 /opt/homebrew/Caskroom/miniforge/base/bin/autopep8 경로에 있음. 123&quot;editor.formatOnSave&quot;: true,&quot;python.formatting.provider&quot;: &quot;autopep8&quot;,&quot;python.formatting.autopep8Path&quot;: &quot;Autopep8 실행파일 위치&quot;,","link":"/Vscode%EC%97%90%EC%84%9C-Python-Convention-%EC%A7%80%EC%A0%95%ED%95%98%EA%B8%B0/"},{"title":"Wille zur Macht - Fridrich Nietzsches","text":"Friedrich Nietzsches(프리드리히 니체)는 모든 존재하는 것들을 살아있으며 생명을 가지고 있는 것으로 상정한다. 이런 생명의 존재를 가능하게 설명해주는 요소가 바로 “Wille zur Macht(힘에의 의지)”이다. 이것이 바로 니체 본인 철학의 대표개념으로 작용한다. 니체가 이야기하는 모든 사회, 정치, 자연 현상들은 모두 유일하게 힘에의 의지로 설명될 수 있기 때문이다. 힘에의 의지는 모든 존재하는 것들을 자체적으로 살아있게 만드는 개념으로서 곧 “Wille zur lebendigen Kraft”이다. 또한 살아있다는 것은 곧 무언가의 ‘주인’이 되고자 하는 의지이며, 경험작용으로서 생존을 위해 더욱 강해지고자 하는 의지이다. 즉, 힘에의 의지는 존재하는 대상에 대한 명명일 뿐 아니라, 생성하는 존재에 대한 명칭이기도 하다. 힘에의 의지는 존재하는 자가 가지고 있는 힘의 내적 세계를 보완하고 통제하는 힘이기도 하다. 이것을 좁은 범위의 개개인에게 적용시켜보면, 의지가 통하는 곳은 자아와 감정, 신체에 해당하며 그것을 조절할 수 있는 힘이라고 해석할 수 있다. 결국 힘에의 의지라는 것은 인간 내부에 존재하는 모든 요소들을 관철하는 ‘내적 통제형 인간’을 향하는 의지이며, 이것을 추구하다 보면 내부에 상정되어 있는 존재자로서의 자아를 성장하고 강해질 수 있게 만든다. 해석하기에 따라 부정적이고 불편하게 보일 수 있는 말임에도, 나는 이 말을 통해 나의 내적 모든 것을 통제하고 성장할 수 있는 자연과학시대의 인간으로서 희망을 가지게 된다. 어설프지 않게, 휘둘리지 않으며 내가 나를 내적 통제형 인간으로 거듭나게끔 하도록, 힘에의 의지를 가슴속에 품고 살아갈 것이다.","link":"/Wille-zur-Macht-Fridrich-Nietzsches/"},{"title":"Word2vec Visualization","text":"작년에 정부주도의 차세대 정보처리 연구사업(이하 차세정) 2019에 연구원으로서 참여했을 때 처음으로 받았던 과제이지만 당시에는 내 실력이 너무 부족했기 때문에 시도조차 할 수 없었다. 오랜만에 지도교수님을 뵈었을 때, 실력이 늘었으면 한 번 시도해보라는 말씀에 다시 과제를 잡게 되었다. 그런데 웬걸, 몇시간도 걸리지 않고 과제를 완료할 수 있었다. 과제관련 github Van-Thuy Phi라는 사람이 만든 프로젝트로 word2vec model을 json으로 converting한 후 웹페이지에 tree구조로 시각화하는 것이 과제였다. 내가 받은 과제에서는 english-cosine-skipgram으로 생성된 모델들이 전처리가 되지 않은 채 들어있었다. Regex (정규표현식)을 이용하여 전처리를 해준 뒤 바로 웹에 띄워보려고 했지만, 검색하려는 target 단어가 json 모델 안에 key값으로 등재되어 있지 않으면 출력되지 않는 문제가 발생했다. 12345678910if (!(nearest_words[i][&quot;w&quot;] in data)) { if (i != (topn - 1)) { flare += &quot;]},&quot;; } else { flare += &quot;]}&quot;; } continue;}else { 이 부분을 해결하기 위해 frontend/js/index.js 파일에서 5개의 반복문 각각이 시작되기 직전에 위와 같은 if else 구문을 추가하여 검색하려는 단어가 없으면 json format만 채워주고, 검색할 단어가 있을 때만 json을 parsing하는 코드가 돌아가도록 구성했다. 코드 전체 보기 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383var margin = {top: 20, right: 120, bottom: 20, left: 120}, width = 960 - margin.right - margin.left, height = 800 - margin.top - margin.bottom;var i = 0, duration = 750, root;var tree = d3.layout.tree() .size([height, width]);var diagonal = d3.svg.diagonal() .projection(function(d) { return [d.y, d.x]; });var svg = d3.select(&quot;body&quot;).append(&quot;svg&quot;) //.attr(&quot;width&quot;, width + margin.right + margin.left) .attr(&quot;width&quot;, '100%') .attr(&quot;height&quot;, height + margin.top + margin.bottom) .append(&quot;g&quot;) .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);document.getElementById(&quot;submit&quot;).onclick = function() { svg.selectAll(&quot;*&quot;).remove(); var word = document.getElementById(&quot;word&quot;).value; var topn = document.getElementById(&quot;topn&quot;).value; //console.log(word); if (word == &quot;&quot;) alert(&quot;Please enter a word!&quot;); else visualize(word, topn); }function createFlare(word, topn, data) { /*flare = {&quot;name&quot;: &quot;student&quot;,&quot;children&quot;: [{&quot;name&quot;: &quot;graduate&quot;},{&quot;name&quot;: &quot;hosei&quot;},{&quot;name&quot;: &quot;daigaku&quot;},{&quot;name&quot;: &quot;college&quot;},{&quot;name&quot;: &quot;rikkyo&quot;},{&quot;name&quot;: &quot;graduated&quot;},{&quot;name&quot;: &quot;doctorate&quot;},{&quot;name&quot;: &quot;cambridge&quot;},{&quot;name&quot;: &quot;sophia&quot;},{&quot;name&quot;: &quot;doctoral&quot;}]};*/ topn = topn; flare = &quot;{&quot;; word = word; // Check whether word is in list? if (!(word in data)) { console.log(&quot;Word Not Found!&quot;); return 0; } flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + word + &quot;\\&quot;,&quot;; flare += &quot;\\&quot;children\\&quot;: [&quot;; nearest_words = data[word]; for (i = 0; i &lt; topn; i++) { flare += &quot;{&quot;; flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + nearest_words[i][&quot;w&quot;] + &quot;\\&quot;,&quot;; //Before loop 1 // Loop 1 flare += &quot;\\&quot;children\\&quot;: [&quot;; if (!(nearest_words[i][&quot;w&quot;] in data)) { if (i != (topn - 1)) { flare += &quot;]},&quot;; } else { flare += &quot;]}&quot;; } continue; } else { nearest_words_1 = data[nearest_words[i][&quot;w&quot;]]; for (i1 = 0; i1 &lt; topn; i1++) { flare += &quot;{&quot;; flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + nearest_words_1[i1][&quot;w&quot;] + &quot;\\&quot;,&quot;; //Before loop 2 // Loop 2 flare += &quot;\\&quot;children\\&quot;: [&quot;; if (!(nearest_words_1[i1][&quot;w&quot;] in data)) { if (i1 != (topn - 1)) { flare += &quot;]},&quot;; } else { flare += &quot;]}&quot;; } continue; } else { nearest_words_2 = data[nearest_words_1[i1][&quot;w&quot;]]; for (i2 = 0; i2 &lt; topn; i2++) { flare += &quot;{&quot;; flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + nearest_words_2[i2][&quot;w&quot;] + &quot;\\&quot;,&quot;; //Before loop 3 // Loop 3 flare += &quot;\\&quot;children\\&quot;: [&quot;; if (!(nearest_words_2[i2][&quot;w&quot;] in data)) { if (i2 != (topn - 1)) { flare += &quot;]},&quot;; } else { flare += &quot;]}&quot;; } continue; } else { nearest_words_3 = data[nearest_words_2[i2][&quot;w&quot;]]; for (i3 = 0; i3 &lt; topn; i3++) { flare += &quot;{&quot;; flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + nearest_words_3[i3][&quot;w&quot;] + &quot;\\&quot;,&quot;; //Before loop 4 // Loop 4 flare += &quot;\\&quot;children\\&quot;: [&quot;; if (!(nearest_words_3[i3][&quot;w&quot;] in data)) { if (i3 != (topn - 1)) { flare += &quot;]},&quot;; } else { flare += &quot;]}&quot;; } continue; } else { nearest_words_4 = data[nearest_words_3[i3][&quot;w&quot;]]; for (i4 = 0; i4 &lt; topn; i4++) { flare += &quot;{&quot;; flare += &quot;\\&quot;name\\&quot;: \\&quot;&quot; + nearest_words_4[i4][&quot;w&quot;] + &quot;\\&quot;&quot;; //Before loop ? // If this is the final layer --&gt; modify (delete &quot;,&quot;) if (i4 != (topn - 1)) { flare += &quot;},&quot;; } else { flare += &quot;}&quot;; } } } flare += &quot;]&quot;; // End Loop 3 if (i3 != (topn - 1)) { flare += &quot;},&quot;; } else { flare += &quot;}&quot;; } } } flare += &quot;]&quot;; // End Loop 3 if (i2 != (topn - 1)) { flare += &quot;},&quot;; } else { flare += &quot;}&quot;; } } } flare += &quot;]&quot;; // End Loop 2 if (i1 != (topn - 1)) { flare += &quot;},&quot;; } else { flare += &quot;}&quot;; } } } flare += &quot;]&quot;; // End Loop 1 if (i != (topn - 1)) { flare += &quot;},&quot;; } else { flare += &quot;}&quot;; } } flare += &quot;]&quot;; flare += &quot;}&quot;; return JSON.parse(flare);}function visualize(word, topn) { word = word; topn = topn; if (document.getElementById(&quot;language&quot;).elements[&quot;language&quot;].value == &quot;English&quot;) { if (document.getElementById(&quot;metric&quot;).value == &quot;Cosine&quot;) { if (document.getElementById(&quot;model&quot;).value == &quot;Skipgram&quot;) data_file = &quot;data/en_data_cosine_skipgram-gb21-Q.json&quot;; else data_file = &quot;data/en_data_cosine_cbow.json&quot;; } else { if (document.getElementById(&quot;model&quot;).value == &quot;Skipgram&quot;) data_file = &quot;data/en_data_euclidean_skipgram.json&quot;; else data_file = &quot;data/en_data_euclidean_cbow.json&quot;; } } else { if (document.getElementById(&quot;metric&quot;).value == &quot;Cosine&quot;) { if (document.getElementById(&quot;model&quot;).value == &quot;Skipgram&quot;) data_file = &quot;data/ja_data_cosine_skipgram.json&quot;; else data_file = &quot;data/ja_data_cosine_cbow.json&quot;; } else { if (document.getElementById(&quot;model&quot;).value == &quot;Skipgram&quot;) data_file = &quot;data/ja_data_euclidean_skipgram.json&quot;; else data_file = &quot;data/ja_data_euclidean_cbow.json&quot;; } } d3.json(data_file, function(error, json) { if (error) throw error; json_data = json; root = createFlare(word, topn, json_data); // Word Not Found if (root == 0) { alert(&quot;Word Not Found!&quot;); return; } root.x0 = height / 2; root.y0 = 0; function collapse(d) { if (d.children) { d._children = d.children; d._children.forEach(collapse); d.children = null; } } root.children.forEach(collapse); update(root); }); d3.select(self.frameElement).style(&quot;height&quot;, &quot;800px&quot;);}function update(source) { // Compute the new tree layout. var nodes = tree.nodes(root).reverse(), links = tree.links(nodes); // Normalize for fixed-depth. nodes.forEach(function(d) { d.y = d.depth * 180; }); // Update the nodes… var node = svg.selectAll(&quot;g.node&quot;) .data(nodes, function(d) { return d.id || (d.id = ++i); }); // Enter any new nodes at the parent's previous position. var nodeEnter = node.enter().append(&quot;g&quot;) .attr(&quot;class&quot;, &quot;node&quot;) .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + source.y0 + &quot;,&quot; + source.x0 + &quot;)&quot;; }) .on(&quot;click&quot;, click); nodeEnter.append(&quot;circle&quot;) .attr(&quot;r&quot;, 1e-6) .style(&quot;fill&quot;, function(d) { return d._children ? &quot;lightsteelblue&quot; : &quot;#fff&quot;; }); nodeEnter.append(&quot;text&quot;) .attr(&quot;x&quot;, function(d) { return d.children || d._children ? -10 : 10; }) .attr(&quot;dy&quot;, &quot;.35em&quot;) .attr(&quot;text-anchor&quot;, function(d) { return d.children || d._children ? &quot;end&quot; : &quot;start&quot;; }) .text(function(d) { return d.name; }) .style(&quot;fill-opacity&quot;, 1e-6); // Transition nodes to their new position. var nodeUpdate = node.transition() .duration(duration) .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + d.y + &quot;,&quot; + d.x + &quot;)&quot;; }); nodeUpdate.select(&quot;circle&quot;) .attr(&quot;r&quot;, 4.5) .style(&quot;fill&quot;, function(d) { return d._children ? &quot;lightsteelblue&quot; : &quot;#fff&quot;; }); nodeUpdate.select(&quot;text&quot;) .style(&quot;fill-opacity&quot;, 1); // Transition exiting nodes to the parent's new position. var nodeExit = node.exit().transition() .duration(duration) .attr(&quot;transform&quot;, function(d) { return &quot;translate(&quot; + source.y + &quot;,&quot; + source.x + &quot;)&quot;; }) .remove(); nodeExit.select(&quot;circle&quot;) .attr(&quot;r&quot;, 1e-6); nodeExit.select(&quot;text&quot;) .style(&quot;fill-opacity&quot;, 1e-6); // Update the links… var link = svg.selectAll(&quot;path.link&quot;) .data(links, function(d) { return d.target.id; }); // Enter any new links at the parent's previous position. link.enter().insert(&quot;path&quot;, &quot;g&quot;) .attr(&quot;class&quot;, &quot;link&quot;) .attr(&quot;d&quot;, function(d) { var o = {x: source.x0, y: source.y0}; return diagonal({source: o, target: o}); }); // Transition links to their new position. link.transition() .duration(duration) .attr(&quot;d&quot;, diagonal); // Transition exiting nodes to the parent's new position. link.exit().transition() .duration(duration) .attr(&quot;d&quot;, function(d) { var o = {x: source.x, y: source.y}; return diagonal({source: o, target: o}); }) .remove(); // Stash the old positions for transition. nodes.forEach(function(d) { d.x0 = d.x; d.y0 = d.y; });}// Toggle children on click.function click(d) { if (d.children) { d._children = d.children; d.children = null; } else { d.children = d._children; d._children = null; } update(d);} word2vec 모델이 tree 구조로 완벽하게 시각화 된 것을 확인할 수 있었다. 코퍼스의 규모가 작아서 타겟 단어들이 key값으로 들어있지 않다 하더라도 검색이 되는 것을 확인할 수 있다. (노드에서 파란색 동그라미가 흰색이 되었지만 tree구조로 시각화되지 않으면 검색이 안되는 단어들임) 애초에 모델을 잘 생성하는 게 좋긴 한가보다. 그래도 확장성과 관련하여 여러 코퍼스 모델에 적용하기도 용이하고, NLP관련 연구에 많은 도움이 될 것 같다. javascript 언어를 사용해본 적도 없는데 이렇게 과제를 해결한 걸 보면 확실히 실력이 늘긴 늘었나보다. 뿌듯하구만.","link":"/Word2vec-Visualization/"},{"title":"Your True Review - 감정분석기반 영화 리뷰 평점 예측 서비스","text":"안녕하세요. ‘2020 국어 정보 처리 시스템 경진 대회’에 참가하게 된 42Seoul의 Team Gaepodong 입니다. 너무나 좋은 동료분들과 함께 프로젝트를 진행하였으나, 대회에는 아쉽게 탈락하게 되었습니다.😭 그럼에도 이렇게 대회 참여 후기를 작성할 수 있게 해주신 Polarbear님과, 이런 좋은 경험을 할 수 있게 해준 42Seoul에 다시금 감사함을 느끼면서 저희의 프로젝트를 공유해드리고자 합니다. 1. 대회 소개 문화체육관광부와 국립국어원이 주최하는 ‘2020 국어 정보 처리 시스템 경진 대회’는 국립국어원이 구축한 말뭉치 자원의 활용도를 높이고 국어 정보를 처리하는 시스템의 개발 및 수준을 높이기 위한 대회로, 매년 개최되고 있습니다. 올해는 지정분야로 ‘감성 분석’과 그 외 자연어처리 Task에 관련된 자유주제의 일반분야로 참가할 수 있었습니다. 저희는 일반분야로 참가하였습니다. 2. 팀 소개개포동 팀은 42NLP를 통해 구성되었습니다. 42NLP는 자연어처리에 관심있는 사람들이 자발적으로 모여서 만들어진 스터디 모임입니다. 우연히 해당 대회에 대한 안내 메일을 받게 되었고, 팀원들을 소집할 수 있었습니다. &lt; 팀원 &gt; Yohlee: Humble하게 Hustle하는 팀장, GPT-42의 예비부모. Jungwlee: 세련된 모델링의 실력자, 강화학습의 차기 선두주자. Iwoo: 커뮤니케이션과 프로젝트 관리능력의 대명사, 잘 배우고 치킨을 좋아함. Sanam: 42Seoul 전체 랭킹 2위, 뛰어난 학습력과 매력적인 두뇌(두피)의 소유자. Kylee: 명료한 통찰력과 세밀함을 갖춘 조력자, 탁월한 정리능력은 덤. 3. 프로젝트 개요바쁜 현대인들은 간소화된 정보인 리뷰와 평점만으로 영화를 선택하는 습성이 있습니다. 하지만 현 시스템 상 리뷰와 평점은 조작이 가능하여 소비자들의 선택에 방해가 될 수 있습니다. 가령 위의 그림처럼, 리뷰 작성자는 영화가 1점이라고 판단하였으나, 배우에 대한 개인적인 감정으로 10점을 주어 리뷰와 평점 간에 큰 괴리가 발생한 것처럼 말이죠. 이런 상황에서 단순 평점으로 영화를 판단한다면 영화의 본질과 다른 해석이 우려될 수 있습니다. 이렇듯 현대인들의 요약된 정보만을 읽는 습성을 반영하였을 때, 영화의 본질과 다른 해석이 가능한 리뷰와 평점 시스템은 사용자가 잘못된 기대감으로 영화를 소비할 수 있음을 암시합니다. 저희는 이러한 문제에 착안하여 대표성을 띠는 리뷰를 추출하고 평점데이터를 보다 객관적인 지표로 사용할 수 있는 방법을 제안하며, 리뷰와 평점 간 괴리를 해결할 수 있는 서비스를 구축하고자 했습니다. 그렇게 ‘감정분석 및 텍스트랭크를 이용한 영화 대표 리뷰 추출과 평점 부여 서비스(Your True Review)’가 탄생하게 되었습니다. 4. 사용기술과 설계버전 정보123456789python3python3-pippython venvmxnet-cu101glonnlpsentencepiece==0.1.85transformers==2.1.1torch==1.3.1konlpy (mecab) Workflow ‘Your True Review’ 웹사이트에서 사용자가 리뷰를 입력하게 되면, 그 리뷰의 긍∙부정 감정수치를 분석하여 그 리뷰의 예상 평점을 부여합니다. 또한 기존 리뷰들을 대상으로 대표성이 있는 문장들을 추출해내고, 많이 쓰인 키워드들을 분석하여 사용자가 알아보기 쉽게 워드클라우드 이미지를 제공합니다. 이를 위해 1) 선택한 영화에 대해 긍∙부정 대표 리뷰 각 10건을 확인하는 ‘리뷰 큐레이션 기능’과 2) 리뷰와 별도로 평점을 매길 수는 없지만, 리뷰를 남겼을 때 문장에서 추출한 긍∙부정 비율 및 평점을 확인하는 ‘자동 평점 부여 기능’을 이용할 수 있도록 설계하였습니다. Frontend영화 목록 페이지와 리뷰분석 페이지로 구성되어 있습니다. 영화를 선택하면 리뷰분석 페이지로 이동하고, 긍∙부정 대표 리뷰 각 10건씩과 워드클라우드, 수집 데이터의 기존평점과 예측평점을 확인할 수 있으며 HTML과 CSS를 이용하여 디자인 되었습니다. Backend사용자의 요청을 받아 응답하는 서버와 리뷰 감정분석 및 평점부여 API 서버를 따로 구성하여 프론트엔드와 연결시켰으며, 웹 서버는 AWS의 EC2 Instance를, API 서버는 Goorm IDE의 컨테이너를 사용했습니다. API에서 모델의 예측을 빠르게 진행하기 위해 RAM 4GB의 CPU 성능이 높은 컨테이너를 사용했습니다. Modeling저희 서비스를 위해 고안한 모델은 Multi-Task와 Transfer-Learning(전이학습) 기반의 Movie-BERT입니다. 언어모델로는 한국어에 최적화된 SKT Brain의 사전훈련 KoBERT를 사용하였으며 NSMC(Naver Sentiment Movie Corpus v1.0) 말뭉치로 미세조정하였습니다. 이 때 한 가지 모델로 감정수치 분석을 통한 긍∙부정 분류와 평점부여라는 두 가지 과제를 수행하기 위해 Hard Parameter Sharing 구조의 멀티태스크 기법이 사용되었습니다. 첫 번째 과제를 학습한 후, 전이학습을 통해 두 번째 과제를 학습합니다. 모델의 학습을 위해 네이버 영화 사이트에서 리뷰와 평점 데이터를 수집했습니다. 해당 사이트의 영화 평점들은 대부분 높게 형성되어 있고, 부정적인 댓글보다 긍정적인 댓글들이 상대적으로 많이 존재합니다. 이러한 점을 고려하여 대조군의 수량을 맞추기 위해 평점이 6~7점이며 5,000개 이상의 리뷰가 존재하는 영화 29개를 선정하였고, 통합적인 전처리 후 리뷰와 평점으로 이루어진 총 16만 개의 원시말뭉치를 구축했습니다. Textrank대표성을 띄는 리뷰를 추출하기 위해 Textrank 방식을 이용합니다. 텍스트랭크는 문서 집합을 추출적요약(Extractive summarization)하는 대표적인 방법으로, 문장그래프를 구축한 뒤 구글이 제안한 Pagerank를 이용하여 키워드와 핵심문장을 선택합니다. 두 문장에 공통으로 등장한 단어개수를, 두 문장 단어 개수의 총합으로 나누어 리뷰들 간의 유사도를 계산한 후 유사도에 따른 문장그래프를 구축합니다. 이 때 품사를 명사∙형용사∙동사로 제한하는데, ‘은/는’ 또는 ‘이/가’ 등 의미를 가지지 않은 품사를 포함시키면 다른 단어와의 유사도가 압도적으로 높아지기 때문입니다. 최종적으로 구축한 문장그래프를 페이지랭크 알고리즘을 통해 학습시킵니다. 이렇게 계산된 랭크를 이용하여 문장을 추출하고, 상위 100개의 단어를 이용하여 워드클라우드 그래프를 생성합니다. 5. 결과이 모든 기술들을 결합시켜서 아래와 같은 서비스를 구현할 수 있었습니다. 영화 ‘마약왕’은 네이버에 등재되어 있는 평점이 6.33인 반면에 본 시스템의 예측평점은 4.85로, 약 1.5가량 낮게 측정된 것을 볼 수 있습니다. 이것은 부정적인 댓글들의 극성을 평점이 정확하게 담아내지 못한 것이라고 해석할 수 있습니다. 또한 “후…진짜 노잼… 연기력으로 보려했는데 영화끝나고 남은게 없고 이게 뭘 말하는 영화지…? 송강호연기는 인정하지만 그냥 마약왕의 일생 쭉 나열하다가 끝…?” 이라는 리뷰를 입력했을 때 76% 확률로 부정이며, 예측 평점은 3점으로 리뷰 작성자의 마음을 평점 정보에 반영했다는 것을 알 수 있습니다. 더 자세한 정보가 궁금하신 분들은 아래 링크를 참조해주세요. >&gt;Explore Github>&gt;Watch Youtube>&gt;Read Paper 6. 참여후기 한 줄 평 끝.","link":"/Your-True-Review-%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D%EA%B8%B0%EB%B0%98-%EC%98%81%ED%99%94-%EB%A6%AC%EB%B7%B0-%ED%8F%89%EC%A0%90-%EC%98%88%EC%B8%A1-%EC%84%9C%EB%B9%84%EC%8A%A4/"},{"title":"const char * vs. char const *","text":"const char s : char is constant.char const s: pointer is constant. const는 바로 뒤에 나오는 것을 상수취급한다. 따라서 const char s-&gt; s++; (o) s = ‘a’ (x) char const s-&gt; s++; (x) s = ‘a’ (o)","link":"/const-char-vs-char-const/"},{"title":"네트워크와 OSI 참조모델","text":"네트워크 효율적인 데이터 전송을 위한 장비와 장비간의 연결을 칭함프로토콜 데이터 전송시 필요한 규칙 및 약속을 미리 정의한 도구외부와 통신 시 IP 프로토콜을 사용하면 Encapsulation과 Decapsulation을 통해 데이터 전송 가능.Encapsulation, Decapsulation 데이터(PDU-Protocol Date Unit)를 패키지화, 해제하는 과정.OSI(Open System Interconnection) 참조 모델 컴퓨터 응용프로그램 정보가 다른 컴퓨터 응용프로그램으로 어떻게 이동하는지 설명.네트워크 설계를 위한 프레임워크 제공(호환성)네트워크 장애 발생 시 (트러블슈팅) 해결방법에 접근하기 위해 필요.Upper Layer 데이터 생성을 담당Lower Layer 데이터 전송을 담당","link":"/%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%99%80-OSI-%EC%B0%B8%EC%A1%B0%EB%AA%A8%EB%8D%B8/"},{"title":"경사하강법","text":"미분 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로 최적화에서 가장 많이 사용하는 기법이다. 미분은 함수 f의 주어진 점 (x, f(x))에서의 접선의 기울기를 구한다. 미분값을 더하면 경사상승법(gradient ascent)이라 하며, 함수의 극대값의 위치를 구할 때 사용한다. 미분값을 빼면 경사하강법(gradient descent)이라 하며, 함수의 극소값의 위치를 구할 때 사용한다. 경사상승/경사하강법은 극값에 도달하면 움직임을 멈춘다. 12345678# gradient: 미분 계산 함수# init: 시작점, lr: 학습률, eps: 알고리즘 종료조건var = initgrad = gradient(var)while(abs(grad) &gt; eps): var = var - lr * grad grad = gradient(var) 미분은 변수의 움직임에 따른 함수값의 변화를 측정하기 위한 도구로서, 최적화에서 제일 많이 사용하는 기법이다. 벡터가 입력인 다변수 함수의 경우 편미분(partial differentiation)을 사용한다. 각 변수 별로 편미분을 계산한 그래디언트 벡터(gradient vector)를 이용하여 경사하강/상승법에 사용할 수 있다. Nabla(역삼각형) 기호로 표기 역행렬을 이용한 선형회귀분석 np.linalg.pinv 를 이용하면 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있다. 행렬 X(n, m)에서 n &gt;= m인 경우, XB = y_hat ~= y -&gt; B = X^+ * y (L2-노름을 최소화) -&gt; = (X^T X)^-1 X^T * y 선형모델의 경우 역행렬을 이용하여 회귀분석이 가능하다. 경사하강법을 이용한 선형회귀분석 선형회귀의 목적식은 ||y - XB||2 이고, 이를 최소화하는 B를 찾아야 하므로 아래와 같은 그래디언트 벡터를 구해야 한다. # norm: L2-노름을 계산하는 함수 # lr: 학습률, T: 학습횟수 for t in range(T): error = y - X @ beta grad = - transpose(X) @ error beta = beta - lr * grad 이론적으로 경사하강법은 미분가능하고 볼록(convex)한 함수에 대해선 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있다. (볼록한 함수는 그래디언트 벡터가 항상 최소점을 향하기 때문) 확률적 경사하강법(SGD, Stochastic Gradient Descent) 확률적 경사하강법은 모든 데이터가 아닌 데이터 한 개 또는 일부(mini batch)를 활용하여 업데이트한다. 볼록함수가 아닌(non-convex) 목적식은 SGD를 통해 최적화할 수 있다. SGD는 데이터의 일부를 가지고 파라미터를 업데이트하기 때문에 연산자원을 좀 더 효율적으로 활용하는데 도움이 된다. 전체 데이터(X, y)를 쓰지 않고 미니배치(X(b), y(b))를 사용하여 업데이트하므로 연산량이 b / n 으로 감소한다.","link":"/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95/"},{"title":"WaveNet 리뷰","text":"Short Summary 적은 데이터셋과 단일 모델만으로 다양한 오디오 파형을 생성할 수 있어서 TTS, 음악, Voice Conversion 등 분야에서 SOTA(state-of-the-art)를 기록한 WaveNet은 화자의 Identity나 음악의 장르 등을 특징($h$)으로 추가하면 특징에 맞는 output을 산출할 수 있는 확률적 모델이자 자기회귀(AR) 모델이다. 이전에 신호처리와 Image Segmentation에 사용되던 Causal Convolution은 많은 레이어를 필요로했고, Receptive Field를 확장시키기 위해서는 큰 필터가 필요하여 연산에 많은 비용이 들었다. WaveNet은 이러한 문제들을 Dilated Convolution을 사용하여 Receptive Fields를 효율적으로 넓힐 수 있었다. 그 결과 Causal Convolution보다 연산비용을 줄이고 학습속도를 시퀀셜 모델을 처리하던 RNN, LSTM보다 획기적으로 줄일 수 있었다. 기존 16bit 정수형 시퀀스로 저장되던 음성신호들의 연산에는 각 레이어마다 65,536개의 확률 계산이 필요했는데, $\\mu$-law companding transformation 방법을 사용하여 256개의 신호로 양자화시킬 수 있었다. Activation Unit Gate (LSTM에서의 input게이트와 유사) 를 사용하여 linguistic features (주파수, 음의 높낮이, 숨소리, 세기 등)들을 재현할 수 있었다. Abstract WaveNet은 음성의 파형을 생성하는 모델. 과거의 음성 데이터 $x_1, x_2, … , x_{t - 1}$ 가 주어졌을 때 $t$ 시점을 기준으로 $x_t$ 라는 데이터가 음성으로써 성립할 확률 $P(x_1, …, x_{T-1}, x_T)$ 을 학습한 확률론적(probabilistic) 모델이자 자귀회귀(autoregressive) 모델이다. TTS(Test-To-Speech)에 적용할 때는 SOTA(당시 2016년)를 달성했고 영어와 중국어에서 사람의 음성만큼 자연스러웠음. WaveNet은 다양한 화자의 음성적 특징들을 동일한 정확도로 감지할 수 있음. 음악을 학습했을 때도 사실적인 음악 파형들을 생성했음. Waveform을 결합확률분포로 표현 -&gt; Conv Layer를 쌓아서 모델링하겠다. IntroductionWaveNet은 PixelCNN 구조를 기반으로 한 Audio 생성모델이다. WaveNet의 특징 WaveNet은 이전에는 TTS 분야에서 불가능했던 자연스러운 음성 신호를 생성할 수 있다. long-range temporal dependencies를 해결하기 위해 dilated causal convolution 을 개발했으며, receptive fields를 매우 크게 넓힐 수 있었다. 단일 모델로 다양한 음성을 생성할 수 있다. 적은 음성인식 데이터셋으로도 좋은 성능을 낼 수 있으며 TTS, 음악, Voice Conversion 등 여러 분야에 응용될 수 있다. WaveNetWaveNet은 Audio의 파형을 직접적으로 생성할 수 있다. 파형 $x = \\{x_1, \\ …,\\ x_T\\}$ 의 결합확률은 아래와 같이 조건부확률의 곱으로 표현될 수 있다. P(x_1, ..., x_{T-1}, x_T) = \\prod_{t=1}^{T} P\\left(x_{t} \\mid x_{1}, x_{2}, \\ldots, x_{t-1}\\right)즉, 각각의 오디오데이터 $x_t$ 는 모든 이전 timestep을 바탕으로 조건부 확률이 계산되고, 그 확률을 이용하여 다음 시점의 오디오를 생성할 수 있다. PixelCNN과 유사하게 조건부확률분포는 Convolution Layer의 스택으로 모델링된다. 여기에는 Pooling Layer가 없고 모델의 output은 input의 차원과 동일하다. 모델은 현재 시점의 데이터 $x_t$ 에 대해서 softmax와 최적화를 거쳐 다항분포를 산출한다. Dilated Causal Convolutions WaveNet의 핵심은 Causal Convolutions이다. 이것을 사용함으로써 데이터의 순서를 훼손하지 않을 수 있으며 그와 동시에 $P(x_{t+1}\\ | \\ x_1,\\ …, \\ x_t)$ 는 미래의 timestep인 $x_{t+1},\\ x_{t+2},\\ …,\\ x_T$ 에 의존하지 않는다. Causal Convolution의 핵심은 마스크텐서를 구성하고 convolution kernel에 적용하기 전에 mask와 elementwise 곱 연산을 수행하게 된다. 실측된 x를 알고있기 때문에 학습 시점에서 모든 타임스텝의 조건부확률은 병렬적으로 계산된다. 모델은 음성을 sequential한 확률로 만드는데, 각각의 예측값들이 산출된 이후에는 다음 예측으로 샘플들이 되돌아간다. Causal Convolutions 모델은 반복되는 connection들을 가지고 있지 않기 때문에 매우 긴 시퀀스를 적용하더라도 RNN보다 학습이 빠르다. Causal Convolution의 문제점 중 하나는 많은 레이어를 필요로하며, receptive field 를 확장시키기 위해 큰 필터가 필요하다는 것이다. 해당 논문에서는 엄청난 연산비용의 증가 없이 대규모의 receptive fields를 위해 dilated convolutions을 사용했다. dilated convolution (옆으로 팽창한 합성곱)은 특정 스텝에서의 input을 스킵함으로써 그 길이보다 큰 영역에 필터를 적용시킨 합성곱이다. 이것은 Convolution에서의 pooling이나 strides와 비슷하지만 input과 output의 차원을 동일하게 하고 훨씬 효율적이다. Dilated Convolution은 이전에 신호처리나 Image Segmentation에 사용되었다. 해당 논문에서는 $1, 2, 4, \\ …, \\ 512, 1, 2, 4,\\ …, \\ 512, 1, 2, 4, \\ …, \\ 512$ 총 30개의 dilations를 사용하였다. Softmax Distributions조건부 확률 분포를 모델링할 때의 접근방법 중 하나는 Mixture Density Network 혹은 MCGSM(mixture of conditional Gaussian scale mixtures) 혼합모델이었다. 그러나 PixelCNN 등장 이후 단순 연속된 데이터(이미지 픽셀이나 오디오 샘플 등)에도 Softmax 분포가 대체로 성능이 더 좋다고 알려졌다. 이 이유 중 하나는 shape에 대한 가정이 없기 때문에 다항분포가 더 유연하고 쉽게 임의의 확률분포를 모델링할 수 있다는 것이다. 원시 오디오샘플은 전형적으로 16비트 정수형 시퀀스로 저장되기 때문에, softmax layer는 매 타임스텝마다 65,536개의 확률을 다뤄야 할 필요가 있다. 이것을 더 다루기 쉽게 하기 위해 $\\mu$-law companding transformation(ITU-T) 방법을 적용했고, 256개로 양자화시킬 수 있었다. f\\left(x_{t}\\right)=\\operatorname{sign}\\left(x_{t}\\right) \\frac{\\ln \\left(1+\\mu\\left|x_{t}\\right|\\right)}{\\ln (1+\\mu)}\\ \\ \\ \\ (-1 < x_t < 1\\ and \\ \\mu=255)이러한 비선형적 양자화는 선형적인 스키마보다 상당히 의미있는 재현을 해냈다. 특히 Speech 분야에서 더욱 original과 가까운 신호를 재현해냈다. Gated Activation UnitsWaveNet에는 PixelCNN에서 사용된 Activation Unit Gate가 똑같이 사용된다. \\mathbf{z}=\\tanh \\left(W_{f, k} * \\mathbf{x}\\right) \\odot \\sigma\\left(W_{g, k} * \\mathbf{x}\\right) $*$: convolution operator $\\odot$: element-wise multiplication $\\sigma$: sigmoid $k$: layer index $f\\ and\\ g$: filter and gate. $W$: learnable convolution filter. Residual And Skip Connections Residual 과 Skip Connections가 네트워크에 사용되었는데, 이 덕분에 수렴하는 속도가 빨라지고 더욱 깊은 신경망도 학습이 가능해졌다. Residual과 Skip-Connections 들이 신경망마다 Stacking 되어 있다. Conditional WaveNetsWaveNet은 Conditional Modeling $P(x|h)$ 가 가능하다. 이것은 새로운 input인 특징(h)를 추가하면 특징에 맞는 output을 산출할 수 있다는 것이다. p(\\mathbf{x} \\mid \\mathbf{h})=\\prod_{t=1}^{T} p\\left(x_{t} \\mid x_{1}, \\ldots, x_{t-1}, \\mathbf{h}\\right)새로운 input인 $h$ 가 추가적으로 들어왔을 때의 조건부 확률은 위의 수식을 따르게 된다. 이러한 상황은 예를들어 다중화자가 있는 환경에서, 우리는 화자를 한 명 선택하고 모델에 추가적인 input으로 화자의 특징을 전달하게 될 때 발생하게 된다. TTS에서 텍스트 정보를 추가적인 인풋으로 주는 것과 유사하다. 다른 인풋에 대한 모델의 조건부확률은 Global Conditioning 과 Local Conditioning 두 가지 방법으로 나눌 수 있다. Global Conditioning: 시점에 따라 변하지 않는 조건 정보를 추가하는 방법 \\mathbf{z}=\\tanh \\left(W_{f, k} * \\mathbf{x}+V_{f, k}^{T} \\mathbf{h}\\right) \\odot \\sigma\\left(W_{g, k} * \\mathbf{x}+V_{g, k}^{T} \\mathbf{h}\\right) 모델로부터 여러 화자의 음성을 생성하고 싶을 때 사용하는 방법으로 화자의 특징(h)를 모든 시점에 동일하게 추가하여 모델을 학습시킨다. 화자의 특징은 timestep 별로 변하는 정보가 아니기 때문에 전역적으로 모든 시점에 영향을 주게 된다. Local Conditioning: 시점에 따라 변하는 조건 정보를 추가하는 방법 \\mathbf{z}=\\tanh \\left(W_{f, k} * \\mathbf{x}+V_{f, k} * \\mathbf{y}\\right) \\odot \\sigma\\left(W_{g, k} * \\mathbf{x}+V_{g, k} * \\mathbf{y}\\right) \\\\ y=f(h) Context StacksReceptive Field를 증가시키는 방법으로 Dilation 숫자를 증가시킴 더 많은 Layer 구성 더 큰 Filter 사용 Dilation factor 증가 등이 있다. 이것을 보완하는 접근법이 바로 context stack이다. ExperimentsMulti-Speaker Speech Generation단일 WaveNet이 여러 화자의 특징을 포함한 음성을 생성할 수 있는지를 검증함. Text-To-SpeechLinguistic Features (음소, 음소길이, 주파수 등)을 추가로 학습한 후 HMM, LSTM-RNN Model과 비교. Mean Opinion Score(MOS) Test 결과 압도적으로 WaveNet이 가장 높은 점수 획득. Music영어와 중국어 음악 데이터셋을 활용해 LSTM-Concat, HMM과 비교. Speech Recognition생성모델이지만 음성인식 과제로도 실험. Conclusion본 논문은 음성 파형 수준에서 직접적으로 음성을 생성하는 심층 생성 모델 WaveNet을 제안했다. WaveNet은 AutoRegressive하며 Receptive Fields를 확장시킨 Dialted Convolutions과 결합한 Causal Filters를 사용하고 있다. 여기에서 WaveNet이 입력에 따라 global (화자의 특징) 혹은 local (언어적 특징) 적으로 어떻게 조절되는지를 보여주었다. WaveNet을 TTS에 적용했을 때 현존 최고의 성능을 보여주었으며, Music과 Speech Recognition에도 매우 좋은 결과를 보여주었다. 출처 https://joungheekim.github.io/2020/09/17/paper-review/ https://medium.com/@satyam.kumar.iiitv/understanding-wavenet-architecture-361cc4c2d623 https://www.youtube.com/watch?v=GyQnex_DK2k&amp;t=1068s https://www.youtube.com/watch?v=CqFIVCD1WWo&amp;t=294s","link":"/WaveNet-Review/"},{"title":"그리스문자 정리","text":"문자영어한글로마자 표기법Α αAlpha알파aΒ βBeta베타bΓ γGamma감마gh, g, jΔ δDelta델타d, dhΕ εEpsilon엡실론eΖ ζZeta제타zΗ ηEta에타iΘ θTheta쎄타thΙ ιIota요타iΚ κKappa카파kΛ λLambda람다lΜ μMu뮤mΝ νNu뉴nΞ ξXi크시x, ksΟ οOmicron오미크론oΠ πPi파이pΡ ρRho로rΣ σ ςSigma시그마sΤ τTau타우tΥ υUpsilon입실론y, v, fΦ φPhi피fΧ χChi카이ch, khΨ ψPsi프사이psΩ ωOmega오메가o","link":"/%EA%B7%B8%EB%A6%AC%EC%8A%A4%EB%AC%B8%EC%9E%90-%EC%A0%95%EB%A6%AC/"},{"title":"뉴럴 네트워크 - Multi Layer Perceptron","text":"Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains. \\begin{aligned} \\frac{\\partial \\operatorname{loss}}{\\partial w} &=\\frac{\\partial}{\\partial w} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\\\ &=\\frac{\\partial}{\\partial w} \\frac{1}{N} \\sum_{i=1}^{N}\\left(y_{i}-w x_{i}-b\\right)^{2} \\\\ &=-\\frac{1}{N} \\sum_{i=1}^{N}-2\\left(y_{i}-w x_{i}-b\\right) x_{i} \\end{aligned} \\begin{aligned} &w \\leftarrow w-\\eta \\frac{\\partial \\operatorname{loss}}{\\partial w} \\\\ &b \\leftarrow b-\\eta \\frac{\\partial l o s s}{\\partial b} \\end{aligned}","link":"/%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-Multi-Layer-Perceptron/"},{"title":"딥러닝 학습방법 이해하기","text":"비선형모델 신경망 신경망 모델은 기본적으로 비선형 모델이지만, 보통 선형 모델과 비선형 모델의 결합으로 이루어져 있다. 소프트맥스(softmax) 연산 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산임. 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다. def softmax(vec): denumerator = np.exp(vec = np.max(vec, axis=-1, keepdims=True)) numerator = np.sum(denumerator, axis=-1, keepdims=True) val = denumerator / numerator return val 신경망은 선형모델과 활성함수(activation function)을 합성한 함수이다. Multi-layer Perceptron(MLP)은 신경망이 여러층 합성된 함수이다. 활성함수 활성함수는 비선형함수이다. 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다. 시그모이드 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만 딥러닝에선 ReLU 함수를 많이 쓰고 있다. 층을 여러 개 쌓는 이유 이론적으로는 2층 신경망으로도 임의의 연속함수를 근사할 수 있다. 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하다. 층이 얇으면 필요한 뉴런의 숫자가 기하급수적으로 늘어나서 넓은(wide) 신경망이 되어야 한다. 딥러닝 학습원리: 역전파 알고리즘 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 파라미터를 학습한다. 역전파 알고리즘은 합성함수 미분법인 ‘연쇄법칙(chain-rule)’ 기반 자동미분(auto-differentiation)을 사용한다.","link":"/%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%95%99%EC%8A%B5%EB%B0%A9%EB%B2%95-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/"},{"title":"베이즈 통계학 맛보기","text":"조건부확률 조건부확률 P(A | B)는 사건 B가 일어난 상황에서 사건 A가 발생할 확률을 의미한다. 베이즈 정리는 조건부확률을 이용하여 정보를 갱신하는 방법을 알려준다. theta: 모수 (데이터가 관찰될 확률) 사후확률: 데이터를 관찰했을 때 이 가설이 성립할 확률 (데이터 관측 이후 측정한 확률이라 사후확률임) 사전확률: 모델링 이전에 사전에 주어진 확률로 이해할 것. (데이터 분석 전 타겟에 대한 모수나 가설 등 미리 설정한 값.) 가능도: 현재 주어진 파라미터(모수)가정에서 이 데이터가 관찰될 확률 Evidence: 데이터 전체의 분포 ex) COVID-99의 발병률이 10%로 알려져있다. COVID-99에 실제로 걸렸을 때 검진될 확률은 99%, 실제 걸리지 않았을 때 오검진될 확률이 1%라고 할 때, 어떤 사람이 질병에 걸렸다는 결과가 나왔을 때 정말로 COVID-99에 감염되었을 확률은? 사전확률, 민감도(Recall), 오탐율(False alarm)을 가지고 정밀도(Precision)을 구하는 문제임. 사전확률 P(theta) = 10% = 0.1 가능도 P(D given theta) = 99% = 0.99, 오탐율 = 1% = 0.01 evidence P(D) = sum(P(D given theta) P(theta)) = 0.99 0.1 + 0.01 * 0.9 = 0.108 So, P(theta given D) = 0.1 * (0.99 / 0.108) ~= 0.916 COVID-99 판정을 받은 사람이 두 번째 검진을 받았을 때도 양성이 나왔을 때, 진짜 COVID-99에 걸렸을 확률은? 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있다. 조건부 확률은 유용한 통계적 해석을 제공하지만, 인과관계(Causality)를 추론할 때 함부로 사용해서는 안된다. 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다. 인과관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다. 제거하지 않았을 때는 가짜연관성(spurious correlation)이 나온다.","link":"/%EB%B2%A0%EC%9D%B4%EC%A6%88-%ED%86%B5%EA%B3%84%ED%95%99-%EB%A7%9B%EB%B3%B4%EA%B8%B0/"},{"title":"벡터가 뭐에요?","text":"벡터란? 벡터는 공간에서 한 점 을 나타낸다. 벡터는 원점으로부터의 상대적인 위치 를 표현한다. 벡터에 숫자를 곱해주면(스칼라곱) 길이가 변하지만, 스칼라가 0보다 작다면 반대 방향 이 된다. 벡터는 숫자를 원소로 가지는 리스트 혹은 배열이다. 벡터끼리 같은 모양을 가지면 덧셈, 뺄셈, 성분곱을 계산할 수 있다. 두 벡터의 덧셈은 다른 벡터로부터 상대적인 위치 이동 을 표현한다. (뺄셈은 방향을 뒤집은 덧셈) 벡터 x에 대하여, $x = 0 + x$ 가 성립한다. 즉 x라는 벡터는 원점벡터에서부터의 상대적인 위치를 나타낸다. 벡터의 노름 벡터의 노름(norm)은 원점에서부터의 거리 이다. L1-노름은 각 성분의 변화량의 절대값을 모두 더한 값이다. L2-노름은 피타고라스 정리를 이용하여 유클리드 거리 를 계산한다. 노름의 종류에 따라 기하학적 성질이 달라진다. L1-노름 상의 원은 마름모꼴 (Robust 학습, Lasso 회귀) L2-노름 상의 원은 흔히 알고 있는 원이다. (Laplace 근사, Ridge 회귀) 두 벡터 사이의 거리를 계산할 때는 벡터의 뺄셈을 이용한다. 뺄셈을 거꾸로 해도 거리는 같다. L2-노름에서 두 벡터 사이의 거리를 이용하여, 제2 코사인 법칙에 의해 두 벡터 사이의 각도를 계산할 수 있다. 내적(inner product)을 통해 분자를 쉽게 계산할 수 있다. 벡터의 내적 내적은 두 성분곱의 총합이다. 내적은 정사영(orthogonal projection)된 벡터의 길이와 관련이 있다. Pros(x)는 벡터 y로 정사영된 벡터 x의 그림자 를 의미한다. Proj(x) = ||x||2cos(theta) 내적은 정사영의 길이를 벡터 y의 길이 ||y||만큼 조정한 값이다. = Proj(x) * ||y||2 = ||x||2||y||2cos(theta)","link":"/%EB%B2%A1%ED%84%B0%EA%B0%80-%EB%AD%90%EC%97%90%EC%9A%94/"},{"title":"부스트캠프 AI Tech 2주차 회고 (2021-08-09 ~ 2021-08-13)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 학습현황 21.08.09 월 (09:47 ~ 19:20) 21.08.10 화 (09:30 ~ 19:19) 21.08.11 수 (09:34 ~ 20:44) 21.08.12 목 (09:30 ~ 23:35) 21.08.13 금 (09:50 ~ 20:30) 진도 Data Viz (시각화의 요소 - Python &amp; Matplotlib - Bar Plot - Line Plot - Scatter Plot) Deep Learning Basic (MLP - Optimization - Convolution - Modern CNN - CV App - RNN - Transformer - Generative Models) 지난 주 선언에서 지킨 것과 지키지 못한 것 수식을 손으로 풀어보자고 했는데 하지 않았다. 따로 수학공부를 시작했다. 이번 주의 개선할 사항 및 자기성찰 이론적인 부분에 많이 몰입하지 못했다. 수학 공부를 따로 시작했으나 목표를 세우지 않고 시작하여 학습에 대한 기준점을 세우지 못했다. 과제 수행과정과 결과물 5개의 필수과제와 3개의 선택과제 분류해결 여부 or 결과 필수과제 MLPO필수과제 OptimizationO필수과제 CNNO필수과제 LSTMO필수과제 Multi Headed AttentionO선택과제 ViTO선택과제 AAEO선택과제 MDNX Facts (사실, 객관) 강사님이 강의에서 필수과제를 다루어주셨기 때문에 어렵지 않게 모두 잘 해결할 수 있었다. 선택과제 두 개는 해결할 수 있었지만 하나는 손도 못댔음. 그마저도 해결한 선택과제는 코드만 작동하게 한 것에 불과하다. Feelings (느낌, 주관) MNIST라는 친숙한 데이터셋을 가지고 여러 테크닉들을 실험해본 정도였기 때문에 그나마 익숙하게 느껴졌다. 손도 못댄 선택과제만 보더라도, 여전히 수학이 너무 어렵게 느껴지고 수식만 보면 토나온다. Pytorch에 대해서 잘 안다고 자만하면 안되겠다. 모르는 것들이 아직도 수두룩빽빽. Findings (배운 점) Pytorch의 편리함 때문인지 이전까지는 Weight Update 과정들만 보더라도 두세줄 정도로 매우 간결한 코드를 썼었는데, 해당 과정들을 한 줄로 쓰니 가독성도 좋고 Process가 한 눈에 보이는 느낌으로 더 좋은 것 같다. ViT와 Auto-Encoder에 대한 깊은 내용은 아직은 잘 모르지만, 그래도 코드로 구현 과정을 보았다는 것이 그나마 건질 수 있었던 배운 점인 것 같다. Affimation (자기선언) 이제부턴 모델들을 구현한 코드들이 과제에 등장하는데, 단순히 과제를 해결하는 것에 급급하지 말고 코드 전체 내용을 꼭 따라가고 이해하도록 해보자. 특히 논문과 밀접한 관련이 있는 좋은 베이스라인 코드들이기 때문에 꼭 숙지할 것! 다음주에는 모든 선택과제를 시도할 수 있도록 더 많은 공부를 해야겠다. 피어세션알고리즘 스터디 아직 Git에 익숙하지 않은 팀원들이 있음에도 협업 프로세스가 잘 진행이 된 것 같다. 특히 fork - pr - review 의 과정이 처음치곤 잘 이루어진 것 같아서 기쁘다. Git 특강이 있었기 때문에 실습 느낌으로 경험해볼 수 있었어서 시너지가 더 있었던 것 같다. 그럼에도 Conflict 해결 과정은 아직 어렵게 느끼는 팀원이 있는 것 같아서 더 많은 것을 공유할 수 있도록 노력해야겠다. 한 문제만 풀었기 때문에 알고리즘 풀이 실력이라던지 신박한 방법을 알 수 있었다던지 하는 것들은 크게 없었던 것 같다. 다음주부터는 개선될 듯! 논문리뷰 세 줄 요약만으로 다른 분들이 읽은 논문의 전반적인 정보들은 파악하기가 힘들었다. 다른 사람들의 말을 듣고 소화하는 것도 결국엔 내 능력이겠지. 나는 음성인식 딥러닝 모델이 최초로 고전모델을 이긴 사례에 대한 논문을 읽었으나 음성인식은 처음이라 그런지.. 모르는 것들 투성이었다. 특히 여러 베이스가 되는 개념들에 대한 용어조차 모르고 있어서 전반적인 이해를 했다고는 할 수 없을 것 같다. 영어공부를 좀 더 해야될 것 같다. 단어만이라도 더 외우자. 시간을 좀 분배해서 하루에 몰아서 읽는 것보다는 틈틈히 읽는 것이 더 좋은 것 같다. 멘토링 빛예닮 빛예닮 빛예닮 취준에 관해 개인 멘토링을 받았는데, 자신감 뿜뿜할 수 있었던 좋은 시간이었다. 조언해주신 것들 + 노력해야할 것들 꼭 잘 지켜서 하반기 시즌에 지원을 한 번씩 해봐야할 것 같다. 학습회고Feelings (느낌, 주관) 학습에 더 나은 몰입을 위해 제공되는 강의 뿐만 아니라 다른 시간들을 더 타이트하게 쪼갤 필요가 있어보인다. 2주차. 벌써부터 슬슬 많은 딥러닝 이론들을 배우기 시작했는데, 완벽히 학습을 따라갔다고는 할 수 없을 것 같다. 여전히 토나오는게 많은데 덜 익숙해서 그렇겠지. Affimation (자기선언) 이론 강의에서 나오는 수식 하나, 키워드 하나조차 가볍게 넘기지 말자. 더 의문을 가져보자. 학습 정리를 좀 더 꾸준하게 해보자. 필기한 내용들을 블로그에 하나도 올리지 않았는데, 포스팅을 위해서라도 내 언어로 바꾸고 정리해보자. 내가 따로 노력한 것들 인공지능을 위한 수학 (절반 정도 읽음) … 끝! 총평 저번주의 회고에서 다짐한 것들은 50% 정도 지켜진 것 같다. 이번주에 다짐한 것들은 다음주에 꼭 다 지킬 수 있도록 할 것. 이번 주 역시 시간이 부족했다. 저번주엔 강의가 많아서 그랬다면 이번주는 강의 난이도가 높아서 그랬다. 복습이 필요할 것 같다. 시간확보에 조금 더 노력하자. 귀중한 기회인 만큼 남은 올해만큼이라도 부캠에 올인하자. 취준멘토링을 통해 부캠에서의 방향성이 얼추 잡힌 듯 하다. CV를 준비하고 짬을내서 프로젝트를 해보자. 난 다 소화할 수 있다.","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-2%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-09-2021-08-13/"},{"title":"멀티태스킹과 식사하는 철학자 문제","text":"https://namu.wiki/w/%EC%8B%9D%EC%82%AC%ED%95%98%EB%8A%94%20%EC%B2%A0%ED%95%99%EC%9E%90%20%EB%AC%B8%EC%A0%9C n명의 철학자가 스파게티를 먹고 있다고 가정한다. 철학자들은 잠자기, 먹기, 생각하기 셋 중 한 번에 하나만 할 수 있고 모든 철학자가 스파게티를 먹어야 한다. 식사하는 중에는 무조건 두 개의 포크를 사용해야 한다. 포크의 개수는 철학자의 인원수와 같으며, 포크는 각각의 철학자 사이에 놓이게 된다. 이 때 만약에 모든 철학자들이 동시에 자신의 왼쪽 포크를 잡게 된다면, 자기 오른쪽의 포크가 사용 가능해질 때까지 기다려야 하는데, 모든 철학자들이 기다리는 상태에 놓이게 된다. 이렇게 아무것도 진행할 수 없는 상태를 교착(deadlock)상태라고 한다. 멀티태스킹이 가능한 멀티쓰레드 환경일 때 사용자 레벨에서 Semaphore나 Mutex를 이용하여 한 쓰레드가 동작중일 때 다른 쓰레드가 CPU를 잡지 못하게 만들면 교착상태를 방지할 수 있다. 참고자료 https://twinw.tistory.com/97 https://bitsoul.tistory.com/156 https://artwook.tistory.com/17","link":"/%EB%A9%80%ED%8B%B0%ED%83%9C%EC%8A%A4%ED%82%B9%EA%B3%BC-%EC%8B%9D%EC%82%AC%ED%95%98%EB%8A%94-%EC%B2%A0%ED%95%99%EC%9E%90-%EB%AC%B8%EC%A0%9C/"},{"title":"부스트캠프 AI Tech 1주차 회고 (2021-08-02 ~ 2021-08-06)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 진도 파이썬기초 -&gt; Numpy -&gt; Pandas AI 기초수학 -&gt; 벡터 -&gt; 행렬 -&gt; 경사하강법 -&gt; 확률론 -&gt; 통계학 -&gt; CNN -&gt; RNN) 개선할 사항 및 자기성찰 Python에 익숙하다고 자만하며 강의를 대충 들었다. 아예 모른다고 할 수 있는 것들도 별로 없었고, 처음 들어보는 것도 없었으나 실제 코딩할 때는 강의를 다시 돌려보는 모습을 보며 아직 자만할 때가 아니라고 느꼈다. 특히 Python pickle과 logging 패키지를 사용한 영속성 객체관리 및 로거사용법과 같이 찾아보면서 할 수 있는 수준으로만 그쳐있는 지식들을 한 번쯤은 파보는 것도 괜찮을 것 같다. 위에 언급한 Pickle과 Logging과 같은 자주 사용될 수 있는 친구들은 Baseline을 따로 만들어서 보관하는 것이 좋을 것 같다. 수학에 좀 더 익숙해지기 머리를 쓰지 말고 손으로 풀어보자. 선형대수/확률론/통계학 세 과목만큼이라도, 반드시 따로 공부를 더 해야한다. 완벽하게 이해하지 못한 내용 AI Math 전체.. (책을 구입해서 차근차근 또 봐야겠다.) 자연상수와 관련된 계산공식들 (Log​, Sigma, ​ Exp 같은..) 꼭 알아둘 것! L2 norm Maximum Likelihood Estimation Bayesian Statistics &amp; Probability 과제 수행과정과 결과물 5개의 필수과제와 3개의 선택과제, 9번의 퀴즈 분류해결 여부 or 결과 필수과제 1 ~ 5, 선택과제 1, 3O선택과제 2X퀴즈 1, 8, 1080점퀴즈 2, 5, 6, 7, 9100점퀴즈 3~460점 Facts (사실, 객관) 필수과제의 경우, 너무 구현에만 급급해서 좋은 코드를 짜지 못했다. 예외처리 레벨을 어느정도까지 했어야 할 지 많은 테스트를 하지 못했다. Pythonic한 코드를 작성하려고 노력하지 않았다. (for문 남발) 절차지향적인 코드로만 작성하였다. 돌진형개발 습관을 버려야되는데 그러지 못했다. 퀴즈를 모두 만점받지 못한 것이 너무 아쉽다. 9개의 퀴즈가 모두 AI Math 관련이었는데, 틀린문제를 보면 대부분 수식을 묻는 것이었다. Feelings (느낌, 주관) 필수과제의 난이도는 쉬운 편이었으나, 선택과제는 체감상 너무 어려웠다. (필수과제는 파이썬, 선택과제는 수학) Baseline의 경우는 매우 공들여서 작성되었다는 것이 한눈에 느껴졌으나 그에 걸맞는 코드내용을 작성하려고 노력해야할 것 같다. 변명하자면 시간이 너무 부족했다. Colab은 너무 불편하다. 화면분할 안되나..? Findings (배운 점) 필수과제 구현 시 너무 급하게 하느라 모든 요구사항들을 제대로 파악하지 않고 코딩을 하다가 수많은 에러를 만나게 되었다. 특히 잘 작성했다고 생각했어도 테스트케이스에서 걸리는 경우가 굉장히 많았다. 이미 추상화가 되어있는 함수들이기 때문에 적어도 과제작성자의 의도를 한 번쯤은 파악하고 들어가자. backpropagation 의 공식 유도 이름만 알고 있었던 최대가능도의 공식들과 활용예제 Affimation (자기선언) 앞으로도 계속 수학관련된 문제에서 낭패를 볼 것 같으니 각오하고 할 수 있을 때 공부해놓자. 제발! 앞으로는 돌진형 개발을 ‘절대’ 하지 않겠다. (TDD가 가능하게끔 테케도 전부 주는데 왜 안했니??) 퀴즈 대충보지 말자. 100점 안맞으면 창피하다. 피어세션 떡볶이조 로그 코드리뷰를 위한 깃 사용 매뉴얼 회의록 첫 피어세션모든 다른 조들도 마찬가지였겠지만 너무너무 어색했다. 처음 보는 사람들과 줌에서 아무렇지도 않게 이야기해야 하는데, 이게 수학보다 어려운듯. 그래도 어찌저찌 서로에 대해서 조금씩 알게되고 있고, 조금씩 말도 많이 할 수 있다는게 신기하면서 재밌는 부분이다. 그리고 모두가 실력자들이다. 굿굿 내가 첫 번째 모더레이터로 찍혀서 어떻게 대화를 주도해보고 그러긴 해봤지만, 역시 너무 어렵다. 제대로된 역할 수행을 하지 못한 것 같아서 팀원들에게 죄송하다. 뭔가 이것저것 막 시도를 해보고 싶은데 강의와 과제와 병행하려니 다른 시도를 할만한 시간적 여유가 없는 것 같다. 아직 첫 주밖에 안돼서 그런거겠지? 그래도 팀 레포지토리도 만들고, 회의록과 그 외 협업에 쓰면 좋을만한 것들을 하나하나 차근차근 만들어보고 있다. 아직 뭔가 제대로된 활동을 피어세션에서 하진 않지만 점점 잘 하게 되겠지!! 멘토님말이 필요없다. 너무 갓갓이다. 솔직히 “멘토 뽑기운이 중요하다” 라는 좀 기분나쁠 수 있는 이야기를 여기저기서 들었는데 우리 멘토님은 이번주에만 벌써 세 번이나 멘토링을 해주셨다. 첫 주고, 방학이셔서 특별히 시간을 더 써주셨는데 덕분에 팀원분들과의 어색한 분위기를 조금은 풀 수 있었고, 우리끼리 회의만 했으면 답이 없었을 피어세션 활동도 제시해주셨다. 말이 필요없다면서 길었는데, 아무튼 너무 좋다!! 학습회고Feelings (느낌, 주관) 위에서 작성했듯이 이번 주 학습은 파이썬 복습과 인공지능 기초수학이었다. ‘기초’라면서. 수학 왜이렇게 어렵니. 복습은 커녕 1회 수강도 간신히 했다. 시간 확보가 가장 중요한 것 같다. 원래의 시간표대로 삶을 진행하는 것이 상당히 어렵다. 강의듣는 시간엔 강의만 듣고 개인학습 시간엔 공부를 해야하는데 통으로 강의만 듣고 과제만했다. 과제를 19시 이후에 학습종료 누르고 진행해야할 것 같다. 강의듣고 정리하는 것만 해도 시간이 빠듯함. Affimation (자기선언) 어떻게 쓴 얘기들이 전부 시간이 부족하다인데 학습시간 확보 잘하자! 그냥 잠을 줄여버려! 총평 강의를 들으며 정리를 해보고, 수식을 따라가려다 보니까 강의듣는 시간이 매우 오래걸린다. 파이썬 기초 강의는 쉬워가지고 두 배속으로 들을 수 있어서 괜찮았는데. AI Math는 배속도 못하고, 개어려웠다. 피어세션은 생각보다 만족한다. 아직은 덜 친해서 한 시간 반이 생각보다 길다고 느껴지는데, 멘토님이 내주신 과제(알고리즘문제풀이와 과제코드리뷰, 페이퍼리뷰 등)들을 하다보면 아마 피어세션 시간이 부족할 듯. 개인학습 시간을 잘 확보해야한다! 과제는 최대한 밤에! 퀴즈는 성심성의껏 보고, 돌진형 개발은 말아라! 여유를 가져보자.","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-1%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-02-2021-08-06/"},{"title":"부스트캠프 AI Tech 3주차 회고 (2021-08-17 ~ 2021-08-20)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 학습현황 21.08.17 화 (09:50 ~ 19:15) 21.08.18 수 (10:20 ~ 23:50) 21.08.19 목 (09:36 ~ 05:00) 21.08.20 금 (09:48 ~ 23:59 예정) 진도 Pytorch (Basic - Pytorch Template - Custom Model - AutoGard &amp; Optimizer - Dataset &amp; Dataloader - Custom Dataset - Monitoring tools - Multi GPU - Hyperparameter Tuning - Troubleshooting) 지난 주 선언에서 지킨 것과 지키지 못한 것 수식 하나, 키워드 하나조차 가볍게 넘기지 말라는 다짐을 어느정도 지킬 수 있었던 것 같다. 파이토치의 공식문서를 트래킹하면서 어떤 역할을 하는지 몰랐던 함수들에 대해서 하나씩 정리해나갈 수 있었다. 이번 주의 개선할 사항 및 자기성찰 한 번에 너무 많은 양의 코드를 실행해보고, 너무 많은 지식을 받아들이려고 하다 보니 뇌용량에 한계가 온 것 같다. 단기간에 기억해야 하는 것들을 전부 기억할 수 있다는 자신감이 떨어지는데, 오래 볼 수 있도록 한 번 정리를 하는게 좋지 않았을까 하는 아쉬움이 남는다. 다음주에는 성실한 블로그 포스팅에 조금 더 신경써보자. 과제 수행과정과 결과물 2개의 필수과제와 1개의 선택과제 분류해결 여부 or 결과 필수과제 1 - Custom Model 개발O필수과제 2 - Custom Dataset 개발O선택과제 1 - Transfer Learning &amp; Parameter TuningO Facts (사실, 객관) 파이토치 공식문서를 읽는 것에 대해 어느정도 익숙해질 수 있었음. 내부 코드들을 뜯어보면서 어떤 식으로 함수를 작성하고 디버깅해야 하는지를 대략적으로 파악할 수 있었음. 과제 양이 역대급으로 많아서 과제 일부는 문제 해결에만 급급하여 제대로 보지 않았다. Feelings (느낌, 주관) 파이토치에 대한 막연한 이해(?) 때문에 무지성코딩만 해왔었는데 이제는 조금 구조적으로, 빠른 시간안에 코딩할 수 있을 것 같다는 자신감이 어느정도 생긴 것 같다. 그와 동시에 너무 많은 양이 있기 때문에 이것을 다 알 수 있을까? 라는 두려움이 생겼다. Findings (배운 점) 공식문서 Tracking 방법과 내부 모듈 코드를 Tracking 하면서 함수의 호출 시점 및 반환여부와 동작과정을 어떻게 알 수 있는지 배울 수 있었다. Affimation (자기선언) 지난 주 선언대로 이번 주의 모든 과제에 도전하였고, 코드 전체 내용을 트래킹할 수 있었다. 다만 다음주에는 중간중간 채 신경쓰지 못하거나 급하게 넘기는 부분들이 더 줄어들어야 할 것이다. 피어세션알고리즘 스터디 지난주보다 어려운 문제에 도전했고 대부분이 잘 풀어냈다. 특히 깔끔한 코드를 짜거나 신박한 방식으로 문제에 접근한 팀원분들의 코드를 보면서 많은 것을 배울 수 있었던 것 같다. 또한 점점 팀원들이 변수명 혹은 함수명에 대한 고찰을 하기 시작했고 관련된 discussion도 리뷰에서 활발히 일어난 것 같아서 매우 긍정적으로 생각하고 있다. 논문리뷰 지난주 피드백에 따라 이번주에는 세 줄 요약과 더불어 그림이나 사진 및 이해에 도움이 되는 요소들을 추가하여 발표를 진행했다. 기존 피어세션 타임을 조금 넘을 정도로 모두가 열심히 준비했고 덕분에 지난 주 보다는 다른 사람들의 논문을 수월하게 대략적으로 이해할 수 있었다. 영어를 읽는 시간이 너무 오래걸린다. 처음부터 끝까지 간단한 번역작업을 동시에 했음에도 4시간 30분 정도가 걸렸는데 (약 12p) 영어 공부의 필요성이 점점 실감나는 것 같다… 저번 주의 회고대로 틈틈히 읽는 버릇을 들여야 하는데 아직은 시간을 효율적으로 쓰는 것이 어려운 것 같다. 멘토링 지난 주 선택과제 리뷰와 2주차 활동에 대한 피드백을 받을 수 있었다. 나를 조금 과대평가하시는 것 같은데..!? 기대에 부응하고 싶다. 지난 주 챌린징한 논문을 일부러 주셨다는데.. 역시 아직은 좀 ‘많이’ 어렵다. 이번 주 논문 난이도 정도만 돼도 읽을만 할 듯!! 그 외 팀원들끼리 친해지고 익숙해져서 점점 얘기도 많이하고 즐거운 시간들을 보내고 있다. 가장 좋게 생각하는 것은 의견을 많이 낸다는 것! 이렇게 또 성장하는 것 같다. 학습회고Feelings (느낌, 주관) 여전히 처음부터 백지상태 코딩을 하라그러면 못할 것 같다. 너무 많은 대량의 코드를 구조적으로 인식하고 있어야 하며, 각 구조 속에 어떤 코드가 들어가야 할 지 인지하고 있어야 하는데 아직 그 정도 레벨까진 못되는 것 같다. 좋은 코드를 더 많이 보고 기억하고 작성하려는 노력을 해야할 것 같다. Affimation (자기선언) 이번 주차에는 대부분의 시간을 강의를 보고 정리하는 시간보다는 공식문서를 읽고 과제를 해결하는데 시간을 썼던 것 같다. 강의 내용이 겉핥기 수준이었던 반면 과제 자체는 위키독스 수준으로 내용이 많았기 때문에 과제만으로 어느정도 만족할만한 학습시간을 쏟은 것 같긴 하다. 하지만 그럼에도 내실이 살짝 부족한 느낌이 계속 드는데.. 다음 주에는 조금 더 많이 생각하고 많이 질문하며 최대한 체득할 수 있었으면 좋겠다. 내가 따로 노력한 것들 이번 주는 없음. 총평 저번 주 회고의 선언들이 대부분 지켜진 것 같아서 만족스럽다! 다음 주 역시 이번주의 선언을 토대로 개선된 학습을 진행할 수 있었으면 좋겠다. 본격적으로 파이토치를 다루면서 파이토치 내부 클래스 및 함수들에 대한 공부를 많이 했는데, 여전히 차원을 다루고 원하는대로 슬라이싱하고 쪼개고 뽀개고 합치고 하는 걸 잘 못하겠다. 시간이 해결해주려나..? 취준에 대한 시간을 평일에는 따로 투자하지 못했는데, 주말에는 꼭 해보자!!","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-3%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-17-2021-08-20/"},{"title":"부스트캠프 AI Tech 4주차 회고 (2021-08-23 ~ 2021-08-27)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 학습현황 21.08.23 월 (09:40 ~ 20:20) 21.08.24 화 (10:30 ~ 23:59) 21.08.25 수 (07:00 ~ 23:59) 21.08.26 목 (09:50 ~ 23:59) 21.08.27 금 (10:00 ~ 23:59) 진도 이미지 분류 대회를 위한 EDA - Dataset - Data Generation - Model - Training - Inference - Ensemble - Experiment Toolkits 이번 주의 개선할 사항 및 자기성찰 협업을 위한 코드에서 고려하지 못한 점들이 너무 많이 있었다. 내가 아는 것을 다른 사람도 알 거라는 착각 아 이걸로 되겠지 하면서 push 했다가 오류 발생하는 지점들 예외처리 고려 여부 Package fix TODO List 공유 x 수정사항 및 기능 설명이 우선시 되었어야 하는데 하지 않았음. 앞으로 팀 전체 코드를 위해서는 상대방의 아무것도 없는 환경 상태에서 바로 돌아갈 수 있도록 더 많은 신경을 써서 코드를 작성해야겠다. 피어세션프로젝트 관리 Github에서 Kanban 기반의 Project Dashboard를 도입하였다. 깃허브 프로젝트 이외의 Google Docs를 활용하여 회의록을 기록하고 실험결과를 공유하는데 신경써야 할 채널이 너무 많아져서 헷갈린다. P Stage 이전에 특히 우리조가 하는 것이 많았어서 어떤 것을 선택하고 집중해야 할 지 잘 몰랐기 때문에 대회 초반에는 어수선했는데 조금은 안정이 된 것 같다. 그 외 나는 베이스라인 코드를 작성하는 것에 집중하였다. Pytorch-Template을 사용하기로 했었기 때문에 해당 템플릿에 우리 과제를 적용하려고 많은 노력을 했다. 여러 실험을 하고 싶었지만 깃에 익숙하지 않거나 코드를 실행할 줄 모르는 팀원들이 있었기에 같은 환경을 조성하고 서로가 많은 실험을 할 수 있도록 코드를 공유하고 알려주는 것에 많은 시간을 할애했다. 또한 논문리뷰 정리를 위한 Git Pages 를 만들었고 설명과 함께 공유하였으나 이번주는 너무 바빠서 아무도 진행하지 못했다. 원래 우리 조가 하는 것이 많았고 협업에 익숙하지 않은 팀원들이 대다수였기 때문에 진행했어야 하는 부분들은 책임을 지고 내가 수행했어야 했던 것이 좀 아쉬운 것 같다. 깃 페이지와 베이스라인 작성을 하기 위해 이번주에는 거의 아무것도 진행하지 못했다. ㅠㅠ 다음주에는 좀 많은 실험들을 할 수 있었으면 좋겠다. 총평 P Stage가 되면서 많이 정신없는 것이 느껴진다. 이번주에만 학습시작 누르는 것을 두 번이나 지각하고 연락오는 것들도 다 못받고 대회에만 집중했기 때문이다. 잘 하고 있는 거겠지..? 내가 팀에는 많은 기여를 한 것 같지만 부족한 점이 너무 많이 느껴진다. 더 좋은 코드, 환경을 조성하기 위해 더 열심히 살아야겠다. 서버좀 그만 다운됐으면 좋겠다. 벌써 두번째라 작업한게 다 날아가서 의욕이 많이 떨어진다. 하..","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-4%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-08-27/"},{"title":"부스트캠프 AI Tech 6주차 회고 (2021-09-06 ~ 2021-09-10)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 학습현황 21.09.06 월 (05:30 ~ 20:15) 21.09.07 화 (05:20 ~ 20:50) 21.09.08 수 (06:00 ~ 22:00) 21.09.09 목 (08:28 ~ 23:59) 21.09.10 금 (00:00 ~ 03:00, 09:58 ~ 21:50) 진도 Intro to NLP, Bag of Words - Word Embedding - Recurrent Neural Network - LSTM and GRU - Sequence to Sequence - Beam Search and BLEU score 이번 주의 개선할 사항 및 자기성찰 이력서는 미리미리 자소서는 미리미리 과제 수행과정과 결과물 4개의 필수과제와 1개의 선택과제 분류 해결 여부 or 결과 필수과제 1 - Data Preprocessing O 필수과제 2 - RNN-based Language model O 필수과제 3 - Subword-level Language model O 필수과제 4 - Preprocessing for NMT model O 선택과제 1 - BERT Fine-tuning with transformers O Facts (사실, 객관) 대략적으로만 알던 RNN, LSTM, GRU를 깊게 살펴볼 수 있었음. 수식을 통해 어떤 식의 shifting 이 일어나는지 등 수식의 의미를 조금 더 곱씹어볼 수 있었음 Feelings (느낌, 주관) 전반적으로 나름 익숙한 것들이었기에 강의에서 완전 새롭게 배운다 하는 것은 없었지만, 그게 아니었다면 강의가 너무 어려웠을 것 같다. 사실 흘려들은 내용들도 많기 때문에 다시 한 번 들을 수 있을 때 복습해보는 것이 좋을 것 같다. Affimation (자기선언) 사실 이번주는 이력서쓰느라 너무 바빴기 때문에 학습에 많은 몰입을 할 수 없었던 것 같다. 다음주에는 다음 주 내용을 공부하는 것도 좋지만 이번 주 내용을 꼭 복습해봐야할 것 같다. 피어세션새로운 조라서 그런지 적응할 시간이 필요하다. 물론 이력서에 자소서를 준비하느라 완전 몰입하여 참여하지 못한 것에 대한 아쉬움이 크게 남는다. 질의응답 팀에 선생님이 한 분 계셔서 든든하다. 퀄리티 좋은 답변을 위해 나도 이것저것 찾아보기 때문에 하나의 질문을 커버하는 것에도 시간이 좀 걸린다. 미리 많은 것을 궁금해하고 준비하여 좀 빠르게 여러 질문들에 대응이 가능해졌으면 좋겠다. 멘토링 첫 멘토링이라서 자기소개 정도만 하고 수다하는 시간을 가졌다. 딱 주마다 한 번씩, 한 시간씩만 보게 되는 것 같은데 멘토님께 조금 더 많은 도움을 얻었으면 좋겠다. 예닮님 보고싶다. 학습회고Affimation (자기선언) 이번주 너무 공부하지 못한 시간이 많았는데 다음주부터는 정말 공부 열심히 하자 ㅠㅠ 총평 많은 시간을 학습에 투자하지 못했기 때문에 회고할 내용도 크게 없는 것 같다.. 다음주부터는 진짜 빡세게 할 것!! 실습이나 과제도 무난무난했고 특별히 어려운 것은 없었던 것 같다. 하지만 강의가 너무 어려웠다. 제대로 공부하고 싶다. 앎에 있어서 허전한 느낌을 받고 싶지 않다.","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-6%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-06-2021-09-10/"},{"title":"부스트캠프 AI Tech 7주차 회고 (2021-09-13 ~ 2021-09-17)","text":"강의내용 강의정리 및 강의노트는 저작권상 이유로 공개하지 않습니다. 학습현황 21.09.13 월 (09:30 ~ 20:15) 21.09.14 화 (09:20 ~ 20:50) 21.09.15 수 (09:40 ~ 23:59) 21.09.16 목 (09:58 ~ 23:59) 21.09.17 금 (10:00 ~ 22:50) 진도 Transformer - Self-supervised Pre-training Models - Advanced Self-supervised Pre-training Models 이번 주의 개선할 사항 및 자기성찰 멘탈 트레이닝을 잘 해야겠다. 부캠을 떠나서 그냥 이번 주에는 감정적인 대응도 많이 하고, 어떤 일의 결과들도 쉽게 승복하지 못했던 것 같다. 과제 수행과정과 결과물 2개의 선택과제 분류 해결 여부 or 결과 선택과제 2 - NMT Training with Fairseq O 선택과제 3 - Byte Pair Encoding O Facts (사실, 객관) Fairseq의 사용방법을 배울 수 있었다. Byte Pair Encoding 과정을 논문을 보면서 이해할 수 있었다. Feelings (느낌, 주관) 과제를 왜 이렇게 냈을까라는 생각을 참 많이 했다. 억지스러운 테스트케이스와 어줍잖은 Fairseq 사용은 참 많이 아쉬웠다. Affimation (자기선언) 공부해야할 양이 정말 많았는데, 당장에 하는 것들보다는 앞서 해야할 것들에 우선순위를 부여하고 여유롭게 실행해보자. 피어세션나름 질의응답에도 많이 참여하려고 하고, 저번주보다는 조금 친밀한 느낌을 받으면서 세션을 진행할 수 있었다. 멘토링 NLP에서의 EDA를 소개해주셨는데, 꽤 유용한 것들을 알려주셔서 많은 도움을 받을 수 있었다. 학습회고Affimation (자기선언) 그동안 게을리하던 논문읽기를 그래도 좀 한 것 같다. 대략적인 내용들을 파악한 뒤 읽으니까 생각보다 빠르게 읽을 수 있었던 것 같다. 그래도 아직 꼼꼼하게 하나하나의 의미를 전부 따져가면서 읽는 것은 좀 어렵지만, 점점 익숙해지면 더 나아질 것 같다. 총평 새로운 팀을 구했는데, 방향성도 일치하고 한분한분 너무 실력있고 좋은 분들이라 기대가 많이 된다. 여러 학습을 진행했는데 정작 정리가 된 것은 하나도 없다. 반성하자.","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-7%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-13-2021-09-17/"},{"title":"부스트캠프 AI Tech 9주차 회고 (2021-09-27 ~ 2021-10-01)","text":"P Stage 강의 커리큘럼 인공지능과 자연어 처리 자연어의 전처리 BERT 언어모델 소개 한국어 BERT 언어 모델 학습 BERT 기반 단일 문장 분류 모델 학습 BERT 기반 두 문장 관계 분류 모델 학습 BERT 언어모델 기반의 문장 토큰 분류 대회관련협업전략, 툴협업툴 협업툴은 Notion을 메인으로 사용한다. 여러 툴에서 파편적인 기능들만을 선별적으로 사용할 수도 있지만, 데드라인이 주어진 대회라는 플랫폼 하에서는 채널이 다양할 수록 혼란만 가중되는 것 같다. Notion에서 여러 유용한 Template을 하나하나 가져와서 꽤 괜찮은 Dashboard를 만들고, 코드관리를 제외한 모든 것을 Notion으로 해결하고 있다. 기록의 중요성에 공감을 많이 하여 모인 팀 답게, Notion을 적극적으로 사용하고 있고 그로인해 체계적인 실험과 작업관리가 가능해진 것 같다. 또한 Wandb를 팀단위로 만들어서 각자 한 실험에 대한 log를 시각화하여 볼 수 있게 되었다. good practice인듯!! 협업전략 실험전략은 개인이 하고싶은 것 위주로 플래닝을 진행하며, 작은 모델을 Base로 하여 빠른 실험이 가능하도록 했다. 또한 실험을 통한 성능비교를 위해 hyperparameter나 seed등 변수가 될 수 있는 모든 것들을 fix하여 진행한다. 코드의 경우 본인의 실험에 따라 각자의 코드가 전부 달라지며, 그것을 하나로 통합하면서 진행하는 것은 대회에서는 굉장히 비효율적인 방식인 것 같다. 한 번 진행해보고 PR이나 Merge에 병목이 생기는 것을 경험한 뒤로, 언제든 그때그때의 실험으로 Rollback이 가능하도록 branch를 여러개로 나누되, 합치진 않기로 결정했다. NLP AugmentationNLP에서 Text Augmentation 방법은 크게 텍스트의 일부를 변형하여 데이터를 증강하는 방법과 생성모델을 사용하여 새로운 텍스트를 생성하여 데이터를 증강하는 방법이 있다. 그 중에서 가장 손쉽게 접근할 수 있는 방법은 KoEDA 라이브러리를 사용하는 것이었다. KoEDA는 EDA와 AEDA 논문에서 소개된 방식을 한국어 Wordnet 으로 Porting하여 공개한 오픈소스 라이브러리이다. EDA의 경우 SR, RI, RS, RD 네 가지 Operation을 제공하며, 한 문장에 대해서 몇 개의 문장을 만들건지에 따라 $\\alpha$ 값에 조정이 필요하며, 4문장 이하는 $p=0.1$, 4문장 초과는 $p=0.05$ 정도의 확률값으로 데이터를 변형하는게 가장 성능이 좋았다고 저술되어있다. 하지만 텍스트 데이터의 특성상, 위치를 바꾸거나 일부 단어를 제거하는 것은 결국 본 문장의 의미를 손실시키는 행위이기 때문에 AEDA 방법론이 등장하게 된다. AEDA는 문장을 손실시키지 않게 하기 위해 Special character를 문장 곳곳에 배치하는 방법론으로, 역시 많은 특수문자가 들어가게 되면 성능이 떨어지기 때문에 적절한 확률값을 찾는 것이 중요하다. KoEDA의 Snippeteda.py12345678910111213augmenter = EDA( morpheme_analyzer: str = None, # Default = &quot;Okt&quot; alpha_sr: float = 0.1, alpha_ri: float = 0.1, alpha_rs: float = 0.1, prob_rd: float = 0.1 )result = augmenter( data: Union[List[str], str], p: List[float] = None, # Default = (0.1, 0.1, 0.1, 0.1) repetition: int = 1 ) aeda.py1234567891011augmenter = AEDA( morpheme_analyzer: str = None, # Default = &quot;Okt&quot; punc_ratio: float = 0.3, punctuations: List[str] = None # default = ('.', ',', '!', '?', ';', ':') )result = augmenter( data: Union[List[str], str], p: float = None, # Default = 0.3 repetition: int = 1 ) 하지만 EDA, AEDA 모두 KLUE-RE Task에선 별 효과가 없는 것 같다.. 다음 방법은 생성모델을 활용한 방법으로 Conditional BERT Contextual Augmentation 논문에 소개되었다. 기존 BERT에서는 token embedding + segment embedding + positional embedding 으로 representation을 구성하지만, conditional BERT의 경우 token embedding + label embedding + positional embedding 으로 representation을 구성하고, label을 부착한 상태로 데이터셋을 MLM task로 pretraining한다. 이후에 mask token을 replace하는 것과 마찬가지로 label에 대하여 token replace를 수행한다. 논문을 자세히 읽어보진 않았지만, 충분히 시도해볼만한 가치가 있는 것 같긴 하다. 현재 augmentation에 대한 효과를 전혀 보지 못했기 때문에 이번 실험이 끝나면 해봐야지… Huggingface살면서 huggingface 공식문서를 가장 많이 읽은 한 주였다. Trainer와 Tokenizer는 거의 다 읽어는 봤는데, 기능이 너무 많아서 두고두고 찾아보면서 개발해야할 것 같다. 그 중에서 인상깊은 기능들은 아래와 같다. hyperpameter_search of Trainer class (Huggingface, transformers)KLUE RE(Relation Extraction) 과제를 하면서 모델을 finetuning 해보다가 hyperparameter가 생각보다 성능에 많은 영향을 준다는 것을 알게 되었다. 그에 따라 huggingface transformers의 Trainer 클래스에서 제공하는 hyperparameter_search 를 사용하여 최적의 hyperparameter를 찾아보는 실험을 하게 되면서 사용법을 알 필요가 있었다. trainer의 hyperparameter_search method를 보시면, 하이퍼파라미터를 찾을 때 optuna , Ray Tune, SigOpt 세 친구들을 통해 hyperparameter를 탐색할 수 있다. huggingface의 trainer가 train 시 wandb와 연동되는 메커니즘과 비슷하다고 이해하면 될듯. Snippet hp_search.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from transformers import AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, Trainer, TrainingArguments# TODO: load your tokenizer &amp; dataset# tokenizer = ...# dataset = ...# TODO: change your pretrained model pathconfig = AutoConfig.from_pretrained(&quot;YOUR_MODEL_PATH&quot;)def model_init(): return AutoModelForSequenceClassification.from_pretrained( model_path, config=config)# TODO: fill it your training argumentstraining_args = TrainingArguments(...)# TODO: fill it your trainer argumentstrainer = Trainer( model_init=model_init, args=training_args, ...)# NOTE: optunadef optuna_hp_space(trial): return { &quot;learning_rate&quot;: trial.suggest_float(&quot;learning_rate&quot;, 5e-6, 5e-4, log=True), &quot;num_train_epochs&quot;: trial.suggest_int(&quot;num_train_epochs&quot;, 1, 5), &quot;seed&quot;: trial.suggest_int(&quot;seed&quot;, 1, 42), }# NOTE: ray tunedef ray_hp_space(): from ray import tune return { &quot;learning_rate&quot;: tune.loguniform(5e-6, 5e-4), &quot;num_train_epochs&quot;: tune.choice(range(1, 6)), &quot;seed&quot;: tune.choice(range(1, 42)), }trainer.hyperparameter_search( direction=&quot;maximize&quot;, # NOTE: or direction=&quot;minimize&quot; hp_space=ray_hp_space, # NOTE: if you wanna use optuna, change it to optuna_hp_space backend=&quot;ray&quot;, # NOTE: if you wanna use optuna, remove this argument) 사용법은 생각보다 간단하다. hp_space 함수에서 원하는 하이퍼파라미터와 그 하이퍼파라미터의 범위를 key, value 형태로 return 해주면 해당 dictionary를 토대로 실험이 진행된다. 위에서는 3개의 하이퍼파라미터만 작성되어 있지만 warmup_steps, weight_decay, per_device_train_batch_size 등을 동일한 방식으로 추가할 수 있고, training argument에 들어가는 대부분의 hyperparameter들을 탐색할 수 있는 것 같다. 하지만 위에서 사용하는 optuna 혹은 raytune이 hyperparameter를 탐색하는 알고리즘도 다르고, CLIReporter, Pruner, Scheduler 또한 전부 다르기 때문에 해당 라이브러리에 대한 이해가 필요한 것 같다. backend (str or HPSearchBackend, optional) – The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending on which one is installed. If all are installed, will default to optuna. default 는 optuna 로 설정되어 있고, 자신의 환경에 설치되어 있는 걸 우선적으로 사용하도록 되어있다. 하이퍼파라미터를 search하는 direction을 maximize 또는 minimize로 설정할 수 있는데, 이 때 사용자의 입맛대로 목적함수를 작성하여 사용할 수도 있다. 12345678def my_objective(metrics): # Your elaborate computation here return result_to_optimizetrainer.hyperparameter_search( direction=&quot;maximize&quot;, compute_objective=my_objective) custom object function의 경우 공식문서나 discussion 참고할 것. Ray https://docs.ray.io/en/latest/tune/index.html optuna https://optuna.org/#code_examples https://discuss.huggingface.co/t/using-hyperparameter-search-in-trainer/785/10 Focal Loss다음은 Focal Loss function인데, 이번 대회에서 효과를 톡톡히 보았다. Focal Loss란 RetinaNet 논문에서 처음 제안되었는데, Class Imabalance를 해결하기 위한 목적함수이다. 쉽게 말하면 예측하기 쉬운 값에 대해서는 0에 가까운 loss값을 부여하고, 예측하기 어려운 negative sample에 대해서는 기존보다 높은 loss값을 부여하여 weighted scale의 형태로 표현할 수 있게 만든 손실함수이다. 이것 역시 나중에 꼭 본 논문을 읽어보도록 하자…. Reference [딥러닝 기계 번역] Seq2Seq: Sequence to Sequence Learning with Neural Networks (꼼꼼한 딥러닝 논문 리뷰와 코드 실습) Paper Reivew - FastText: Enriching Word Vectors with Subword Information 한국어 자연어처리 1편_서브워드 구축(Subword Tokenizer, Mecab, huggingface VS SentencePiece) huggingface transformers의 attention mask huggingface tokenizer로 konlpy 사용하기 BERT 톺아보기 LM Training from scratch 나만의 BERT Wordpiece Vocab 만들기 나만의 언어모델 만들기 - Wordpiece Tokenizer 만들기 BERT만 잘 써먹어도 최고가 될 수 있다","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-9%EC%A3%BC%EC%B0%A8-%ED%9A%8C%EA%B3%A0-2021-09-27-2021-10-01/"},{"title":"부스트캠프 AI Tech Pstage1 회고 (2021-08-23 ~ 2021-09-04)","text":"🔥 Final Score🥉Public Leaderboard Acc: 81.508 F1 (macro): 0.766 (11등) 🥉Private Leaderboard Acc: 80.746 F1 (macro): 0.754 (13등 - 2위 하락) 📝 Task DescriptionCOVID-19의 확산으로 우리나라는 물론 전 세계 사람들은 경제적, 생산적인 활동에 많은 제약을 가지게 되었습니다. 우리나라는 COVID-19 확산 방지를 위해 사회적 거리 두기를 단계적으로 시행하는 등의 많은 노력을 하고 있습니다. 과거 높은 사망률을 가진 사스(SARS)나 에볼라(Ebola)와는 달리 COVID-19의 치사율은 오히려 비교적 낮은 편에 속합니다. 그럼에도 불구하고, 이렇게 오랜 기간 동안 우리를 괴롭히고 있는 근본적인 이유는 바로 COVID-19의 강력한 전염력 때문입니다. 감염자의 입, 호흡기로부터 나오는 비말, 침 등으로 인해 다른 사람에게 쉽게 전파가 될 수 있기 때문에 감염 확산 방지를 위해 무엇보다 중요한 것은 모든 사람이 마스크로 코와 입을 가려서 혹시 모를 감염자로부터의 전파 경로를 원천 차단하는 것입니다. 이를 위해 공공 장소에 있는 사람들은 반드시 마스크를 착용해야 할 필요가 있으며, 무엇 보다도 코와 입을 완전히 가릴 수 있도록 올바르게 착용하는 것이 중요합니다. 하지만 넓은 공공장소에서 모든 사람들의 올바른 마스크 착용 상태를 검사하기 위해서는 추가적인 인적자원이 필요할 것입니다. 따라서, 우리는 카메라로 비춰진 사람 얼굴 이미지 만으로 이 사람이 마스크를 쓰고 있는지, 쓰지 않았는지, 정확히 쓴 것이 맞는지 자동으로 가려낼 수 있는 시스템이 필요합니다. 이 시스템이 공공장소 입구에 갖춰져 있다면 적은 인적자원으로도 충분히 검사가 가능할 것입니다. 😷 Dataset마스크를 착용하는 건 COIVD-19의 확산을 방지하는데 중요한 역할을 합니다. 제공되는 이 데이터셋은 사람이 마스크를 착용하였는지 판별하는 모델을 학습할 수 있게 해줍니다. 모든 데이터셋은 아시아인 남녀로 구성되어 있고 나이는 20대부터 70대까지 다양하게 분포하고 있습니다. 간략한 통계는 다음과 같습니다. 전체 사람 명 수 : 4,500 한 사람당 사진의 개수: 7 [마스크 착용 5장, 이상하게 착용(코스크, 턱스크) 1장, 미착용 1장] 이미지 크기: (384, 512) 🔎 Class Description 아이콘 제작자 Freepik from www.flaticon.com Label Mask Gender Age 0 Wear Male &lt; 30 1 Wear Male &gt;= 30 and &lt; 60 2 Wear Male &gt;= 60 3 Wear Female &lt; 30 4 Wear Female &gt;= 30 and &lt; 60 5 Wear Female &gt;= 60 6 Incorrect Male &lt; 30 7 Incorrect Male &gt;= 30 and &lt; 60 8 Incorrect Male &gt;= 60 9 Incorrect Female &lt; 30 10 Incorrect Female &gt;= 30 and &lt; 60 11 Incorrect Female &gt;= 60 12 Not Wear Male &lt; 30 13 Not Wear Male &gt;= 30 and &lt; 60 14 Not Wear Male &gt;= 60 15 Not Wear Female &lt; 30 16 Not Wear Female &gt;= 30 and &lt; 60 17 Not Wear Female &gt;= 60 회고록Naver Connect 재단에서 운영하는 Boostcamp AI Tech의 첫 번째 대회인 마스크 착용 분류 대회에 참여하게 되었다. 나는 이번이 세 번째 대회 참가인데, 지난 두 번의 대회에서는 그저 참여에만 의의를 두었었기 때문에 이번 대회에서는 그간 공부하고 성장한 것을 조금이나마 입증하는 겸, 무조건 순위권에 들어야겠다는 목표를 가지고 대회에 임하게 되었다. 결과적으로는 38팀 중 13위에 그쳤고, 처음 목표에 비해서는 너무나도 아쉬운 결과지만 지난 두 번의 대회에 비하면 매우 주도적으로 대회에 임했고 많은 코드를 작성하고 실험을 하며 어느정도 인공지능 대회라는 프로세스에 익숙해질 수 있었다는 것에서 나름 만족스러운 대회였던 것 같다. Timetable (21.08.23 ~ 21.09.02) 08.23 ~ 08.24: EDA 및 Data Labeling 08.25 ~ 08.26: Baseline 코드 작성 08.27 ~ 08.30: Augmentation 적용 및 실험 08.31 ~ 09.01: 개별모델 분리 및 앙상블 09.02: 외부데이터셋 추가 및 최종제출 대회 전략이번 대회는 2주동안 팀 단위로 진행되었는데, 1주차는 개인제출, 2주차는 팀제출로 진행되었다. 팀원들이 모두들 순위에 욕심이 있었기 때문에 1주차에는 데이터 EDA에 집중하며 노이즈나 Labeling이 잘못된 데이터를 최대한 발견하고 수정하는 쪽으로 진행했으며, 2주차에는 최대한 성능을 끌어올리는 식으로 계획을 가져갔다. 하지만 깃과 협업문화에 익숙하지 않은 팀원들이 있었고, 내 서버가 계속 죽는 불상사가 있었기 때문에 Github를 이용한 프로젝트 관리에 어려움이 있었고 더 나은 환경에서 서로의 작업을 상세하게 공유하는 것이 생각보다 쉽지 않았다. 결국에는 전략이랄 것도 없이 그저 각자의 환경에서 서로다른 실험을 계속하는 것밖엔 이루어지지 않았는데 이것이 가장 큰 아쉬움으로 남는 것 같다. 데이터 분석과 EDA데이터에는 예상대로 노이즈가 몇 군데 있었다. 애초에 잘 정제되어있는 데이터였기 때문에 많지는 않았으나 남자를 여자로, 여자를 남자로 적어놓은 데이터가 있는가 하면, 마스크를 끼고 있는데 파일이름에는 마스크를 착용하지 않았다고 나와있는 것들이 존재했다. 다행히 AIStage의 토론게시판과 팀원들의 도움으로 모든 노이즈를 찾아서 제거해줄 수 있었다. EDA 과정에서도 토론게시판과 팀원들의 도움을 많이 받았다. 특히 데이터가 잘 정제되어 있다는 것을 해당 단계에서 알아차릴 수 있었으며 마스크를 잘못 착용한 데이터에는 턱스크는 물론 마스크를 눈에 착용한다던지 하는 괴상한 사진도 있었다. 가장 좋은 분석 중 하나는 데이터의 분포를 시각화로 알아낼 수 있었다는 것인데, 60세 이상 데이터는 60세만 존재하였고 해당 데이터가 가장 적은 비율로 존재했기 때문에 ‘나이’를 제대로 예측하는 것이 가장 중요하겠다는 판단을 할 수 있었다. 지금까지는 Visualization에 대한 일종의 편견이 있었기 때문에 오히려 배우려고 들지 않았던 것 같은데, 이번 대회를 통해서 그 중요성을 인지할 수 있었던 것 같다. Seaborn과 Matplotlib을 잘 사용하는 것이 실무에서도 굉장한 도움을 얻을 수 있을 것이라고 생각된다. 베이스라인 코드 및 협업환경 구성팀원들과 베이스라인 코드로 U Stage 에서 배웠던 파이토치 템플릿을 사용하기로 했다. 워낙 우리 조에서 하는 것이 많다 보니 다들 파이토치 템플릿에 익숙하지가 않았기 때문에 간단하게 바로 실험을 돌려볼 수 있는 베이스라인 코드를 잘 작성하는 것이 생각보다 매우 어려웠다. 구조적으로 잘 짜여있고 모듈화가 잘 되어있기 때문에 파이토치 템플릿을 사용하기로 했었는데, 지금 생각해보면 이것도 더 좋은 성능을 내지 못한 이유 중 하나일 것으로 생각된다. 그렇게 생각하는 원인은 아래와 같다. 협업을 위한 환경 Fix가 어렵다. 모델에 Config 파일 정보를 같이 저장하기 때문에 Config파일 없이 모델을 불러올 수가 없다. Remote 환경으로 작업하기 때문에 CLI에 익숙하지 않으면 원하는 대로 파이썬 스크립트를 실행해보기가 어렵다. 모듈화가 잘 되어있지만 의존성을 가지는 함수들이 있기 때문에 구조를 제대로 파악하지 않으면 원하는대로 커스텀하기가 어렵다. 대부분 대회에서는 빠른 커스터마이징과 실험을 통해 계속해서 성능을 올리는 것을 목표로 한다. 하지만 우리 조원들중에는 CLI와 Git을 처음 써보는 팀원도 있었고 파이썬 코드에 익숙하지 않은 팀원도 있었기 때문에 파이토치 템플릿같은 구조를 완벽하게 이해하지 못해서 내가 만든 베이스라인을 팀원들이 이해하고 실행하는데 꽤 오랜 시간을 소비하게 되었다. Dot ENV 혹은 이미지 경로 등 MetaData와 환경을 고정할 수 있는 요소들을 미리 감안하지 않은 것, 그리고 애초에 쉽게 구조를 파악할 수 없는 템플릿을 사용하여 많은 실험을 못하게 된 것을 다음 협업때부터는 고려할 수 있도록 해보아야 할 것 같다. 또한 Python 스크립트도 좋지만 Jupyter 환경에서 돌아가는 코드를 작성하는 것도 매우 좋을 것 같다..!= 모델링과 실험이번 P Stage에서 협업을 제외하고 가장 큰 아쉬움으로 다가오는 것은 실험일지를 체계적으로 정리하지 않았고, 파일관리를 제대로 하지 않았다는 것이다. 총 100기가의 저장공간 중 실제 사용할 수 있는 용량은 약 6-70 기가정도였는데 Epochs 단위마다 모델을 저장해놓았기 때문에 계속해서 서버용량에 제약을 받게 되었다. 그리고 점점 끝이 다가올수록 성능향상도 안되고 마음도 급해져서 잘 확인하지도 않고 파일을 삭제하는 것을 반복했었는데, 그러다가 최고 성능을 낸 모델까지 지워버리는 참사가 발생했다. 최종 제출 직전에 확인하고 현재 최고 스코어가 재현이 안된다는 것을 알았을 때 정말, 그 좌절감은 이루말할 수 없다. 그리고 기존 실험하고 있던 모델과 하이퍼파라미터를 픽스하고 여러가지 개선방안 중에서 하나씩 적용해보면서 성능이 오른 것을 취하는 전략이 가장 좋았을 것 같은데, 그렇게 하지 않았기 때문에 분명히 성능이 오를 요소들을 제대로 포착하지 못했다. 예를들어 Augmentation이 적용된 이미지들을 만들고 거기에 Augmix 등 여러 Augmentaiton을 Soft하게 적용하여 실험했는데 성능이 떨어졌다. 이 때 데이터를 추가해서 성능이 떨어진건지, 아니면 다른 Augmentation 때문에 성능이 떨어진 건지 알지 못했다. 실험일지를 잘 작성하고 기존에서 성능을 올리는 쪽으로만 전략을 취할 수 있도록 다음 대회부터는 일지를 작성해봐야할 것 같다. 또한 위의 이야기와 더불어 실험을 했던 코드를 체계적으로 정리했으면 어땠을까 싶다. 모듈화가 잘 되어 있는 템플릿을 사용하면서 여러 실험을 할 때는 대부분 하드코딩 아니면 모듈화를 시키지 않은 채 코드를 작성했기 때문에 실험환경을 바꿀 때마다 피로도가 굉장히 컸다. 그리고 Wandb와 Tensorboard 등 시각화 도구들을 사용할 때 Runtime 에러가 발생했을 때도 모든 로그를 기록하게 해서 서버 용량이 굉장히 빠르게 차올랐는데, 이 부분 역시 쉽게 적용여부를 결정할 수 있도록 만들었으면 더 좋았을 것 같다. 총평근본없이 너무 무지성 실험을 반복했던 것 같다. 결국 그렇게 하다가 성능이 오르긴 했었지만, 그렇게 성능이 올랐을 때의 환경을 픽스하지 못하고 다른 추가적인 것들을 적용하다 보니 최종스코어를 재현할 수도 없고 오히려 성능이 떨어지는 상황이 수도없이 반복되었다. 또한 계속 협업의 중요성을 말하고 다녔지만 결국엔 나부터도 버전관리와 공유에 소홀했다. 다시금 그 중요성을 생각하면서 같은 실수는 반복하지 않도록 노력해야겠다.","link":"/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-AI-Tech-Pstage1-%ED%9A%8C%EA%B3%A0-2021-08-23-2021-09-04/"},{"title":"소켓과 select를 이용한 멀티플렉싱 서버 구축하기","text":"1. 소켓 이해하기전화를 걸기 위해서는 ‘단연코’ 전화기가 필요하다. 전화기는 멀리 떨어져 있는 두 사람이 대화할 수 있도록 만들어주는 매개체이다. 소켓은 전화기처럼 멀리 떨어져 있는 두 개의 호스트(host)를 연결해주는 매개체 역할을 한다. 1234#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol); 우리는 누군가에게 걸려오는 전화를 받게 된다. 그리고 전화를 받으면 ‘여보세요’라는 응답을 주게 된다. 그렇다면, 전화기(소켓)을 만든 이후에 무엇이 더 필요할까? 당연히 전화번호가 필요하다. 그래야 누가 나한테 전화를 할 수 있기 때문이다. 그렇다면, 전화번호를 우리의 전화기에 할당시켜야 하는 것처럼, 소켓에도 일종의 ‘전화번호’가 필요하다. 소켓에서 전화번호의 역할을 하는 것은 바로 IP 주소이며, bind 함수를 이용하여 소켓에 주소를 할당할 수 있다. 123#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *myaddr, int addrlen); 이제 전화를 받을 준비가 끝났다. 하지만, 전화가 케이블에 연결되어 있지 않으면 소용이 없다. 즉, 전화선을 연결시키고 전화를 받을 상태를 만들어주어야 한다. 마찬가지로 소켓도 연결 가능한 상태가 되도록 대기를 시켜주어야 하는데, listen 함수를 이용하여 연결 요청이 가능하도록 만들어줄 수 있다. 123#include &lt;sys/socket.h&gt;int listen(int sockfd, int backlog); 자, 이제 전화벨이 울린다. 통화를 하기 위해서는 통화버튼을 눌러야 한다. 즉 누군가 대화를 요청했을 때 이것을 수락해야 한다는 뜻인데 소켓 역시 누군가가 데이터를 주고받기 위해 대화(연결)를 요청했을 때 그 요청을 수락할 수 있어야 한다. 이것을 accept 함수가 수행하게 된다. 123#include &lt;sys/socket.h&gt;int accept(int sockfd, struct sockaddr *addr, int *addrlen); 지금까지의 내용을 정리해보면, 네트워크 프로그래밍의 기본 과정은 전화기를 구한다. -&gt; socket() 전화번호를 할당한다. -&gt; bind() 전화를 케이블에 연결한다. -&gt; listen() 전화를 수락한다. -&gt; accept()위 과정을 수행하게 된다. 그리고 이 과정을 거친 프로그램이 바로 ‘서버’ 프로그램이 되는 것이다. 자 그러면 이제 전화를 받을 수 있는 상황은 다 만들어졌다. 이제 남은 것은 전화를 거는 사람!을 구하는 것이다. 네트워크의 개념 속에서 이 사람은 ‘클라이언트’가 되고 클라이언트 역시 위와 비슷한 과정을 거쳐야 한다. 하지만 여기서 달라지는 것은 서버에게 먼저 전화를 걸어야 한다는 것. 이 기능을 하는 함수가 connect 이다. 클라이언트 소켓 구현 전화를 걸기 위한 전화기를 구한다. socket() 전화를 건다. (connect) 연결이 완료되면 데이터를 주고받는다. (read) 자 지금까지의 내용을 전부 이해했다면 의아한 부분이 있을 것이다. 바로 전화를 걸기위한 ‘전화번호’를 부여받지 않는다는 것이다. 이 궁금증은 추후에 풀어갈 것이다. 파일 조작하기 리눅스에서는 모든 것(파일, 소켓, 콘솔)을 ‘파일’로 간주한다. 즉 파일 입・출력 함수를 소켓의 입・출력에서 사용할 수 있다. 42 과정을 잘 진행했다면, 파일 입・출력에 관한 것은 도사가 되어있을 것이니 파일 디스크립터에 대한 내용은 생략하도록 한다! File Open Mode Mode 의미 O_CREATE 필요한 경우 파일을 생성함 O_TRUNC 존재하던 데이터를 모두 삭제 O_APPEND 존재하던 데이터를 보존하고 뒤에 이어서 저장 O_RDONLY 읽기 전용 모드로 파일을 연다. O_WRONLY 쓰기 전용 모드로 파일을 연다. O_RDWR 읽기 쓰기 전용 모드로 파일을 연다. socket() 으로 소켓을 생성하면 소켓 파일 디스크립터가 생성된다. 소켓 파일 디스크립터 역시 3번부터 넘버링된다. 2. 소켓 생성과 프로토콜 설정123456789#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int socket(int domain, int type, int protocol);/*** domain: 생성할 소켓의 통신을 위해 프로토콜 체계(Protocol Family)를 설정한다.** type: 생성할 소켓의 데이터 전송을 위해 전송 타입을 설정한다.** protocol: 두 호스트 간 통신을 위해 특정 프로토콜을 지정한다.*/ socket() 함수의 인자들을 제대로 이해하기 위해서는 프로토콜 체계를 이해할 필요가 있다. 프로토콜 체계 정의 PF_INET IPv4 인터넷 프로토콜 PF_INET6 IPv6 인터넷 프로토콜 PF_LOCAL Local 통신을 위한 UNIX 프로토콜 PF_PACKET Low Level socket을 위한 인터페이스 PF_IPX IPX 노벨 프로토콜 PF_INET을 소켓 함수의 첫 번째 인자로 전달하는 경우, 생성되는 소켓은 IPv4 기반으로 하는 인터넷 프로토콜에 적합한 소켓이 생성될 것이다. 이렇듯 데이터를 주고 받는 환경에 따라 그에 맞는 적합한 프로토콜을 지정해주어야 한다. 이 말은 즉 소켓은 모든 프로토콜을 수용할 수 있다는 뜻이며, “소켓은 프로토콜에 독립적이다”라고 표현한다. 소켓 함수의 두 번째 인자인 Type은 데이터 전송 타입을 말한다. 주의해야 할 것은 domain(프로토콜 체계)이 정해졌다고 해서 데이터 전송 방법까지 결정된 것은 아니라는 것이다. 즉, 하나의 프로토콜 체계 안에서도 데이터를 전송하는 방법이 여러 개 있을 수 있다는 뜻이다. 가장 대표적으로 사용되는 두 가지의 데이터 전송방식은 다음과 같다. SOCK_STREAM SOCK_STREAM 타입의 소켓은 연결 지향형 소켓이다. 연결 지향형 소켓을 사용하게 되면 두 사람이 하나의 라인을 통해 물건을 주고받을 수 있게 된다. 에러나 데이터의 손실이 없다. - 독립된 전송 라인을 통하여 데이터를 전달하기 때문에 라인의 문제가 아니라면 데이터는 반드시 전달될 수 있다. 전송하는 순서대로 데이터가 전달된다. 전송되는 데이터의 경계가 존재하지 않는다. - 이 말이 좀 애매할 수 있는데, 용량이 큰 데이터를 보낼 때 여러 개로 나누어서 보낼 수 있고, 하나의 큰 데이터를 여러 개로 나누어 옮길 수 있다는 뜻으로 이해하면 될 것 같다. 정리하면, 신뢰성이 있는 순차적 바이트 기반의 연결 지향 전송 타입이다. SOCK_DGRAM 비연결 지향형 소켓이다. 전송 순서에 상관없이 가장 빠른 전송을 지향한다. 전송 데이터는 손실될 수 있고 에러가 발생할 수 있다. 전송 데이터의 경계가 존재한다. 한 번에 전송되는 데이터의 크기가 제한된다. 소켓 함수의 세 번째 인자인 protocol은 호스트 대 호스트가 사용할 프로토콜을 설정하기 위해 사용된다. 프로토콜 체계가 PF_INET인 경우, IPPROTO_TCP: TCP 기반의 소켓 생성 IPPROTO_UDP: UDP 기반의 소켓 생성두 가지 프로토콜이 올 수 있다. 근데 사실, 첫 번째와 두 번째 인자만 잘 정해졌어도, 세 번째 인자에 0이 들어와도 적합한 소켓이 생성될 수 있다. 그렇다면, 이 세 번째 인자는 언제 사용되느냐? 바로 하나의 프로토콜 체계 안에서 최종적으로 통신하는 형태가 다른 여러 개의 프로토콜이 존재하는 경우를 위해서다. RAW_SOCKET을 생성하는 경우가 유용한 경우라고 하는데, 일단은 스킵하도록 한다. 대체로, socket(PF_INET, SOCK_STREAM, ) 인 경우에는 세 번째 인자로 TCP 타입이, socket(PF_INET, SOCK_DGRAM, )인 경우에는 세 번째 인자로 UDP 타입이 들어간다. 3. 주소 체계와 데이터 정렬일반적으로 하나의 컴퓨터 안에서 여러 프로그램을 동시에 실행시킨다. 인강을 들으면서 카톡을 한다던가 하는 경우… 이러한 경우 두 개 이상의 프로그램이 인터넷을 통해 데이터를 주고 받고 있다고 할 수 있다. 컴퓨터는 하나의 IP 주소를 가지고 있으며, 인터넷을 통해 데이터를 주고 받는 프로그램이 여러 개가 실행되고 있다고 하더라도 데이터를 송・수신 하는 길은 하나밖에 존재하지 않을텐데, 어떻게 수신한 데이터를 구분하여 각각의 프로그램에 전달해 줄 수 있는 것일까? 이것을 위하여 Port가 사용된다. 포트는 short int (0 ~ 65535)의 범위를 가지며, 포트를 사용하여 수신한 데이터 패킷(네트워크 상에서 이동하는 데이터 블록)을 어떤 프로그램에 전달할 것인지 결정한다. TCP 소켓과 UDP 소켓은 하나의 포트를 공유할 수 없으며, 데이터 전송 시에는 IP 주소 뿐만 아니라 Port 정보도 포함시켜야 한다. 자 그러면 이제 IPv4의 주소 체계를 알아보자. 123456struct sockaddr_in { sa_family_t sin_family; // 주소 체계 (address family) uint16_t sin_port; // 16비트 TCP / UDP port struct in_addr sin_addr; // 32비트 IPv4 주소 char sin_zero[8]; // 사용되지 않음}; 생소한 데이터 타입이 등장하는데, 이 타입들은 POSIX(Portable Operating System Interface)에서 근거를 찾을 수 있다. Data Type Description Header sa_family_t address family socklen_t length of struct 구조체의 sin_family 변수에는 주소 체계 정보를 대입한다. 아래 표의 값들이 사용될 수 있다. Address Family 정의 AF_INET IPv4 인터넷 프로토콜 AF_INET6 IPv6 인터넷 프로토콜 AF_LOCAL Local 통신을 위한 UNIX 프로토콜 sockaddr_in 구조체 변수에 값을 대입할 경우에는 반드시 네트워크 바이트 순서로 값을 변경한 다음에 대입해야 한다. Big-Endian 방식 상위 바이트의 값이 메모리 상에 먼저 표시되는 방법이다. Little-Endian 방식 하위 바이트의 값이 메모리 상에 먼저 표시되는 방법이다. 위 두 방식 중 어떤 방식을 택할것인지는 사용자의 CPU에 따라 달라진다. 이것을 ‘호스트 바이트 순서’라고 한다. 하지만 호스트 바이트 순서는 일정치 않다는 문제점이 있기 때문에, 네트워크 바이트 순서로 Big-Endian 방식만을 사용하기로 약속되어 있다. Big-Endian 방식으로 데이터를 변경하기 위해서 일종의 함수들을 사용해야 하는데, 함수 이름만 보면 그 기능을 충분히 파악할 수 있다. 123456789h: host byte ordern: network byte orders: short (16bit)l: long (32bit)unsigned short htons(unsigned short); // host to network short : 16비트 데이터를 host byte 순서에서 network byte 순서로 바꾸어라!unsigned short ntohs(unsigned short); // network to host short : 16비트 데이터를 network byte 순서에서 host byte 순서로 바꾸어라!unsigned long htonl(unsigned long); // host to network long : 32비트 데이터를 host byte 순서에서 network byte 순서로 바꾸어라!unsigned long ntohl(unsigned long); // network to host long : 32비트 데이터를 network byte 순서에서 host byte 순서로 바꾸어라! 일반적으로 short 타입은 Port 정보의 바이트 순서를, long 타입은 IP 주소의 바이트 순서를 변경하는데 사용된다. sockaddr_in 안에서 주소를 나타내기 위해 선언된 멤버의 자료형은 unsgined long 이었다. 따라서 IP 주소를 할당하기 위해서는 unsigned long 타입의 IP 주소를 표현해야 한다. 12345#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;unsigned long inet_addr(const char *string); inet_addr() 함수를 사용하면 해당 unsigned long 타입의 데이터 값을 반환해준다. 실패 시 INADDR_NONE 을 리턴한다. 하지만 inet_addr()을 사용하게 되면 반환값을 sockaddr_in-&gt;in_addr 구조체 안에 대입해야 한다. 이 번거로운 과정을 inet_aton 함수가 한 번에 해결해준다. 반대로 32비트 값을 Dotted-Decimal Notation(IP주소)으로 변환하기 위해서는 inet_ntoa 함수를 사용하면 된다. 123456#include &lt;sys/socket.h&gt;#include &lt;netinet/in.h&gt;#include &lt;arpa/inet.h&gt;int inet_aton(const char *string, struct in_addr *addr);char *inet_ntoa(struct in_addr addr); 주소 정보 구조체를 생성하고 초기화 하는 방법을 살펴보았다. 이제는 소켓에 주소를 할당하는 일만 남았다. 123456789101112#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int bind(int sockfd, struct sockaddr *myaddr, int addrlen);/*** sockfd: 소켓의 파일 디스크립터를 인자로 전달한다.** myaddr: sockaddr_in 구조체 변수의 포인터를 인자로 전달한다.** addrlen: 인자로 전달된 주소 정보 구조체의 길이를 전달한다.** success: return 0** fail: return -1*/ bind() 함수 호출이 성공하면 sockfd가 가리키는 소켓에 myaddr이 가리키는 주소 정보가 할당된다. 3. TCP 기반 서버/클라이언트bind() 함수를 이용하여 소켓에 주소까지 할당했으면, 이제는 listen 함수를 통해서 ‘연결 요청 대기 상태’로 들어갈 차례이다. 여기까지 진행이 되어야 클라이언트가 연결 요청을 할 수 있겠지. 즉, 여기까지 진행이 되어야 클라이언트가 연결을 요청하는 함수 connect()를 호출해도 오류가 발생되지 않는다! 1234567#include &lt;sys/types.h&gt;int listen(int s, int backlog);/*** s: 클라이언트의 연결 요청을 받아들이는 역할을 하게 될 소켓의 파일 디스크립터를 인자로 전달한다. 이 소켓을 서버 소켓 이라고 함.** backlog: 연결 요청 대기 큐(Queue)의 크기를 나타낸다. 보통 인자로 최소 15이상의 값이 들어오고, 큐의 크기가 5가 되어 클라이언트의 연결 요청을 5개까지 대기시킬 수 있게 된다.*/ 클라이언트의 연결 요청 역시 인터넷을 통해 들어오는 일종의 ‘데이터 전송’이다. 따라서 당연히 받아들이기 위해서는 소켓이 하나 있어야 한다. listen 함수의 첫 번째 인자로 전달된 소켓(서버 소켓)이 하는 일이 바로 이것이다. 일종의 수문장 역할인 셈이다. 클라이언트: “저기여,, 제가 혹시 감히 연결해도 될까요..?”서버소켓: “아 그래도 되긴합니다 ㅋ. 근데 시스템이 바쁘니까 번호표 받고 대기실 가서 기다리세여 ㅋ.” listen() 함수가 호출되면 첫 번째 인자로 들어온 소켓을 서버 소켓으로 만들어주고, 두 번째 전달되는 backlog 값을 참조하여 대기실을 만든다. 이 대기실이 바로 Queue의 크기가 되는 것이고, 클라이언트의 연결 요청을 대기시키게 한다. 자, 연결 대기 상태까지 만들어 놨으면 당연히 연결 요청을 수락해야 한다. 그리고 당연히 수락하는 과정에도 소켓이 필요하다. 하지만, 서버 소켓은 이미 하는 일이 있으니… 소켓을 하나 더 만들어야 하는데… 다행히 accept 함수를 이용하면 직접 만드는 헛수고를 하지 않아도 된다. 123456789#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int accept(int s, struct sockaddr *addr, int *addrlen);/*** s: 서버 소켓의 파일 디스크립터를 인자로 전달한다.** addr: 연결 요청을 수락할 클라이언트의 주소 정보를 저장한다. 호출 성공시 클라이언트의 주소 정보로 채워진다.** addrlen: 인자로 전달된 addr의 크기를 저장한다. 호출 성공시 리턴받은 클라이언트의 주소 길이가 바이트 단위로 채워진다.*/ accpet() 함수는 대기실에서 클라이언트의 연결 요청을 수락해 주는 함수이다. 따라서 호출 성공시 내부적으로 클라이언트와의 데이터 입・출력을 위한 소켓을 생성하고, 그 소켓의 파일 디스크립터를 리턴한다. 중요한 것은 소켓은 알아서 만들어지기 때문에, 직접 소켓을 만들지 않아도 된다. 자 그렇다면 이제는 TCP 기반 클라이언트를 살펴보자. 앞서 이야기했듯이, 서버보다 간단하게 구현할 수 있다. 차이가 났던 부분은 ‘연결 요청’과정(connect)과 전화번호를 할당(bind)받지 않는다는 것이었다. 서버가 listen() 함수를 호출하게 되면 클라이언트의 연결 요청이 대기실에 들어갈 수 있다고 했다. 그렇다면 클라이언트의 연결 요청은 어느 시점에 발생할까? 눈치챘겠지만 바로 connect() 함수 호출 시점이다. 123456789#include &lt;sys/types.h&gt;#include &lt;sys/socket.h&gt;int connect(int sockfd, struct sockaddr *serv_addr, int addrlen);/*** sockfd: 미리 생성해 놓은 소켓의 파일 디스크립터이다.** serv_addr: 연결 요청을 보낼 서버의 주소 정보를 지닌 구조체 변수의 포인터이다.** addrlen: serv_addr 포인터가 가리키는 구조체 변수의 크기이다.*/ connect 함수가 반환되는 시점은 연결 요청이 서버에 의해 수락(accept)되거나 오류가 발생하여 연결 요청이 중단되는 경우다. 만약 연결 요청이 바로 이루어지지 않고 서버의 대기 큐에서 대기하고 있는 상태라면 connect 함수는 리턴되지 않고 블로킹(blocking) 상태에 머무르게 된다. 자, 그렇다면 왜 클라이언트 영역에서는 소켓의 ‘전화번호’를 할당하지 않았을까? 전화번호인 IP 주소가 필요없다는 것인가?! 물론 아니다. 그렇다면 언제 주소를 할당받는 다는 것인가? Connect 함수가 호출할 때 주소가 자동으로 할당된다. 운영체제, 보다 정확하게 이야기하자면 커널이 생성해준다… 스스로 알아서 생성해준다… 커널은 호스트에 할당된 IP 주소를 찾아서 할당시키고, 남아돌고 있는 포트 중 하나를 골라서 할당시킨다. 즉, connect 함수 호출 시 자동적으로 소켓에 주소와 포트가 할당된다! bind 함수를 사용할 필요가 없다! 자, 여기까지 이해한 내용을 바탕으로 코딩을 진행하면, 우리의 서버는 한 클라이언트의 요청에만 응답하고 더이상 다른 어떤 요청도 무시하고 바로 종료된다. 이건 우리가 상상한 서버의 모습이 아니지 않은가? 따라서 클라이언트의 요청을 항시 대기할 수 있도록 해야한다. 간단하다. 클라이언트 연결이 종료되는 close() 함수 호출이 끝나고, 다시 listen() 상태로 돌아가게 하면 된다. 1234567891011while (1){ client_socket = accept(server_socket, (struct sockaddr *)&amp;client_addr, &amp;client_addr_size); if (client_socket == -1) { error_handling(&quot;accept() error&quot;); break ; } write(client_socket, message, sizeof(message)); close(client_socket);} 얼마나 간단한가! 지금까지 과정으로 간단한 에코 서버와 에코 클라이언트를 구현할 수 있게 되었다. 하지만 에코 클라이언트에서 생각할 문제가 있다. TCP는 연결 지향 프로토콜로서 데이터의 경계가 없다고 했다. 그렇다 보니 한 번의 write 함수 호출을 통해 “ABCD”라는 문자열을 전송한다고 했을 때 이 데이터들이 반드시 하나의 패킷으로 구성되어서 전송된다고 보장할 수 없다. 상황에 따라서 “AB” 문자열이 먼저 하나의 패킷으로 전송되고, 그 다음에 “C”가 전송되고, 마지막으로 “D”가 전송될 수 있다. 그렇다면, 이러한 데이터 전송 특징에 문제가 되는 것은 무엇일까? 정리하면, 지금까지는 한 번의 write 함수로 데이터를 전송하고 한 번의 read 함수를 통해 데이터를 수신했다. 이 때 한 번의 read 함수로는 일부 데이터 패킷만을 받았을 때 문제가 생긴다. 그렇다면, 반드시 하나의 패킷으로 구성되어야 온전한 데이터 송・수신을 보장할 수 있는데, TCP는 이것을 절대로 보장해주지 않는다. 이 때 123456789101112131415while (1){ ... for (recv_len = 0; recv_len &lt; str_len; ) { recv_num = read(sock, &amp;message[recv_len], str_len - recv_len); if (recv_num == -1) error_handling(&quot;read() error&quot;); recv_len += recv_num; } ...} 데이터를 전송하려는 과정에서 반복문을 통해 read 함수를 계속해서 호출하며 배열에 저장시킨다면, 정확히 전송한 바이트 크기만큼의 데이터를 수신할 수 있다. TCP 기반의 에코 클라이언트가 구현하기 쉬운 이유가, 수신할 데이터의 크기를 미리 알고 있다는 것에 있다. 다른 소켓 프로그램의 경우에서는 데이터를 수신하는 측에서 몇 바이트를 수신해야 하는지 알 수 없다. 이러한 경우엔 프로그램을 구현하기가 조금 더 어려워질 것이다. 하지만, EOF를 사용하여 전송한 데이터의 끝을 알리게 한다면 해결할 수 잇을 것이나, 제한적일 수밖에 없다… 경험과 시행착오를 통해서 항상 최선의 적합한 정답을 찾도록 하자……… 그리고 에코 서버에서 123456...accept()sleep(5);str_len = read(client_socket, message, BUFFER_SIZE);write(client_socket, message, str_len); 5초간 sleep을 걸어주면서 write 함수 호출이 여러번 들어왔을 때를 대비할 수 있다. 4. I/O 멀티플렉싱소켓 프로그래밍의 기본을 알았다면, 이제는 서브젝트에 해당하는 내용인 멀티플렉싱에 대해서 살펴볼 차례이다. Webserv 과제는 select() 함수를 이용한 단일 프로세스의 멀티플렉싱(multiplexing) 서버를 구현하는 것이다. 보통 다중 접속 서버를 구현하기 위해서 fork와 pipe를 이용하여 다중 프로세스로 처리한다. 하지만 프로세스의 생성은 상당히 많은 연산과정을 거치며, 생성 후에도 많은 자원을 차지하게 된다. 따라서 모든 프로세스들은 서로 독립적인 메모리 공간을 할당받아서 사용하기 때문에 프로세스간 통신을 하기 위해서는 다소 복잡한 방법을 선택할 수 밖에 없다. 하지만, 만약 하나의 프로세스로 여러 클라이언트들과 데이터를 주고받을 수만 있다면 보다 좋은 서버의 모델이 될 것이다. 이것이 바로 I/O 멀티플렉싱 방법이 등장한 배경이다. 자, 그렇다면 멀티플렉싱 서버란 어떠한 서버를 의미하는 것인가? 전화기지국과 다른 전화기지국을 연결하는 하나의 중계선이 있다고 가정해보자. 이 때 비록 선은 하나이지만, 이 선을 통해서 여러 사람이 통화를 할 수 있다. 즉 하나의 선을 여러 사람이 공유하며 통화를 위해 사용한다는 것인데, 이것이 바로 멀티플렉싱이다. 이렇듯 멀티플렉싱이란 데이터를 전송하는데 있어서 장치의 효율성을 높이기 위해 최소한의 물리적인 요소만 사용하는 경제학적인 방법이다. 서버에 멀티플렉싱 개념을 도입해보자. 서버에서의 멀티플렉싱은 ‘여러 개’를 묶어서 ‘하나’로 만드는 것(그 하나를 공유한다)이라고 생각하면 이해하기 쉽다. 멀티프로세스 기반에서는 하나의 서버(부모프로세스)를 토대로 여러 개의 자식 프로세스를 만들고 그 자식 프로세스들을 통해 클라이언트와 소통하게 한다. 하지만 멀티플렉싱 기반의 서버는 여러 개의 자식 프로세스를 생성하는 것 대신에, 클라이언트의 입・출력을 하나로 묶어서 하나의 서버로 여러 클라이언트와 통신할 수 있도록 하며, 클라이언트의 연결 요청 엮시 하나로 묶어서 처리한다. 이해가 쉽게 되지 않을 수도 있다. 다른 예를 들어보자. 한 반에 학생이 열 명이 있고, 한 명의 선생님에게 열 명의 아이들이 동시에 질문한다고 생각해보자. 학교 측에서는 이 질문세례를 막기 위해 선생님 9명을 추가로 고용한다. 이렇게 일 대 일 로 교사와 학생을 연결시키는 것을 멀티 프로세스라고 생각하면 된다. 하지만, 아이들이 공부를 하면서 점점 지식이 쌓이고, 질문의 수가 갑자기 확 줄어버렸다. 따라서 교사 한 명으로 커버가 될 것이라 예상하고 나머지 교사 9명을 해고했다고 해보자. 이 때 동시에 질문이 들어오는 것을 막기 위해 질문하고 싶은 학생에게는 손을 들게 하고, 교사는 손을 든 학생이 있는지 확인하다가 손을 든 학생의 질문을 받기로 했다. 이것이 바로 멀티플렉싱 방법이다. 서버는 주기적으로 데이터를 전송하는 클라이언트가 있는지 확인하며, 발견한 경우에 그 클라이언트로부터 데이터를 받아온다. 이렇게 생각하면 이해가 된다! 그렇다면, 어떠한 방식으로 멀티플렉싱 서버를 구현할 수 있을까? 대표적으로 select() 함수를 통해 멀티플렉싱 서버를 구현한다. select() 함수를 사용하면 한 곳에 모아놓은 여러 개의 파일 디스크립터(소켓)를 동시에 관찰할 수 있다. 데이터 전송 시 파일 디스크립터를 블로킹 처리하지 않고 바로 전달 가능한 파일 디스크립터가 어떤 것인지, 예외가 발생한 파일 디스크립터가 어떤 것인지 지켜보게 된다. 1234567891011121314#include &lt;sys/time.h&gt;#include &lt;sys/types.h&gt;#include &lt;unistd.h&gt;int select(int n, fd_set *readfds, fd_set *writefds, ft_set *exceptfds, struct timeval *timeout);/*** n: 검사 대상이 되는 파일 디스크립터의 개수** readfds: &quot;입력 스트림에 변화가 발생했는지&quot; 확인하고자 하는 소켓들의 정보. 입력 스트림에 변화가 발생했다는 것은 수신할 데이터가 존재한다는 뜻이다.** writefds: &quot;데이터 전송 시, 블로킹되지 않고 바로 전송이 가능한지&quot; 확인하고자 하는 소켓들의 정보.** exceptfds: &quot;예외가 발생했는지&quot; 확인하고자 하는 소켓들의 정보.** timeout: 함수 호출 후, 무한대기 상태에 빠지지 않도록 설정할 수 있는 변수** return value: -1(에러), 0(타임아웃), 0보다 큰 경우: 변경된 파일 디스크립터의 수*/ select 함수는 호출했을 때 관찰 대상들에게서 변화가 발생해야 반환되며, 그렇지 않으면 변화가 발생할 때까지 무한정으로 블로킹 상태에 빠지게 된다. fd_set 은 관찰하려고 하는 파일 디스크립터 정보를 모아놓은 비트 단위 배열이다. 함수 선언 기능 FD_ZERO(fd_set *fdset); fdset 포인터가 가리키는 변수의 모든 비트를 0으로 초기화 FD_SET(int fd, fd_set *fdset); fdset 포인터가 가리키는 변수에 fd로 전달되는 파일 디스크립터 정보를 설정 FD_CLR(int fd, fd_set *fdset); fdset 포인터가 가리키는 변수에서 fd로 전달되는 파일 디스크립터 정보를 삭제 FD_ISSET(int fd, fd_set *fdset); fdset 포인터가 가리키는 변수가 fd로 전달되는 파일 디스크립터 정보를 지니고 있는지 확인 select() 함수는 아래의 상황에서 반환된다. 수신할 데이터가 있을 때 - 읽어들일 데이터(read() 함수의 입력 버퍼)가 존재하는 경우(+ 클라이언트의 연결이 요청되었을 때). 파일 디스크립터를 통해 데이터를 전송할 경우 블로킹되지 않았을 때 - write() 함수의 출력 버퍼에 전송하지 못한 데이터가 남아있어서 곧바로 데이터 전송을 할 수 없는 경우 블로킹된다. 파일 디스크립터가 가리키는 소켓에서 에러가 발생하지 않았을 때. select() 함수 호출 예제12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;stdio.h&gt;#include &lt;unistd.h&gt;#include &lt;stdlib.h&gt;#include &lt;sys/types.h&gt;#include &lt;sys/time.h&gt;#define BUFSIZE 30int main(int argc, char **argv){ fd_set reads, temps; int result; char msg[BUFSIZE]; int str_len; struct timeval timeout; FD_ZERO(&amp;reads); FD_SET(0, &amp;reads); /* standard input 설정 */ while (1) { temps = reads; timeout.tv_sec = 5; timeout.tv_usec = 0; result = select(1, &amp;temps, 0, 0, &amp;timeout); if (result == -1) /* select 함수 오류 발생 */ { puts(&quot;select error&quot;); exit(EXIT_FAILURE); } else if (result == 0) /* timeout */ { puts(&quot;time over&quot;); } else { if (FD_ISSET(0, &amp;temps)) { str_len = read(0, msg, BUFSIZE); msg[str_len] = '\\0'; fputs(msg, stdout); } } }} select() 함수의 사용법까지 알았느니 실제 멀티플렉싱 서버를 구현해보자. /study/tutorial/ 디렉토리에 있는 multiplexing_server 를 임의의 포트와 연결시켜 실행하고, 여러 개의 터미널을 통해 multiplexing_client를 실행시켜보자. 12345# multiplexing_server./multiplexing_server 9130# multiplexing_client./multiplexing_client 127.0.0.1 9130 5. Webserv 과제 진행하기본격적으로 공부한 것을 토대로 과제에 진입해보자. 웹서비스는 서버와 클라이언트 모델을 따른다. 이 때 두 가지 모델에서 정보를 제대로 주고받고 해석하기 위해서는 두 가지 프로토콜이 필요하다. 하나는 데이터요청과 응답에 관한 프로토콜이고, 또 다른 하나는 데이터의 형식을 정의하기 위한 프로토콜이다. 전자가 HTTP, 후자가 HTML이다. 클라이언트가 형식에 맞게 웹서버에 무엇인가를 요청하면, 웹서버는 이 요청을 분석하여 알맞은 형태로 응답한다. 응답을 받은 클라이언트는 정보를 해석해서 사용자에게 보여준다. 이번 과제에서는 HTTP 프로토콜만 신경쓰면 되는 것 같다. 자 그럼 HTTP 프로토콜을 살펴보자. 요청방법 (GET/POST/HEAD/PUT/DELETE …) 요청 페이지 (index.html) 프로토콜 (HTTP/1.1) GET 요청 방법부터 구현해보자. 웹브라우저로 웹페이지를 요청할 때 사용하는 일반적인 방법이다. 요청할 URL: GET이면 요청할 페이지, POST라면 클라이언트가 보낸 자료를 처리할 서버의 url이다. 프로토콜 버전: 책에 나오는 시점에서 최신은 1.1이라고 했는데 아직도 1.1인지는 모르겠다. (나중에.. 확인해보자.) 요청을 한 이후에는 적절한 응답을 보내주어야 하는데, 이것 역시 HTTP 형식을 따라야 한다. (최소 아래 모든 헤더를 구현해야 한다.) 123456789101112131415161718◦ Accept-Charsets # 클라이언트가 이해할 수 있는 문자◦ Accept-Language # 서버가 돌려주기로 예상된 언어◦ Allow # HTTP 요청 방법◦ Authorization # 서버와 함께 유저 에이전트를 인증하기 위한 자격증명◦ Content-Language # 사용자 언어◦ Content-Length # 문서의 크기◦ Content-Location # 반환된 데이터의 대체 위치◦ Content-Type # 이미지나 동영상 등 다양한 형식이 나타날 수 있기 때문에 형태를 알려주기 위해 꼭 필요하다◦ Date # 문서를 전송한 시간◦ Host # 서버가 listening 중인 TCP 포트 번호◦ Last-Modified # 마지막 수정 날짜◦ Location # 페이지를 리다이렉트할 url◦ Referer # 현재 페이지로 연결되는 링크가 있던 이전 웹 페이지의 주소◦ Retry-After # 클라이언트가 다음 요청을 생성하기 전에 대기해야 하는 시간◦ Server # 웹서버 프로그램의 정보◦ Transfer-Encoding # 데이터 전송을 위한 인코딩 형식◦ User-Agent # 사용자 에이전트의 어플리케이션 타입◦ WWW-Authenticate # 리소스에 접근할 때 사용되어야 하는 인증메소드 임의 페이지에 GET 요청을 보내보자. 123456# Telnet을 사용할 때는 반드시 끝에 개행문자가 두 개 와야한다.% telnet www.joinc.co.kr 80GET /modules/moniwiki/wiki.php/FrontPage HTTP/1.0\\n\\n 웹서버 프로그램을 만들기 위해 HTTP 프로토콜과 응답방식 등에 대해서 알아보았다. 자 그러면 구조를 설계해보자. 클라이언트의 요청을 받은 웹서버는 파일 시스템에서 파일 정보를 읽어서 클라이언트에 전송한다. 혹은 외부의 다른 프로그램을 실행해서 그 결과를 클라이언트에 전송하기도 한다. 외부 프로그램으로의 정보 입출력은 CGI 규격을 사용한다. CGI 너는 도대체 무엇이냐..! 끝도 없이 새로운 게 나타난다… PHP와 같이 이해해보자. 웹서버는 웹 서비스를 위한 하부구조 역할을 한다. 이 때 웹서버는 다양한 서비스를 개발할 수 있도록 외부의 프로그램을 실행시켜 데이터를 대신 처리한다. 이 때 웹 서버와 외부 프로그램 간의 통신 방식을 CGI(Common Gateway Interface)라고 한다. CGI 프로그램은 공통규격만 따르면 되기 때문에 어떤 언어를 사용해도 관계없다. 하지만, 우리는 C++로 만들어야 하지 않은가… 뭐 이러한 특징 때문에 서로 다른 언어들로 디버깅을 하게 되더니 개발과 유지보수가 매우 어려워졌다고 한다. 이렇듯 여튼저튼 이유로 웹페이지 문서에 통합되는 언어로 개발할 필요가 생겼는데, 이렇게 해서 만들어진 언어가 바로 PHP라고 한다. PHP 코드는 HTML 문서에 포함되어 있는데, 덕분에 개발과 디버깅 과정이 웹 문서에 통합된다. 자 그렇다면 이제 웹서버의 구조를 설계할 수 있다. 반드시 C++로 HTTP 서버를 구현해야 한다. 서버는 반드시 RFC 7230 ~ 7535 (http 1.1) 을 준수하도록 한다. 서버와 클라이언트 사이의 통신은 non-blocking으로, 단 하나의 select()와 listen()을 사용하도록 한다. 서버는 정지되면 안되고 클라이언트는 적절하게 연결을 끊을 수 있도록 한다. 서버로 온 Request가 영원히 머물러서는 안된다. CGI를 제외하고는 fork()를 사용하지 말자. 서버 소켓을 생성한다. bind() 함수를 사용하여 주소를 할당한다. listen() 함수를 사용하여 사용자의 요청을 대기하도록 한다. select() 함수를 이용하여 멀티플렉싱으로 사용자의 요청을 accept() 할 수 있게 한다. - 이 때 select 함수는 동시에 read와 write를 동시에 확인하도록 한다. read() 로 클라이언트의 요청을 수신하고 요청방식에 따라 처리할 수 있도록 구조화한다. 요청방식에 따라 응답할 HTTP 형태를 구현한다. write() 로 클라이언트에게 응답한다. 소켓을 닫는다.","link":"/%EC%86%8C%EC%BC%93%EA%B3%BC-select%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%A9%80%ED%8B%B0%ED%94%8C%EB%A0%89%EC%8B%B1-%EC%84%9C%EB%B2%84-%EA%B5%AC%EC%B6%95%ED%95%98%EA%B8%B0/"},{"title":"빛예닮특강 - 논문읽는법","text":"1. 제목 &amp; 저자제목을 통해 해당 논문이 어떤 내용을 담고 있을지 상상해 보는 것이 좋음. 예를들어 Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 해당 논문의 제목만 보고 ‘’아 이 논문은 Image Caption에 대한 논문이겠구나. 오 근데 시각적인 Attention을 이용했네?’ ‘Attention이 뭐지? 그냥 읽을까?’ ‘조금 찾아보고 읽을까?’ ‘아 이건 완벽하게 공부하고 다시 읽자’ 와 같이 모르는 용어가 나타났을 때 어떻게 대처할지 등 논문을 읽을 방향성을 잡을 수 있다. 멘토님은 아주 조오오오금 (사전지식 정도만) 서치하고 읽는다고 하심 또한 제목과 더불어 저자를 보면 논문의 신뢰성을 대략적으로 추정할 수 있음. ex) 조경현교수님: RNN의 축을 그으셨던 대가가 참여했으니 이 논문의 수식이나 피쳐들에는 오류가 없겠구나! 신뢰성이 중요한 이유 중 하나: 매우 유명한 GAN 논문에는 수식의 오류가 있었음. 즉, 저자를 보고 의심할 수 있는 능력이 있어야 함. 2. Abstract2. Abstract최대한 꼼꼼하게 읽자. Abstract 부분은 논문에서 가장 처음으로 읽게 되는 파트다. 해당 파트를 읽고 이 논문을 계속 읽을지 말지 판단할 수 있기 때문이다. ‘Abstract만 읽었는데 재밌네’ -&gt; 논문을 읽어봐야겠다! ‘이거 Abstract 너무 어려운데..?’ 혹은 ‘하나도 이해 못하겠다..’ -&gt; 읽지 않는 편이 좋음. 3. InstructionAbstract를 읽고 논문을 계속 읽는 것으로 결정하였다면, 다음은 Instruction을 읽자. 논문이 나타난 이유나 해당 논문에서 이야기하고 싶은 내용을 파악할 수 있음. 다만 Instruction 파트가 너무 길 경우에는 다짜고짜 읽지 말고 우선 논문 전체의 Figure을 쭉 훑는다. Figure를 보면 해당 논문에서 이야기하고자 하는 모델이 어떤 모델일지 대략적으로 알아낼 수 있다. 또한 Figure를 보면서 해당 논문이 어떤 내용에 대해서 다루는 것일지 예측해보자. 계속해서 뒤에 나올 내용에 대한 예측의 중요성에 대해 언급을 하시는데, 예측이 맞는지 틀린지에 대한 여부로 본인이 논문에 임하는 자세를 수정하거나 자신감을 얻을 수 있음. (예측이 맞다면, 아 내가 논문을 잘 읽고 있구나. 반대로 틀렸다면, 아 조금 더 신경써서 읽어봐야겠다.) Instruction을 읽으면 해당 논문을 가볍게 읽을지 딥하게 읽을 지 결정할 수 있다. 3-1. Light 하게 읽기별 거 없다. 짧게 훑어보고 말자. 3-2. Deep 하게 읽기논문을 신경써서 읽어야겠다고 다짐했다면, 다음으로 봐야할 부분은 Model 파트이다. 모델 구조를 먼저 보자. 최근에 나오는 논문들은 SOTA를 달성한 모델들의 특징이 비슷하게 나타나있는 경우가 많은데, 예를들어 모델 구조에 Encoder가 포함되어 있다면, 쌍으로 Decoder가 등장할 것임을 예측할 수 있다. 여기부터는 Figure 혹은 글을 보고 이해가 되지 않는 부분이 생길 수 있는데 딥하게 읽기로 결정했으니 논문을 반복해서 읽으면서 이해하도록 노력해야한다. 그럼에도 이해하지 못했다? 그럼 또 읽어라. 이렇게 쭉 읽어나가면 된다!","link":"/%EB%B9%9B%EC%98%88%EB%8B%AE%ED%8A%B9%EA%B0%95-%EB%85%BC%EB%AC%B8%EC%9D%BD%EB%8A%94%EB%B2%95/"},{"title":"시각화 이해하기","text":"1. 데이터 이해하기1.1 데이터 시각화데이터 시각화를 위해서는 ‘데이터’가 우선적으로 필요하다. 데이터를 어떻게 시각화하냐는 데이터를 로컬하게 보냐 글로벌하게 보냐에 따라 다르다. 개별 데이터를 보여줄 것인지 전체 데이터에 대한 구성을 보여줄 것인지를 정해야 하고, 그러기 위해서는 어떤 데이터가 있고 어떤 데이터셋이 있는지 알아야 한다. 데이터에 대해서 어떤 것들을 전달할 수 있고, 어떤 데이터셋이 있는지 알아보자. 1.2 데이터의 종류수많은 데이터셋이 있고, 데이터들은 여러 특징들로 나눌 수 있다. 정형 데이터 일반적으로 csv 파일로 제공되는 데이터이다. Row: 데이터 한 개 (item) Column: Attribute (feature) 가장 쉽게 시각할 수 있는 데이터셋이며 통계적 특성과 Feature 사이의 관계들로 많이 이야기할 수 있다. 데이터 간 비교하고, 통계적 특성들을 비교할 수 있음. 시계열 데이터 시간 흐름에 따른 데이터이며, Time-Series Data라고 한다. 기온, 주가 등의 정형데이터와 음성, 비디오와 같은 비정형 데이터가 존재한다. 시간 흐름에 따른 추세(Trend), 계절성(Seasonality), 주기성(Cycle) 등을 살필 수 있다. 지리 데이터 지도 정보와 보고자 하는 정보간의 조화가 중요하며, 지도 정보를 단순화 시키는 경우도 존재한다. 거리, 경로, 분포 등 다양하게 실사용에 이용되고 있다. 정형데이터 시각화와 다르게 실제로 어떻게 사용될 수 있을지가 중요하고, 거리 정보가 굉장히 중요하다. 위도 경도 등도 추가로 학습해야한다. 관계형(네트워크) 데이터 객체와 객체 간의 관계를 시각화할 수 있으며 객체는 Node로 표현하고 관계는 Link로 표현한다. 크기와 색, 수에 따라서 객체와 관계의 가중치를 표현한다. 직관적이지 않아서 Mapping 방법이 굉장히 중요하며 네트워크를 구성할 때 Huristic 하게 이용한다. 계층적 데이터 회사 조직도나 집안의 가계도 등인 Hirarchy 를 강조하는 시각화이며 보통 Tree 구조로 표현한다. 한국 수출 아이템에 대한 분야별 분포 등을 계층적 데이터로 시각화할 수 있다. 다양한 비정형 데이터 데이터는 굉장히 다양하기 때문에 대표적으로 4가지로 분류한다. 수치형 데이터 (Numerical) 연속형 (Continuous): 길이, 무게, 온도 등 데이터 간 실수값들이 존재하는 데이터 이산형 (Discrete): 주사위 눈금, 사람 수 등 중간이 없고 떨어져 있는 한 단위들이 데이터인 것. 범주형 데이터 (Categorical) - 문자열로 나타나는 것들 명목형 (Nominal): 혈액형, 종교, MBTI 등 순서가 딱히 중요하지 않은 데이터 순서형 (Ordinal): 학년, 학점, 등급 등 순서가 존재하는 데이터. 이산형으로 볼 수도 있지만 스케일 자체가 비율이 있다면 이산형, 수치가 절대적이지 않으면 순서형으로 표현하는 편이다. 시각화 이전에 이 데이터는 어떤형 데이터이기 때문에 이런 시각화를 해야겠다! 라는 접근이 필요하다. 2. 시각화 이해하기2.1 마크와 채널 시각화는 보통 그래픽에서 가장 중요한 세 단위(점, 선, 면(mark)) 로 이루어진 데이터를 어떤 식으로 나타내고 어떤 식으로 조화할 지에 대한 고민으로 이루어진다. 점, 선, 면에서 어떠한 Variation을 줄 수 있는지를 visual channel 이라고 하는데, 각 마크를 변경할 수 있는 요소들을 말한다. 평행하게 움직이는지, 수직적으로 움직이는지, 아니면 동시적으로 움직이는지, 길이나 색, 기울기, 모양 등을 변경할 수 있다. 즉 시각화에서는 점, 선, 면을 어떻게 다루고 어떻게 시각화를 할 수 있을지부터 시작한다. 2.2 전주의적 속성데이터에서 변경시킬 수 있는 요소 중에서 주의를 주지 않아도 인지하게 되는 요소를 전주의적 요소 (Pre-attentive Attribute) 라고 한다. 딱 눈으로 봤을 때 직관적으로 기존과 다르다는 인사이트를 바로 전달될 수 있게 사용하는 것이 중요하다. 하지만 점주의적 속성들은 동시에 사용했을 때 인지하기 어렵기 때문에 데이터 자체는 시각적인 분리가 잘 이루어지는 데이터인 경우에도 직관적으로 한 눈에 보기 어려운 경우가 생길 수 있다. 즉 전주의적 속성은 적재적소에 시각화하는 것이 중요하다.","link":"/%EC%8B%9C%EA%B0%81%ED%99%94-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0/"},{"title":"테크세미나 후기 &lt;보이저엑스 남세동 대표, 42 SEOUL 카뎃과의 수다&gt;","text":"오랜만에 일정이 겹치지 않아서 테크세미나에 참여할 수 있었다. 매주 훌륭한 기업들과의 연계를 통해 세미나를 진행하는데, exam 혹은 rush 평가등과 겹쳐서 제대로 참여할 수 있었던 건 3회차 이후로 처음인 것 같다…! 보통 발표자만 발표영상을 켜놓은 이후 강의를 듣는 것처럼 일방향적인 세미나가 많은데, 이번 세미나는 참여자들 대부분이 자신의 화면을 띄워두고 채팅과 음성대화를 통해 쌍방향적인 소통으로 세미나가 진행되었다. 확실히, 젊은 기업 보이저엑스! 크게 두 챕터로 진행되었다. 1. 회사 내부 영상 및 소개 급하게 만드셨다고는 했는데, 생각보다 영상퀄이 훌륭했다. 나중에 알고보니 보이저엑스의 제품을 이용하여 만든 영상이었다…ㅋㅋㅋㅋ 회사출입문이 거의 뭐 우주선이다. 들어갈 때마다 일하고 싶을 것 같다. 회사에 뜬금없이 트램펄린이 있는데, 촬영자분이 갑자기 올라타 뛰셔가지고 너무 웃겼음ㅋㅋㅋ 보이저엑스는 딥러닝/연구개발 인력 18명, 디자인/기획/경영/총무 인력 6명으로 구성이 되어있다. 생각보다 인력이 많지는 않은데 인턴직 월급을 300만원 주는 것만 보면 알 수 있듯이, 그 적은 인력들 한명한명이 다들 엄청난 것 같다. 보이저엑스가 현재 제공하고 있는 서비스는 VREW, vFlat, 온글잎 세 가지가 대표적이다. VREW 딥러닝을 이용한 영상편집 프로그램으로, 무음구간을 인식하거나 음성인식된 결과물들에 따라 편집시점을 지정해준다. 또한, 얼굴인식과 자동 화면전환 기능을 계획중이라고 한다. vFlat 딥러닝을 활용한 모바일 스캐너로, 곡면을 평면으로 만들어준다는 타사 대비 장점이 있다. OCR기능도 잘 된다고 한다… 온글잎 손글씨를 이용하여 1인 1폰트를 만들어준다는 취지로 계획된 서비스이다. 2. 질의응답(Q&amp;A)Q: OCR과 음성+영상, 온글잎은 각각 읽고 듣고 보고 쓰는 행동과 연결된 제품들인데 이중에 향후 2-3년 안에 가장 세상에 큰 영향을 주게 될 제품이 어떤 것이라고 생각하시나요? 특히, 2-3년 뒤에 VREW의 모습을 어떻게 그리고 있는지 궁금합니다. A: 셋 다 잘될 것이라고 기대한다. 지금 만들어져 있는 제품들의 결과물들을 보면 창의적이고 대단한 일을 하고 있는 것 같지만 딱히 그렇지는 않다. 코딩도 그렇듯이, 영상편집도 소위 노가다라는 것이 매우 심각한데, 그 부분을 인공지능이 줄여줄 수 있지 않을까라는 생각으로 시작했다. 2-3년 뒤의 VREW는 자동으로 표정인식을 하고, 편집시점을 알고리즘으로 찾아내어 영상편집을 좀 더 쉽게 해줄 수 있는, 지금보다 더욱 완성형 플랫폼이 되어있지 않을까라는 생각을 해보셨다고 한다. Q: 사용자 데이터를 강화학습에 사용하지 않는다고 들었는데, 사용자 결과물을 데이터로 사용하지 않는 이유는 무엇인지? 사용자가 많아지면 VREW의 모델이 어떻게 달라질 수 있을까? A: 영상 편집자들 중에서 회의내용, 개인정보 보호 이슈에 민감한 사람들이 있는데, 사용자 데이터를 사용하지 않으면 그런 이슈들을 피해 갈 수 있다. 또한 아무리 데이터를 모아봐야 구글, 네이버, MS에 비해 수집된 데이터의 양이 매우 작을 것이라고 생각한다. 5년뒤면 음성인식 엔진은 무료로 사용할 수 있게 될 것이라서, 사용자가 많아지게 될 시 엔진을 강화하기보다는 사용자의 니즈를 파악하는 것에 집중하는 방향으로 이루어지는 게 이상적일 것이다. Q: 회사 내에서는 프로젝트의 참여여부를 자발적으로 결정하게 한다는데 왜 그렇게 하는 것인지? A: 보이저엑스에서는 정책적으로 하고싶은 일을 할 때 가장 잘 한다고 생각하기 때문에 하고싶은 일을 시키도록 하고 있다. 언제나 1순위 일만 할 수 있는 것은 아니지만, 하기 싫은 경우 시키지는 않음. 인턴의 경우 따로 특별한 보상은 없음. Q: 수학을 모르면 딥러닝을 못할까? A: 딥러닝의 수학이 매우 어렵다고 생각하는 사람들이 많은데, 보통 생각하는게 석박이 아니면 딥러닝을 못한다는 것이다. 그러나 사실 딥러닝을 잘하기 위해서는 고등학교 수학만으로도 충분하다. 레벨의 차이가 있겠지만, 초등학생들도 딥러닝을 할 수 있기 때문이다. 못해도 된다라는 것은 대학원수준 수학을 몰라도 돤다는 식이고, 할 줄 알아야 한다는 것은 고등학교 수준 수학은 해야 한다. (딥러닝 분야에서만 적용) Q: 회사에서 사용하는 툴은 주로 무엇인지? A: 프로젝트를 자발적으로 참여시키는 것처럼, 툴이나 프레임워크 역시 자율적으로 선택하게 한다. 또한 프레임워크나 툴 등이 유료여도 얼마든지 구매해준다. 대신, 팀웍 면에서 문제가 생기는 부분이 있다면, 그건 고려해봐야겠지만. (모두가 슬랙을 쓰는데 카톡을 쓰는 등) Q: 딥러닝을 배운다는 것은 새로운 모델링을 개발하는 것인지, 기존 모델로 응용하는 것인지. A: 배달 앱을 만든다고 가정한다. 우선은 지도 api를 사용하여 배달 상황을 표시하는 기능을 넣으려고 한다. 헌데, 쓰려고 보니까 gps라는 것이 있다. 그래서 gps를 공부하다 보니까 상대성이론이 등장하고, 상대성이론을 공부하다 보면 공부가 끝이나지 않아서 결국 배달 앱을 만들 수 없게 된다. 딥러닝을 공부하기 위해 너무 깊이 들어가는 것은 배달 앱을 만들기 위해 상대성이론을 공부하는 것과 비슷하다. Q: 구글 네이버등과 비교해서 잘할 수 있는 영역인지 여부는 어떻게 판단하는지? A: 구글, 네이버, 마소가 할 수 있는 일은 절대 하지 않는다가 회사의 원칙이다. 10년 전으로 돌아가서 android와 ios와 경쟁하는 os를 만들겠다!하는 일은 하지 않는다. 그러면 스타트업이 하는 일은 과연 무엇인가? 찾아보면 구글, 마소가 하지 못하는 일들이 매우 많다. 따라서 배민, 슈퍼셀, 쿠팡 등이 나오는 것이 그러한 이유일 것이다. 그런관점에서 보이저엑스도 큰회사에서 하지 못할, 하지 않을 일들을 맡아서 한다. 사실 딥러닝에 데이터가 많이 필요한 것은 아니다. 예를들어 음성인식은 데이터가 굉장히 많이 필요하기 때문에 구글이 해야 할 일이다. 즉 음성인식은 구글, 네이버 등의 것을 쓰지만 그 결과를 가지고 적용하는 일들을 보이저엑스에서 한다. 이런 일들은 대단한 데이터가 필요한 것이 아니기 때문. Q: 딥러닝이 다른 모델에 비해 대세가 된 이유는? A: GPT-3나 알파고, 자율주행 등의 놀라운 부분들은 십중팔구 딥러닝 때문이다. 그러면 왜 그렇게 딥러닝이 잘되냐? 그 이유는 아직 아무도 모른다. 개발자, 컴퓨터를 하는 사람들이 어떻게 아무도 모르는 것을 쓸 수 있냐는 질문이 나올 것이다. 우리는 인공적인 세상에서 살았기 때문에 이해하지 못하는 것이 하나도 없기 때문이다. 모든 것을 사람이 이해하고 만들었기 때문에 모르는 것이 있을 수가 없었다. 헌데 사실 세상은 그런 인공적인 분야가 아니라서, 왜 되는지 왜 잘 되는지 모르는게 당연한 것이다. 진짜 산업혁명이 시작된 증기기관 때를 살펴보면, 그 당시에는 열역학이라는 이론도, 에너지보존법칙이라는 이론도 없었고, 증기기관이 정착되고 나서 열역학이 정리가 되었다. 4차산업혁명이라는 단어역시 마찬가지다. 딥러닝은 현재 왜 잘되는지 이유는 모르지만, 실제로 잘 되고 있으며 한계가 있을 것이라는 사람들도 매년 나타나고는 있지만, 그 한계는 계속 돌파되고 있기 때문이다. Q:딥러닝 공부순서는? A: 페이스북에 올린 대표님의 글 참고! 딥러닝공부순서 ㅌㅇ 천재개발자 남세동의 인공지능 정확히 1년 반 전, 우연히 유튜브에서 위 영상을 보고, 인공지능에 대해서 공부해보고자 마음먹게 되었다. 그렇게 문과였던 내가 남세동 대표님을 통해 코딩에 입문하게 되었고, 아마추어이든 주니어이든 어찌저찌 개발자가 된 지금의 내가 남세동 대표님의 세미나를 듣고 있다는 사실이 기쁘기도 하고 놀랍기도 하고 복잡미묘한 감정이 들었다. 어찌됐든, 이렇게 좋은 기회를 얻을 수 있었던 것도 모두 42 덕분인 것 같다. 언젠가는 남세동 대표님과 같이 한 번 식사라도 할 수 있었으면 좋겠다. ㅎㅎ","link":"/%ED%85%8C%ED%81%AC%EC%84%B8%EB%AF%B8%EB%82%98-%ED%9B%84%EA%B8%B0-%EB%B3%B4%EC%9D%B4%EC%A0%80%EC%97%91%EC%8A%A4-%EB%82%A8%EC%84%B8%EB%8F%99-%EB%8C%80%ED%91%9C-42-SEOUL-%EC%B9%B4%EB%8E%83%EA%B3%BC%EC%9D%98-%EC%88%98%EB%8B%A4/"},{"title":"프로그래머로 산다는 것 - 라이엇 게임즈 유석문 CTO","text":"1. 개발자???2013 문화체육관광부 우수학술도서 - 프로그래머로 산다는 것(황상철, 하호진, 이상민, 김성박 - 로드북) 해당 책이 출판되고 나서 많은 개발자들이 화를 냈음. 책 표지가 화장실에서 노트북을 하는 장면이었기 때문임. 미래도 없고 꿈도 없듯이 밤낮없이 화장실에서도 일하면서 과로로 쓰러지라는 것이냐! 라는 느낌이었기 때문. 기존 개발자들은 연봉도 오르지 않고 쇠퇴하는 경우가 많았기 때문에 프로그래머를 평가절하한 것이냐는 의견이었음. 프로그래머가 왜 되려고 하는 것인가? 대부분의 경우 프로그래밍이 어렸을 때부터 너무 재밌었기 때문에 프로그래머가 된다고 한다. 보통 모바일 게임의 경우 화장실에서도 볼일을 다보고도 계속 하듯이, 프로그래머가 되려고 한 자체는 결국에 이 일이 재밌고 즐거워서, 평생 할 수 있어서이기 때문이어야 할 것이다. 해당 책 소개를 의도한 이유는, 재밌다면 그 일을 계속 하라는 것을 이야기하고 싶어서이다. 코딩이 재밌다면 화장실이 아니라 어디에서 하든 상관없다. 하지만 나를 소진해가면서 밥벌이를 위해서라면 하지 마라는 것을 이야기하고 싶었다. 1.1 개발자??? or ?????보통 신입사원 면접을 진행할 때 겪는 일이다. 기술면접 할 때 아이스브레이킹 시간을 가지고 간단한 기술문제를 푼 이후 난이도 조절을 통해 다음단계를 진행하게 된다. 아래는 3-5년차 자바 개발자를 대상으로 면접을 할 때 냈던 문제이다. 위의 문제를 가지고 해당 자료구조(스택)를 만들어보라는 문제를 냈다. 하지만 클래스 선언도 못하는 경우가 허다했다. 이력서는 화려한데 왜 못하지..? 라고 물어보면 “최근에는 개발보단 관리를 많이 하느라..” 라는 답변이 돌아온다. 이런 상황이 이해가 되지 않았음. ‘읭?’ 1.2 개발(놈) Begins - 업무 할당프로그래밍 관련 학과를 졸업하고, 현업에서의 시간도 어느정도 지났는데 왜 하지 못할까라는 고민에 대한 고찰. 회사에 처음 가서 어떤 과제를 받았다고 해보자. 정상적이지 않은 회사라고 한다면 대부분 “이 일을 언제까지 끝낼 수 있겠니? 참고로 시간이 없어.” 라면서 일을 시킨다. 무조건 그 시간안에 다 해야 하고, 심지어 충분한 레퍼런스나 리소스도 없다. 관리자가 시간만 관리하고 기술적 지원/교육도 없는 상태에 데드라인에 맞춰서 일을 끝내야 하는 상황에 놓였다고 상상해봐라.. 검색 복사 &amp; 붙여넣기 되는것처럼 보일때까지 삽질. 1.3 개발(놈)의 탄생 주역네이버든 어떤 조직이든간에 개발 잘하는 사람들이 있는 반면 저런 사람들도 (당연히) 있을 것이다. 운이 좋으면 좋은 팀을 만나서 성장을 막 할 수 있을테고, 운이 나쁘면 저런사람들을 만나서 소진될 것이다. (운 나쁠 확률이 더 많음 ㅋ.) 기피해야 하는 대상 나쁜 고객과 상사 - 어떤 기술을 쓰든 좋은 기술을 쓰든 관심없고 대부분은 기술능력도 없고 결과만 나오면 되길 원함. 이러면 내 인생을 낭비하게 되는 것과 다름없다. 탐욕스러운 회사 - 개발자들은 백날천날 같은 기술만으로는 살아남을 수 없다. 새로운 것을 배우고 적용해봐야 실력이 늘며 성장하는 것인데, 탐욕스러운 회사는 똑같은 일만 시킨다. 그렇게 개발자들은 소진될 것이다. 비협조적인 동료 - 서로 긍정적인 영향을 주고받을 수 있으면 좋은데, 대부분의 동료는 내가 하는 일에 관심이 없을 것임. 더 좋은 방법이 있다고 했을 때 대부분은 그 사실을 설득시키기가 어려움. 거부감이 많기 때문임. 직장생활을 할 때 당연히 겪게되는 문제이다. 하지만 위의 것들은 통제할 수 없는 외부 요인이다. 통제할 수 없는 것들은 깨끗하게 포기하고 스스로를 지키고 성장시킬 수 있는 방법을 터득해야만 한다. 1.4 개발자의 필수능력깔끔한 코드 사람이 이해하기 쉬운 코드 변경이 용이한 코드 유지보수 비용이 낮은 코드 하지만 생각보다 코드 자체의 가독성에 관심없는 사람이 많다. 가독성 없는 코드에 오류가 발생한 뒤 그것을 내가 고쳐야한다..? 어떤 기업에서는 다른 사람이 짠 코드들을 다른 사람이 고치는 경우가 많다. 즉 쉽게 고칠 수 있는 코드가 필요하다. 따라서 개발자라면 매우 깔끔한 코드를 짤 수 있어야 한다. 컴파일 오류든 실행오류든 이딴거 필요없다. 사람이 이해할 수 있는 코드를 작성해야 한다. 이렇게 되면 코드 변경이 용이해져서 버그 수정도 편하고 얼마든지 다른 기능도 쉽게 추가할 수 있다. 신입개발자들 이력서를 보고 자신을 어떤 개발자라고 표현하는지를 보면 굉장히 모호할 때가 많다. 개발자로서 자신을 어필하기 위한 가장 좋은 방법은, ‘코드 깔끔하게 잘 짠다. 변경하기 용이한 코드를 잘짠다. 초등학생도 이해한다.’ 와 같은 말을 쓰는 것이다. “코드를 지우는 사람입니다.” 라는 말에 엄청난 인상을 준 사람이 있었다. 개발자는 코드를 생산한다고 흔히 생각하는데, 그 분은 불필요한 코드를 줄이고 유지보수할 수 있는 코드를 짜는 사람이라는 이야기가 얼마나 내공이 높은 것인가. 적절한 논리력 원리 탐색 능력 제약조건을 고려한 해법 단순한 디자인 “절대 죽지 않는 웹서버를 만들 것이다.” 와 같은 극단으로 가는 것은 불필요하다. 엔지니어라고 한다면 현재 가용가능한 리소스를 가지고 적절한 동작을 하게 하는 것이 필요하다. 나는 이 기술을 많은 사람들이 썼으면 좋겠다고 생각해서 백만명을 수용할 수 있는 서버를 만들었는데 막상 오픈했더니 사용자가 열 명이다. 이러면 말짱도루묵이지 않은가. 따라서 적절한 리소스로 적절하게 문제를 해결하는 것이 가장 좋다. 원리를 잘 탐색하고 무작정 만드는 것이 아니라 제약조건을 고려한 해답을 찾고 단순한 디자인을 만들 수 있는 것이 가장 좋다. 1.5 깔끔한 코드 작성법ATDD (Acceptance Test Driven Development) TDD(Test Driven Development) 두 가지를 모두 잘 해야 한다. 둘 중에 하나를 선택해야 한다고 하면, 당연히 TDD가 될 것이다. 테스트 코드를 만들어놓고 테스트 코드에 걸맞는 양의 코드만 생산을 하는 것임. 사용하는 코드만 만들기 (Caller Create) 리팩토링 (Refactoring) 코드 읽기 (Code Review) 이 세 가지 원칙만 꼭 기억하자. PR에서 변수명, 함수명도 제안하는 것이 매우 좋음. 대부분 코드리뷰에서 버그를 찾으려고 하는데, 보통의 경우 눈으로 봤을 때 버그를 찾을 수 있으면 매우 이상한 것임. 그러진 말고 ‘짧고 간결하게’ 어떤 작업을 했는지 적는 것이 좋음. 1.6 적절한 논리력 알고리즘과 데이터 구조 단순한 디자인 진화적 디자인 협업 기술 벤치마킹 보통 가장 복잡한 케이스부터 고려를 많이 한다. 그러다보면 디자인도 매우 복잡해진다. 대부분의 경우 복잡한 문제들은 매우 작은 문제들의 집합이기 때문에, 복잡한 문제를 처음부터 고민한 것에 비해 작은 문제들부터 작성하는 것이 더 좋음. 코드 리뷰를 할 때는 공격적인 피드백이 매우 중요함. 그리고 계속해서 새로운 기술, 문제를 하기 위해 더 좋은 기술이 있는지 벤치마킹 해야 한다. 1.7 실천법 꾸준한 연습(Daily Practice) 매일 몸값 올리는 시간을 가져라. 취준 입장에서 우리의 동기부여가 무엇일까? 회사에서는 연봉올리는 것 등. 이런 동기가 있으면 시간 투자를 할 이유가 있다. 우리를 동기부여해주는 동기가 무엇인지 찾아보자. 멀리 가고 싶다면 함께 가라 혼자 실천하는 것은 어려운데 동료와 함께면 쉽다. 하기싫어하는 사람 아무나 붙잡고 하면 당연히 안됨! 현재 필요한 만큼만 하라 간단하게 하라 무엇을 하나 만들어야 한다고 하면 미리 하지 말고 현재 딱 필요한 만큼만 해라. 2. 좋은 개발자???2.1 좋은 OO 개발자“좋은” OO 개발자 (서버, 웹, 클라이언트, 임베디드, 모바일, 게임 등..) 불과 몇 년 전까지만 해도 AI 개발자는 취업할 곳이 없었음. 이렇듯 분야가 다양하고 시간 변동이 크다. 어느순간 표준화된 AI 모델이 나오고 나서 파인튜닝하는 것처럼 특별해지지 않을 수 있음. 나는 AI가 전부다! 라는 생각은 너무 위험함. 몇 년 뒤에는 너무 보편적인 기술이 될 수도 있기 때문. 도메인과 함께 망할 수 있기 떄문에 나는 어떤 개발자가 되려는 것에 유동적일 수 있어야 한다. 물론 도메인에 특별한 전문성이 있는 것도 중요하지만 리스크가 있음. “좋은” 이라는 것은 공유와 협업이 잘 이루어지는 사람을 뜻한다. 협업 전혀 안되는데 코딩 졸라 잘하면 그 팀 내에서 주변에 있던 모든 사람들이 회사를 뛰쳐 나간다. 회사 입장에서는 굉장히 위험한 일이다. 그래서 많은 회사들이 협업 잘하는 개발자를 원하게 된다. 즉 좋은 개발자라는 것은 공유를 잘하고 협업이 잘 이루어지는 영역 내에서 말할 수 있다. 2.2 공유하는 이유잘난척하는 거 절대 아님. 공유를 하는 가장 중요한 이유는, 주변이 똑똑해져야 내가 편해지기 때문이다. 사고를 수습하는 일이 줄어듬 중요한 일을 할 여유를 가질 수 있음. 주변이 똑똑하지 않으면 나의 일은 그 사람들이 친 사고를 급급하게 해결하는 것에 국한될 것이다. 그렇기 때문에 주변을 나보다 똑똑하게 만들기 위해 투자하는 것이 좋음. 투자를 하고 나면 좋은 평판을 얻을 수 있고, 그 좋은 평판은 나를 밥먹여준다. 이직을 할 때라던지. 좋은 평판을 받았다고 하면 주변의 추천도 많이 들어오고, 주변의 덕을 볼 확률이 올라간다. 2.3 공유 대상무엇이든. 잘난 것만 공유해야 할 것 같지만, 실패를 공유하는 것도 중요하다. 내가 이렇게 해봤더니 개망했어. 라는 이야기를 공유하면 다른 사람들은 그 방법대로 하지 않을 수 있기 때문이다. 실패했다는 것은 창피한 것이 아니고, 그만큼 투자를 해서 값진 시행착오를 얻게 된 것이다. 새로운 기술을 공유했다라고 하면 그 기술을 시도해보고 장단점에 대해 명확하게 설명할 수 있어야 하는 정도는 되어야 한다. 2.4 협업의 전제조건: 상대를 이해하자고슴도치도 제 새끼는 함함하다. 기획자의 입장에서, 기획자는 고객들이 원하는 것을 바탕으로 디자인을 만들거나 문서를 만들어낸다. 이러한 기획문서의 산출물을 개발자 QA 마케터 등 모두에게 공유하게 된다. 대부분 오 이거 쩐다!라고 하면 좋겠지만, 대부분 그렇지 않고 ‘이거 왜 해야 돼요?’라는 말을 듣게 된다. 기획 아이디어를 외부에 공유했는데 칭찬은 커녕 욕만 들으면 협업하기 빡친다. 개발자의 입장에서, 코드라는 산출물을 공유했는데 ‘이거 이상해요! 버그 있어요!’라는 말을 많이 하게 되는데, 이것도 개빡침. 열 개 잘되고 하나 안되면 열 개 잘된건 무시하고 하나 안된거에 대해서 콕 지적하고 지랄임. QA의 입장에서, 테스트케이스나 버그레포트를 썼는데 ‘그럴리가 없는데?’와 같은 반응을 얻게 됨. 2.5 협업의 필수요소누구든 산출물에 대해 ‘이상하다’라는 말을 듣게 되면 이 새끼가 날 공격하나? 라는 생각이 들 수 밖에 없음. 자아존중감 자신이 존중받을 가치가 있다고 믿음. 있는 그대로의 자신을 인정함. 타인의 부정적 견해에 크게 영향 받지 않음. 결국에 협업을 잘하기 위해서는 자아존중감이 있어야 한다. 실패를 하건 실수를 하건 성공적인 일을 했건 내가 누구보다 위대하고 누구보다 쓰레기고 이런게 절대 아니다. 어떤 상황에서든 나는 나를 사랑할 수 있는 믿음이 있어야 함. 이것이 높으면 사람들이 협업이 잘 된다. 보는 개인의 입장에서 잘 상처받지 않고 상처받더라도 쉽게 회복할 수 있음. 우리는 앞으로 사고 졸라 칠거고 그것이 전혀 나쁜것이 아님. 지극히 정상적인 것인데 그런 사고들을 잘 겪고 이겨낼 수 있어야 한다. 3. 좋은 개발자!!! 이미지출처: http://ifather.tistory.com/category/재밌는세상?page=2","link":"/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EB%A1%9C-%EC%82%B0%EB%8B%A4%EB%8A%94-%EA%B2%83-%EC%9C%A0%EC%84%9D%EB%AC%B8-CTO/"},{"title":"통계학 맛보기","text":"모수란? 통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표이다. 유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하다. 때문에 근사적으로 확률분포를 추정할 수밖에 없다. 데이터가 특정 확률분포를 따른다고 선험적(a priori)으로 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적 방법론이라고 한다. 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면, 그것을 비모수(nonparametric) 방법론이라 부른다. 기계학습의 많은 방법론은 비모수 방법론에 속한다. 모수가 무한히 많거나 모수가 데이터에 따라 바뀌는 것이지, 비모수방법론에는 모수가 없다는 것이 아니다. 확률분포 가정하기(예제) 우선 히스토그램을 통해 모양을 관찰한다. 데이터가 2개의 값(0 또는 1)만 가지는 경우 -&gt; 베르누이분포 데이터가 n개의 이산적인 값을 가지는 경우 -&gt; 카테고리분포 데이터가 [0, 1]사이에서 값을 가지는 경우 -&gt; 베타분포 데이터가 0 이상의 값을 가지는 경우 -&gt; 감마분포, 로그정규분포 등 데이터가 R 전체에서 값을 가지는 경우 -&gt; 정규분포, 라플라스분포 등 데이터로 모수 추정 데이터의 확률분포를 가정했다면, 모수를 추정해볼 수 있다. 정규분포의 모수는 평균과 분산으로, 이를 추정하는 통계량은 아래 식과 같다. 표본분산을 구할 때 N이 아니라 N - 1 로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서다. 통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 (데이터가 많아질수록) 정규분포를 따르게 된다. 이것을 중심극한정리라고 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립한다. 최대가능도 추정법 표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 된다. 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(maximum likelihood estimation, MLE)이다. 데이터집합 X가 독립적으로 추출되었을 경우엔 로그가능도를 최적화한다. 로그가능도를 사용하는 이유 로그가능도를 최적화하는 모수는 가능도를 최적화하는 MLE가 된다. 데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능해진다. 데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해진다. 경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 연산량을 O(N^2)에서 선형시간으로 줄일 수 있다. 대개 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화하게 된다.","link":"/%ED%86%B5%EA%B3%84%ED%95%99-%EB%A7%9B%EB%B3%B4%EA%B8%B0/"},{"title":"행렬이 뭐에요?","text":"행렬이란? 행렬은 벡터를 원소로 가지는 2차원 배열 행렬은 행과 열이라는 인덱스를 가진다. 행렬의 특정 행(열)을 고정하면 행(열)벡터라고 부른다. 전치행렬(transpose matrix)은 행과 열의 인덱스가 바뀐 행렬이다. 벡터가 공간에서 한 점이었다면, 행렬은 여러 점을 나타낸다. 행렬끼리 같은 모양을 가지면 덧셈, 뺄셈, 성분곱을 계산할 수 있다. 행렬곱은 i번째 행벡터와 j번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산한다. 행렬은 벡터공간에서 사용되는 연산자(operator)로 이해한다. 행렬곱을 통해 벡터를 다른 차원의 공간으로 보낼 수 있다. 행렬곱을 통해 패턴을 추출할 수 있고, 데이터를 압축할 수도 있다. 모든 선형변환(linear transform)은 행렬곱으로 계산할 수 있다. 역행렬 어떤 행렬 A의 연산을 거꾸로 되돌린 행렬을 역행렬이라 한다. 역행렬은 행과 열 숫자가 같고 행렬식(determinant)이 0이 아닌 경우에만 계산할 수 있다. 역행렬을 계산할 수 없다면 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose)역행렬을 이용한다. 유사역행렬을 이용하면 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있다.","link":"/%ED%96%89%EB%A0%AC%EC%9D%B4-%EB%AD%90%EC%97%90%EC%9A%94/"},{"title":"프리드리히 키틀러 - 유현주","text":"위 책은 학과장님이 직접 번역하신 책인 “프리드리히 키틀러”이다. 사인도 받았다. 정말 오랜만에 받는 책 선물이다. 교수님께서 좋아하는 학생들에게만 주는 거라고 하셨는데 빈말이라 할지라도 기분이 너무 좋다. 작가소개만 읽었는데, 교수님이 참 존경스럽다. 책이라는 것을 읽어볼 시기가 또 찾아온 것 같다. 얼른 읽고 리뷰도 써봐야지.","link":"/%ED%94%84%EB%A6%AC%EB%93%9C%EB%A6%AC%ED%9E%88-%ED%82%A4%ED%8B%80%EB%9F%AC-%EC%9C%A0%ED%98%84%EC%A3%BC/"},{"title":"협업을 위한 Git 사용 매뉴얼 순한맛","text":"사전준비 git init git config --global user.name &quot;YOUR_GITHUB_NAME&quot; git config --global user.email &quot;YOUR GITHUB_EMAIL&quot; 위 세 단계가 이루어지지 않으면 private 레포지토리에 접근할 수 없을 수도 있습니다! USAGE git clone &quot;YOUR_REPOSITORY&quot; 관리할 레포지토리를 clone. cd &quot;YOUR_REPOSITORY&quot; clone 받은 레포지토리로 change directory git checkout -b &quot;BRANCH_NAME&quot; 을 통해 브랜치를 생성하고 해당 브랜치로 전환. git add &amp; commit git push origin &quot;BRANCH_NAME&quot; add/commit 한 내역들을 BRANCH_NAME 브랜치에 push한다. push 가 되어있지 않다면, git pull origin &quot;BRANCH_NAME&quot; 을 시도해볼 것. 브라우저로 해당 레포지토리로 이동한 후, push한 브랜치로 변경한다. compare &amp; pull request 버튼을 클릭한다. pull request를 날리고 싶은 브랜치(head) 를 설정하고 pull request 버튼을 클릭한다. 다른 팀원들이 리뷰 및 커멘트를 날리고 리뷰가 끝났다면, Merge를 한다. 명령어 git checkout -b &quot;BRANCH_NAME&quot; BRANCH_NAME으로 된 브랜치를 생성하고 현재 브랜치를 생성한 브랜치로 변경한다. git checkout &quot;BRANCH_NAME&quot; 현재 브랜치를 “이미 생성되어 있는 BRANCH_NAME”으로 변경한다. git branch -d &quot;BRANCH_NAME&quot; BRANCH_NAME으로 되어있는 브랜치를 삭제한다. git remote -v 깃에 연결된 원격저장소를 확인한다. 추가예정 그 외 팁들 커밋메세지 템플릿 적용하기 touch ~/.gitmessage.txt gitmessage.txt 파일을 생성한다. vim 등의 에디터로 gitmessage.txt 파일에 아래 내용을 복사&amp;붙여넣기 한다. 123456789101112131415161718192021222324252627# &lt;타입&gt;: &lt;제목&gt;##### 제목은 최대 50 글자까지만 입력 ############## -&gt; |# 본문은 위에 작성######## 본문은 한 줄에 최대 72 글자까지만 입력 ########################### -&gt; |# 꼬릿말은 아래에 작성: ex) #이슈 번호# --- COMMIT END ---# &lt;타입&gt; 리스트# feat : 기능 (새로운 기능)# fix : 버그 (버그 수정)# refactor: 리팩토링# style : 스타일 (코드 형식, 세미콜론 추가: 비즈니스 로직에 변경 없음)# docs : 문서 (문서 추가, 수정, 삭제)# test : 테스트 (테스트 코드 추가, 수정, 삭제: 비즈니스 로직에 변경 없음)# chore : 기타 변경사항 (빌드 스크립트 수정 등)# ------------------# 제목 첫 글자를 대문자로# 제목은 명령문으로# 제목 끝에 마침표(.) 금지# 제목과 본문을 한 줄 띄워 분리하기# 본문은 &quot;어떻게&quot; 보다 &quot;무엇을&quot;, &quot;왜&quot;를 설명한다.# 본문에 여러줄의 메시지를 작성할 땐 &quot;-&quot;로 구분# ------------------ git config --global commit.template ~/.gitmessage.txt 해당 gitmessage.txt를 전역 커밋 템플릿으로 설정한다. Alias 적용하기. 아래와 같이 alias 를 (맥이라면 ./zshrc) 저장하면 단축어로 사용할 수 있다.","link":"/%ED%98%91%EC%97%85%EC%9D%84-%EC%9C%84%ED%95%9C-Git-%EC%82%AC%EC%9A%A9-%EB%A7%A4%EB%89%B4%EC%96%BC-%EC%88%9C%ED%95%9C%EB%A7%9B/"},{"title":"확률론 맛보기","text":"딥러닝에서 확률론이 필요한 이유 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 둔다. 손실함수 등의 작동원리가 데이터 공간을 통계적으로 해석해서 유도하기 때문. 회귀분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습 분류문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습 분산 및 불확실성을 최소화하기 위해서는 측정 방법을 알아야한다. 확률분포 확률분포란 데이터 공간에서 데이터를 추출하는 분포이다. 데이터는 확률변수로 (x, y) ~ D 라고 표기한다. 결합분포 P(x, y)는 D를 모델링한다. D는 이론적으로 존재하는 확률분포이기 때문에 사전에 알 수 없다. 확률변수이산확률변수 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다. 연속확률변수 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링한다. 밀도함수는 누적확률분포의 변화율을 모델링한 것으로 확률로 해석해서는 안된다. 조건부확률과 기계학습 조건부확률 P(y|x)는 입력변수 x에 대해 정답이 y일 확률을 의미한다. 로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용된다. 회귀문제의 경우 조건부기대값을 추정한다. 기대값은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다. 딥러닝은 다층신경망(MLP)를 사용하여 데이터로부터 특징패턴을 추출한다. 특징패턴을 위해 어떤 손실함수를 사용할지는 기계학습 문제와 모델에 의해 결정된다. 몬테카를로 샘플링 대부분 기계학습의 문제들은 확률분포를 명시적으로 모른다. 확률분포를 모를 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방법을 사용해야 한다. 몬테카를로 샘플링은 독립추출만 보장된다면 대수의 법칙에 의해 수렴성을 보장한다.","link":"/%ED%99%95%EB%A5%A0%EB%A1%A0-%EB%A7%9B%EB%B3%B4%EA%B8%B0/"}],"tags":[{"name":"AI, 대회, 협업, 플래닝가이드","slug":"AI-대회-협업-플래닝가이드","link":"/tags/AI-%EB%8C%80%ED%9A%8C-%ED%98%91%EC%97%85-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/"},{"name":"42Seoul","slug":"42Seoul","link":"/tags/42Seoul/"},{"name":"42서울","slug":"42서울","link":"/tags/42%EC%84%9C%EC%9A%B8/"},{"name":"42서울 본과정","slug":"42서울-본과정","link":"/tags/42%EC%84%9C%EC%9A%B8-%EB%B3%B8%EA%B3%BC%EC%A0%95/"},{"name":"42서울 후기","slug":"42서울-후기","link":"/tags/42%EC%84%9C%EC%9A%B8-%ED%9B%84%EA%B8%B0/"},{"name":"La Piscine","slug":"La-Piscine","link":"/tags/La-Piscine/"},{"name":"이너서클","slug":"이너서클","link":"/tags/%EC%9D%B4%EB%84%88%EC%84%9C%ED%81%B4/"},{"name":"이노베이션아카데미","slug":"이노베이션아카데미","link":"/tags/%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8/"},{"name":"공통과정","slug":"공통과정","link":"/tags/%EA%B3%B5%ED%86%B5%EA%B3%BC%EC%A0%95/"},{"name":"Franz Liszt","slug":"Franz-Liszt","link":"/tags/Franz-Liszt/"},{"name":"초절기교 연습곡","slug":"초절기교-연습곡","link":"/tags/%EC%B4%88%EC%A0%88%EA%B8%B0%EA%B5%90-%EC%97%B0%EC%8A%B5%EA%B3%A1/"},{"name":"열정","slug":"열정","link":"/tags/%EC%97%B4%EC%A0%95/"},{"name":"Papago NMT API","slug":"Papago-NMT-API","link":"/tags/Papago-NMT-API/"},{"name":"카카오톡","slug":"카카오톡","link":"/tags/%EC%B9%B4%EC%B9%B4%EC%98%A4%ED%86%A1/"},{"name":"챗봇","slug":"챗봇","link":"/tags/%EC%B1%97%EB%B4%87/"},{"name":"Teachable Machine","slug":"Teachable-Machine","link":"/tags/Teachable-Machine/"},{"name":"Kakao I Open Builder","slug":"Kakao-I-Open-Builder","link":"/tags/Kakao-I-Open-Builder/"},{"name":"Word2Vec","slug":"Word2Vec","link":"/tags/Word2Vec/"},{"name":"시각화","slug":"시각화","link":"/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"},{"name":"WaveNet","slug":"WaveNet","link":"/tags/WaveNet/"},{"name":"42 Seoul","slug":"42-Seoul","link":"/tags/42-Seoul/"},{"name":"Tech Seminar","slug":"Tech-Seminar","link":"/tags/Tech-Seminar/"},{"name":"VoyagerX","slug":"VoyagerX","link":"/tags/VoyagerX/"},{"name":"남세동","slug":"남세동","link":"/tags/%EB%82%A8%EC%84%B8%EB%8F%99/"},{"name":"딥러닝","slug":"딥러닝","link":"/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"},{"name":"보이저엑스","slug":"보이저엑스","link":"/tags/%EB%B3%B4%EC%9D%B4%EC%A0%80%EC%97%91%EC%8A%A4/"}],"categories":[{"name":"Programming","slug":"Programming","link":"/categories/Programming/"},{"name":"für Musik","slug":"fur-Musik","link":"/categories/fur-Musik/"},{"name":"42 Life","slug":"42-Life","link":"/categories/42-Life/"},{"name":"Paper Review","slug":"Paper-Review","link":"/categories/Paper-Review/"},{"name":"Tips","slug":"Programming/Tips","link":"/categories/Programming/Tips/"},{"name":"Competition","slug":"Competition","link":"/categories/Competition/"},{"name":"Study","slug":"Study","link":"/categories/Study/"},{"name":"Boostcamp AI Tech","slug":"Boostcamp-AI-Tech","link":"/categories/Boostcamp-AI-Tech/"},{"name":"Network","slug":"Study/Network","link":"/categories/Study/Network/"},{"name":"Algorithm","slug":"Study/Algorithm","link":"/categories/Study/Algorithm/"},{"name":"AI","slug":"Study/AI","link":"/categories/Study/AI/"},{"name":"NLP","slug":"Study/NLP","link":"/categories/Study/NLP/"},{"name":"Math for AI","slug":"Study/Math-for-AI","link":"/categories/Study/Math-for-AI/"},{"name":"Python","slug":"Study/Python","link":"/categories/Study/Python/"},{"name":"Master Class","slug":"Boostcamp-AI-Tech/Master-Class","link":"/categories/Boostcamp-AI-Tech/Master-Class/"},{"name":"Project","slug":"Programming/Project","link":"/categories/Programming/Project/"},{"name":"Problem Solving","slug":"Programming/Problem-Solving","link":"/categories/Programming/Problem-Solving/"},{"name":"MRC","slug":"Study/NLP/MRC","link":"/categories/Study/NLP/MRC/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"},{"name":"C","slug":"Study/C","link":"/categories/Study/C/"},{"name":"Visualization","slug":"Study/Visualization","link":"/categories/Study/Visualization/"},{"name":"Yonsei","slug":"Yonsei","link":"/categories/Yonsei/"}]}