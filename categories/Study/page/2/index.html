<!doctype html>
<html lang="ko"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>카테고리: Study - YOHAI</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Yohan Lee"><meta name="msapplication-TileImage" content="/image/avatar.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Yohan Lee"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta property="og:type" content="blog"><meta property="og:title" content="YOHAI"><meta property="og:url" content="https://l-yohai.github.io/"><meta property="og:site_name" content="YOHAI"><meta property="og:locale" content="ko_KR"><meta property="og:image" content="https://l-yohai.github.io/image/og_image.jpeg"><meta property="article:author" content="Yohan Lee"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/image/og_image.jpeg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://l-yohai.github.io"},"headline":"YOHAI","image":[],"author":{"@type":"Person","name":"Yohan Lee"},"publisher":{"@type":"Organization","name":"YOHAI","logo":{"@type":"ImageObject","url":"https://l-yohai.github.io/image/logo.png"}},"description":""}</script><link rel="alternate" href="/rss.xml" title="YOHAI" type="application/atom+xml"><link rel="icon" href="/image/avatar.jpg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=Jua:wght@400&amp;family=Source+Code+Pro" rel="stylesheet"><link rel="preconnect" href="https://fonts.gstatic.com"><link href="https://fonts.googleapis.com/css2?family=Nanum+Gothic:wght@700&amp;family=Source+Code+Pro" rel="stylesheet"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-FE2M7J88MT" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-FE2M7J88MT');</script><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.2"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/image/logo.png" alt="YOHAI" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/l-yohai"><i class="fab fa-github"></i></a><a class="navbar-item search" title="검색" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">카테고리</a></li><li class="is-active"><a href="#" aria-current="page">Study</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-12T09:21:46.000Z" title="2021. 8. 12. 오후 6:21:46">2021-08-12</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:23:28.000Z" title="2021. 8. 22. 오후 6:23:28">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/AI/">AI</a></span><span class="level-item">6분안에 읽기 (약 905 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Computer-Vision-Applications-Semantic-Segmentation-and-Detection/">Computer Vision Applications (Semantic Segmentation and Detection</a></h1><div class="content"><h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><p><img src="/image/image-20210812074753902.png" alt></p>
<p>이미지 픽셀 별 분류 과제이다. Dense Classification, Perl Pixel 등으로 불리기도 한다.</p>
<ul>
<li><p>Fully Convolutional Network</p>
<ul>
<li><p>output이 1000개의 채널이라고 하면, Dense Layer를 없애고 Fully Convolutional Network 로 변경하려는 것임.</p>
</li>
<li><p>Fully Connected (Dense) Layer를 사용하는 것과 결과적으로 똑같음. 파라미터도 완전히 똑같음.</p>
<p><img src="/image/image-20210812075323907.png" alt></p>
</li>
<li><p>이러한 과정을 convolutionalization 이라고 한다.</p>
<ul>
<li>Transforming fully connected layers into convolution layers enables a classification net to output a heat map.</li>
</ul>
</li>
<li><p>While FCN can run with inputs of any size, the output dimensions(special dimension) are typically reduced by subsampling. So we need a way to connect the coarse output to the dense pixels.</p>
</li>
</ul>
</li>
<li><p>Deconvolution (conv transpose)</p>
<ul>
<li><p>직관적으로 보았을 때 Convolution의 역 연산이다.</p>
<p><img src="/image/image-20210812102230098.png" alt></p>
</li>
<li><p>special dimension을 키워주는 역할을 하게 된다.</p>
</li>
<li><p>엄밀히 말하면 convolution의 역 연산은 존재할 수 없다. 원래의 값으로 완전히 복원할 수는 없기 때문이다. 하지만 이렇게 생각하면 좋은 이유는 네트워크 구조를 짤 때 파라미터 크기와 네트워크 입출력을 계산할 때 역으로 생각하면 편하기 때문이다.</p>
</li>
</ul>
</li>
</ul>
<h2 id="Detection"><a href="#Detection" class="headerlink" title="Detection"></a>Detection</h2><p>Semantic Segmentation 이랑 비슷하지만, 퓨어픽셀을 이용한 히트맵을 찾는 것이 아니라 객체들의 bounding box를 찾는 과제로 만든 것이다.</p>
<ul>
<li>R-CNN<ol>
<li>R-CNN takes an input image,</li>
<li>extracts around 2,000 region proposals (using Selective search),</li>
<li>compute features for each proposal (using AlexNet),</li>
<li>and then classifies with lenear SVMs.</li>
</ol>
</li>
<li>SPPNet<ul>
<li>RCNN의 가장 큰 문제는 이미지에서 2000개의 바운딩박스를 뽑았을 때, 2000개의 이미지 혹은 패치를 모두 CNN에 통과시켜야 한다. 즉 하나의 이미지를 처리하는데 CPU에서는 1분의 시간이 소요되었었다.</li>
<li>이미지 전체에 대한 컨볼루션 피쳐맵을 만들고, 뽑힌 바운딩박스 위치에 해당하는 컨볼루션 피쳐맵의 텐서만 뽑아온 것으로 CNN에 통과시킴으로써 훨씬 빨라지며, 한 번의 CNN으로 결과를 얻을 수 있다.</li>
</ul>
</li>
<li>Fast R-CNN<ul>
<li><img src="/image/image-20210812103103755.png" alt></li>
<li>기본 컨셉은 SPPNet과 굉장히 유사하지만, 뒷단의 Neural Net(RoI)을 통해서 바운딩박스 리그레션과 분류를 진행했다.</li>
</ul>
</li>
<li>Faster R-CNN<ul>
<li>이미지를 통해 바운딩박스를 뽑아내는 Region Proposal 역시 학습을 시키자는 것임.</li>
<li><img src="/image/image-20210812103228562.png" alt></li>
<li>Region Proposal Network<ul>
<li>이미지가 있으면 이미지 속 특정 영역(패치)이 바운딩박스로서 의미가 있는지 없는지(즉 영역에 물체가 있는지)를 판단하는 것이다.</li>
<li>anchor boxes가 필요한데, 이것은 미리 정해놓은 바운딩박스의 크기이다. 내가 이 영역에 어떤 크기의 물체가 있을 것 같다라는 예상을 전제로 만들어 놓은 것임.</li>
<li>Fully Conv 레이어가 사용됨.</li>
</ul>
</li>
<li>YOLO<ul>
<li>extremely fast object detection algorithm</li>
<li>It simultaneously predicts multiple bounding boxes and class probabilities.<ul>
<li>No explicit bounding box sampling (compared with Faster R-CNN)</li>
</ul>
</li>
<li><img src="/image/image-20210812103801983.png" alt><ul>
<li>이미지 안에 찾고싶은 물체의 중앙이 그리드 안에 들어가면, 그 그리드 셋이 해당 물체의 바운딩 박스와 해당 물체가 무엇인지 같이 예측을 해야 한다.</li>
<li>이 때 B개의 바운딩 박스를 예측하게 된다. 이후 바운딩박스의 refinement(x/y/w/h)를 예측하고, 각각의 그리드셋이 속하는 중점의 오브젝트가 어떤 클래스인지 예측한다.</li>
<li>이 두 가지 정보를 취합하게 되면 박스와 박스가 어떤 클래스인지 예측할 수 있다.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div><a class="article-more button is-small is-size-7" href="/Computer-Vision-Applications-Semantic-Segmentation-and-Detection/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-11T09:20:36.000Z" title="2021. 8. 11. 오후 6:20:36">2021-08-11</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:21:14.000Z" title="2021. 8. 22. 오후 6:21:14">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/AI/">AI</a></span><span class="level-item">10분안에 읽기 (약 1488 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Convolution-intro/">Convolution 소개</a></h1><div class="content"><p>파라미터 수 손으로 직접 계산해보기</p>
<p><img src="/image/image-20210811102654457.png" alt></p>
<p>알렉스넷의 파라미터 수 구하기</p>
<p><img src="/image/image-20210811103940850.png" alt></p>
<p>input = 224 <em> 224 </em> 3<br>filter = 11 <em> 11 </em> 3<br>fisrt param = $11<em>11</em>3<em>48</em>2 ~= 35k$</p>
<h1 id="Modern-CNN"><a href="#Modern-CNN" class="headerlink" title="Modern CNN"></a>Modern CNN</h1><h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>네트워크가 두 개로 나뉘어져 있다. 당시 GPU가 부족했기 때문임. 두 장의 GPU로 학습하고 합쳤다. 인풋에 $11 * 11$ 필터를 사용했는데, 파라미터 관점에서 좋진 않다. 상대적으로 많은 파라미터가 필요하기 때문. 이후 5개의 Convolution 레이어와 3개의 Dense 레이어를 사용한다. 최근 200-300개 네트워크를 가진 신경망에 비하면 Light한 편이다.</p>
<h4 id="Key-Ideas"><a href="#Key-Ideas" class="headerlink" title="Key Ideas"></a>Key Ideas</h4><ul>
<li>Rectified Linear Unit(ReLU) 활성화 함수를 사용했다. $ReLU = max(0, x)$<ul>
<li>리니어 모델이 갖는 좋은 성질들을 가질 수 있게 한다.</li>
<li>리니어 모델들의 성질을 가지고 있기 때문에 학습하는 SGD나 Gradient Descent로 학습을 용이하게 한다.</li>
<li>활성화함수를 사용할 때 이전에 많이 활용하던 Sigmoid나 tanh 같은 경우 값이 커지면 슬로프가 줄어들게 된다. 슬로프가 결국 기울기니까, 뉴런의 값이 많이 크면(0에서 벗어나면) Gradient Slope는 0에 가까워진다. 이 때 Vanishing Gradient 문제가 발생할 수 있는데, ReLU를 사용하면 해당 문제를 해소할 수 있음</li>
</ul>
</li>
<li>GPU Implementation (2 GPUs)</li>
<li>Local Response Normalization(어떠한 입력 공간에서 Response가 많이 나오는 부분을 죽이는 것임. 최근엔 많이 사용되지 않음), Overlapping pooling</li>
<li>Data Augmentation</li>
<li>Dropout</li>
</ul></div><a class="article-more button is-small is-size-7" href="/Convolution-intro/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-10T09:19:10.000Z" title="2021. 8. 10. 오후 6:19:10">2021-08-10</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:19:53.000Z" title="2021. 8. 22. 오후 6:19:53">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/AI/">AI</a></span><span class="level-item">25분안에 읽기 (약 3725 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Optimization-intro/">Optimization 소개</a></h1><div class="content"><p>“Language is the source of misunderstandings” - Antoine de Saint-Exupery (1900-1944)</p>
<p>용어에 대한 정리를 하고 넘어가야 이후에 오해가 생기지 않는다. 용어 통일이 가장 중요하기 때문에 용어에 대한 명확한 컨셉을 잡아볼 것임.</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><ul>
<li>First-order iterative optimization algorithm for finding a local minimum of a differentiable function.</li>
<li>찾고자 하는 파라미터에 대해서 손실함수를 미분한 편미분값을 가지고 학습을 하겠다는 것임.</li>
<li>Local minimum: 손실함수를 미분했을 때 극소적으로 좋은 로컬 미니멈을 찾는 것을 목적으로 함.</li>
</ul>
<h2 id="Important-Concepts-in-Optimization"><a href="#Important-Concepts-in-Optimization" class="headerlink" title="Important Concepts in Optimization"></a>Important Concepts in Optimization</h2><ul>
<li><p>Generalization (일반화)</p>
<ul>
<li>대부분의 경우 일반화 성능을 높이는 것이 목적이다.</li>
<li>일반화 성능을 높이기만 하면 좋은 것인가?<ul>
<li>일반적으로 학습을 시키게 되면 Iteration이 지나면서 Training Error를 줄이게 되는데, Training Error가 0이 되었다고 해서 우리가 항상 원하는 최적값에 도달했다는 보장이 없다.</li>
<li>일반적으로 Training error가 주어지지만, 어느정도 시간이 지나고 나면 Test Error가 커짐, 즉 학습에 사용하지 않은 데이터에 대해서는 성능이 오히려 떨어지게 된다.</li>
<li>즉, 일반적으로 일반화는 테스트에러와 트레이닝 에러의 갭을 줄이는 것을 의미한다.</li>
<li>만약 우리 네트워크가 안좋아서 학습데이터에 대한 성능이 안좋으면, 일반화의 퍼포먼스가 좋다고 해서 테스트 데이터의 성능이 좋다고 말할 수 없다.</li>
<li>일반화의 성능은 테스트와 트레이닝 에러의 갭을 말하기 때문이다.</li>
</ul>
</li>
</ul>
</li>
<li><p>Under-fitting vs. over-fitting</p>
<ul>
<li><p>학습데이터에 대해서는 잘 동작을 하지만 테스트데이터에 대해서 잘 동작하지 않는 것을 OverFitting이라고 한다.</p>
<p><img src="/image/image-20210810115422718.png" alt></p>
</li>
</ul>
</li>
<li><p>Cross validation</p>
<ul>
<li>일반적으로 데이터가 분리되어 있음 (Train, Test, Validate)</li>
<li>보통은 학습 데이터로 학습시킨 모델이 학습에 사용되지 않은 밸리데이션 데이터를 기준으로 학습이 잘 되었는지 확인하는 과정이다.</li>
<li>하지만, 트레인과 테스트를 반반으로 했을 때, 트레인을 80 테스트를 20으로 했을 때 등 Split 범위에 따라서 학습의 퍼포먼스가 달라질 수 있다. 이러한 것을 해결하고자 하는 것이 Cross Validation이다. (혹은 K-Fold Validation)</li>
<li>네트워크를 학습할 때 수많은 파라미터가 존재한다. 또한 Hyper Parameter들에 대한 기준이 없다. 따라서 Cross-validation을 통해 최적의 Parameter Sets 을 찾고, 파라미터를 보정한 상태에서 모든 학습데이터를 학습에 사용해야 더 많은 데이터를 사용할 수 있다.</li>
<li>Test Data는 어떠한 방법으로든 학습에 사용되어서는 안된다. 즉, 학습에는 Train과 Validate 데이터만을 이용해야 한다.</li>
</ul>
</li>
<li><p>Bias-variance tradeoff</p>
<ul>
<li><p><img src="/image/image-20210810115942627.png" alt></p>
</li>
<li><p>사격을 할 때 항상 같은 곳에만 찍히면 (원점이 아니더라도) 좋은 것이다. 전체 모델을 어느정도 평준화 시키면 최적값에 도달할 가능성이 많기 때문이다. 이것을 Low Variance라고 한다. (출력이 얼마나 일관적으로 나오는 지)</p>
</li>
<li><p>이것에 반해 Variance가 크면 비슷한 입력이 들어와도 출력이 많이 달라지는 것을 의미한다.</p>
</li>
<li><p>Bias라는 것은 출력에 따른 평균적으로 True Target에 접근하는 정도를 의미하며, Bias가 크면 Mean에서 많이 벗어나는 것을 의미한다.</p>
</li>
<li><p>학습 데이터에 Noise가 껴있다고 가정했을 때 내가 이 노이즈가 껴있는 타겟 데이터를 미니마이즈하는 것에 대하여 세 파트로 나뉘어진다. 내가 미니마이즈하는 것은 하나의 값(cost)이지만, 그 값은 세 파트로 나뉘어져 있어서 한 파트를 줄이면 다른 파트가 커질 수밖에 없는 Trade-off 관계이다.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[(t-\hat{f})^{2}\right] &=\mathbb{E}\left[(t-f+f-\hat{f})^{2}\right] \\
&=\ldots \\
&=\mathbb{E}\left[\left(f-\mathbb{E}[\hat{f}]^{2}\right)^{2}\right]+\mathbb{E}\left[(\mathbb{E}[\hat{f}]-\hat{f})^{2}\right]+\mathbb{E}[\epsilon]
\end{aligned}</script><p>위 식에서 cost를 minimize한다는 것은 bias와 variance, noise를 minimize 한다는 것을 의미한다.</p>
</li>
</ul>
</li>
<li><p>Bootstrapping</p>
<ul>
<li>신발끈을 말하는 것이며, 그 뜻은 신발끈을 들어올려 하늘을 날겠다라는 허황된 뜻임.</li>
<li>딥러닝에서는 학습 데이터가 100개가 있다고 하면, 100개 중 80개를 사용하겠다 등 일부만 뽑아서 사용한다는 것을 의미한다.</li>
<li>이렇게 했을 때 하나의 입력에 대해서 각 모델들의 출력이 달라질 수 있다. 이 때 각 모델들의 예측값들이 얼마나 일치를 이루는지 보고 전체 모델들의 특성을 알고자 하는 것이다.</li>
<li>학습데이터가 고정되어 있을 때 subsampling을 통해 여러 학습데이터를 만들고 그것을 이용하여 여러 모델 혹은 metric을 만들겠다는 것임.</li>
</ul>
</li>
<li><p>Baggin and boosting</p>
<ul>
<li>Bagging(Bootstrapping aggregating)</li>
<li>학습데이터를 여러 개를 만들어서 (Booststrapping) 여러 모델을 만들고 그 모델의 output들을 가지고 평균을 내겠다는 뜻임.</li>
<li>학습데이터가 고정되어 있으면 이것을 모두 이용하여 하나의 결과를 내는 것이 좋을 것 같지만 사실은 그렇지 않다. N개의 모델을 만들고, N개의 모델의 출력값의 평균이나 Voting을 통해 나오는 결과를 사용하는 것이 보통 성능이 더 좋다.</li>
<li>Boosting: 학습데이터를 시퀀셜하게 바라봐서 간단한 모델을 만들고, 모델에 학습데이터를 다 넣어본다. 모델에서 80개는 잘 예측을 했지만 20개는 잘 안되었다. 이 때 모델을 하나를 더 만드는데 전에 안되었던 20개에 데이터를 잘 예측하는 모델을 만든다. 이렇게 모델들을 만들고 합친 이후에 하나의 struct learner를 만들고, 그 weight를 찾는 방식을 말한다.</li>
</ul>
</li>
</ul>
<h2 id="Practical-Gradient-Descent-Methods"><a href="#Practical-Gradient-Descent-Methods" class="headerlink" title="Practical Gradient Descent Methods"></a>Practical Gradient Descent Methods</h2><h3 id="Stochastic-gradient-Descent"><a href="#Stochastic-gradient-Descent" class="headerlink" title="Stochastic gradient Descent"></a>Stochastic gradient Descent</h3><ul>
<li>하나의 샘플을 이용</li>
</ul></div><a class="article-more button is-small is-size-7" href="/Optimization-intro/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-09T09:17:29.000Z" title="2021. 8. 9. 오후 6:17:29">2021-08-09</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:17:54.000Z" title="2021. 8. 22. 오후 6:17:54">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/AI/">AI</a></span><span class="level-item">몇 초안에 읽기 (약 100 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/Multi-Layer-Perceptron/">뉴럴 네트워크 - Multi Layer Perceptron</a></h1><div class="content"><blockquote>
<p>Neural networks are computing systems vaguely inspired by the biological neural networks that constitute animal brains.</p>
</blockquote>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \operatorname{loss}}{\partial w} &=\frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2} \\
&=\frac{\partial}{\partial w} \frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-w x_{i}-b\right)^{2} \\
&=-\frac{1}{N} \sum_{i=1}^{N}-2\left(y_{i}-w x_{i}-b\right) x_{i}
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
&w \leftarrow w-\eta \frac{\partial \operatorname{loss}}{\partial w} \\
&b \leftarrow b-\eta \frac{\partial l o s s}{\partial b}
\end{aligned}</script></div><a class="article-more button is-small is-size-7" href="/Multi-Layer-Perceptron/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-08T07:59:21.000Z" title="2021. 8. 8. 오후 4:59:21">2021-08-08</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T07:59:41.000Z" title="2021. 8. 22. 오후 4:59:41">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">1분안에 읽기 (약 167 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/greek-alphabet/">그리스문자 정리</a></h1><div class="content"><table style="border-collapse: collapse; width: 100%; height: 500px;" border="1" data-ke-align="alignCenter" data-ke-style="style9"><tbody><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><b><span><span>문자</span></span></b></td><td style="width: 28.9535%; text-align: center; height: 20px;"><b><span><span>영어</span></span></b></td><td style="width: 26.6279%; text-align: center; height: 20px;"><b><span><span>한글</span></span></b></td><td style="width: 24.4186%; text-align: center; height: 20px;"><b><span><span>로마자 표기법</span></span></b></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Α α</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Alpha</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>알파</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>a</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Β β</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Beta</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>베타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>b</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Γ γ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Gamma</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>감마</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>gh, g, j</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Δ δ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Delta</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>델타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>d, dh</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ε ε</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Epsilon</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>엡실론</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>e</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ζ ζ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Zeta</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>제타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>z</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Η η</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Eta</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>에타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>i</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Θ θ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Theta</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>쎄타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>th</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ι ι</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Iota</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>요타</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>i</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Κ κ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Kappa</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>카파</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>k</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Λ λ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Lambda</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>람다</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>l</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Μ μ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Mu</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>뮤</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>m</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ν ν</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Nu</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>뉴</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>n</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ξ ξ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Xi</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>크시</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>x, ks</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ο ο</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Omicron</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>오미크론</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>o</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Π π</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Pi</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>파이</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>p</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ρ ρ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Rho</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>로</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>r</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Σ σ ς</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Sigma</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>시그마</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>s</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Τ τ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Tau</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>타우</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>t</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Υ υ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Upsilon</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>입실론</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>y, v, f</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Φ φ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Phi</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>피</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>f</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Χ χ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Chi</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>카이</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>ch, kh</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ψ ψ</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Psi</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>프사이</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>ps</span></span></td></tr><tr style="height: 20px;"><td style="width: 19.8837%; text-align: center; height: 20px;"><span><span>Ω ω</span></span></td><td style="width: 28.9535%; text-align: center; height: 20px;"><span><span>Omega</span></span></td><td style="width: 26.6279%; text-align: center; height: 20px;"><span><span>오메가</span></span></td><td style="width: 24.4186%; text-align: center; height: 20px;"><span><span>o</span></span></td></tr></tbody></table></div><a class="article-more button is-small is-size-7" href="/greek-alphabet/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-06T09:14:46.000Z" title="2021. 8. 6. 오후 6:14:46">2021-08-06</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:15:29.000Z" title="2021. 8. 22. 오후 6:15:29">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">2분안에 읽기 (약 306 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/RNN-first-step/">RNN 첫걸음</a></h1><div class="content"><h2 id="시퀀스-데이터"><a href="#시퀀스-데이터" class="headerlink" title="시퀀스 데이터"></a>시퀀스 데이터</h2><ul>
<li>소리, 문자열, 주가 등처럼 순차적으로 나타나는 데이터를 시퀀스 데이터로 분류한다.</li>
<li>독립동등분포(i.i.d.) 가정을 잘 위배하기 때문에 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포도 바뀌게 된다.</li>
</ul>
<script type="math/tex; mode=display">
P(X_1, ..., X_t) = P(X_t \mid X_1, ..., X_{t-1})*P(X_1,...,X_{t-1})\\=\prod_{s=1}^{t}P\left(X_{S} \mid X_{S-1}, \ldots, X_{1}\right)</script><ul>
<li><p>이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률분포를 다루기 위해 조건부확률을 이용할 수 있다.</p>
</li>
<li><p>시퀀스 데이터를 다루기 위해선 길이가 가변적인 데이터를 다룰 수 있는 모델이 필요하다.</p>
<p><img src="/image/image-20210806135133092.png" alt></p>
<p>$H_T = Net_\Theta(H_{t-1}, X_{t-1})$​</p>
</li>
<li><p>시퀀스 길이가 길어지면 BPTT를 통한 역전파 알고리즘의 계산이 불안정해지므로 길이를 끊는 것이 필요하다. (truncated BPTT)</p>
</li>
<li><p>이러한 문제들 때문에 Vanila RNN은 길이가 긴 시퀀스를 처리하는데 문제가 있다.</p>
<ul>
<li>이를 해결하기 위해 등장한 것이 LSTM과 GRU이다.</li>
</ul>
</li>
</ul>
</div><a class="article-more button is-small is-size-7" href="/RNN-first-step/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-06T09:14:43.000Z" title="2021. 8. 6. 오후 6:14:43">2021-08-06</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:15:17.000Z" title="2021. 8. 22. 오후 6:15:17">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">2분안에 읽기 (약 354 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/CNN-first-step/">CNN 첫걸음</a></h1><div class="content"><ul>
<li>지금까지 배운 다층신경망(MLP) 구조는 각 뉴런들이 선형모델과 활성함수로 모두 연결된 (fully connected) 구조였다.</li>
<li>Convolution 연산은 이와 달리 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다.</li>
<li>Convolution 연산의 수학적 의미는 신호(signal)을 커널을 이용해 국소적으로 증폭 또는 감소시켜서 정보를 추출 또는 필터링하는 것이다.</li>
<li>커널은 정의역 내에서 움직여도 변하지 않고(translation invariant) 주어진 신호에 국소적(local)으로 적용한다.</li>
</ul>
<h2 id="2차원-Convolution-연산"><a href="#2차원-Convolution-연산" class="headerlink" title="2차원 Convolution 연산"></a>2차원 Convolution 연산</h2><ul>
<li><p>커널을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조</p>
</li>
<li><p>입력크기를 (H, W), 커널 크기를 $(K_H, K_W)$, 출력 크기를 $(O_H, O_W)$ 라고 하면 출력 크기는 다음과 같이 계산한다.</p>
<script type="math/tex; mode=display">
O_H = H - K_H + 1\\
O_W = W - K_W + 1</script></li>
<li><p>채널이 여러개인 2차원 입력의 경우 2차원 Convolution을 채널 개수만큼 적용한다. 텐서를 직육면체 블록으로 이해하면 좀 더 이해하기 쉬움.</p>
</li>
<li><p>Convolution 연산은 커널이 모든 입력데이터에 공통으로 적용되기 때문에 역전파를 계산할 때도 convolution 연산이 나오게 된다.</p>
</li>
</ul>
</div><a class="article-more button is-small is-size-7" href="/CNN-first-step/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-06T09:14:05.000Z" title="2021. 8. 6. 오후 6:14:05">2021-08-06</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:14:34.000Z" title="2021. 8. 22. 오후 6:14:34">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">4분안에 읽기 (약 582 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/bayesian-statistics/">베이즈 통계학 맛보기</a></h1><div class="content"><h2 id="조건부확률"><a href="#조건부확률" class="headerlink" title="조건부확률"></a>조건부확률</h2><ul>
<li><p><img src="/image/image-20210806103516338.png" alt></p>
</li>
<li><p>조건부확률 P(A | B)는 사건 B가 일어난 상황에서 사건 A가 발생할 확률을 의미한다.</p>
</li>
<li><p>베이즈 정리는 조건부확률을 이용하여 정보를 갱신하는 방법을 알려준다.</p>
</li>
<li><p><img src="/image/image-20210806103603477.png" alt></p>
</li>
<li><p><img src="/image/image-20210806103958443.png" alt></p>
<ul>
<li><p>theta: 모수 (데이터가 관찰될 확률)</p>
</li>
<li><p>사후확률: 데이터를 관찰했을 때 이 가설이 성립할 확률 (데이터 관측 이후 측정한 확률이라 사후확률임)</p>
</li>
<li><p>사전확률: 모델링 이전에 사전에 주어진 확률로 이해할 것. (데이터 분석 전 타겟에 대한 모수나 가설 등 미리 설정한 값.)</p>
</li>
<li><p>가능도: 현재 주어진 파라미터(모수)가정에서 이 데이터가 관찰될 확률</p>
</li>
<li><p>Evidence: 데이터 전체의 분포</p>
</li>
<li><p>ex) COVID-99의 발병률이 10%로 알려져있다. COVID-99에 실제로 걸렸을 때 검진될 확률은 99%, 실제 걸리지 않았을 때 오검진될 확률이 1%라고 할 때, 어떤 사람이 질병에 걸렸다는 결과가 나왔을 때 정말로 COVID-99에 감염되었을 확률은?</p>
<ul>
<li><p>사전확률, 민감도(Recall), 오탐율(False alarm)을 가지고 정밀도(Precision)을 구하는 문제임.</p>
</li>
<li><p>사전확률 P(theta) = 10% = 0.1</p>
</li>
<li>가능도 P(D given theta) = 99% = 0.99, 오탐율 = 1% = 0.01</li>
<li>evidence P(D) = sum(P(D given theta) <em> P(theta)) = 0.99 </em> 0.1 + 0.01 * 0.9 = 0.108</li>
<li>So, P(theta given D) = 0.1 * (0.99 / 0.108) ~= 0.916</li>
</ul>
</li>
<li><p>COVID-99 판정을 받은 사람이 두 번째 검진을 받았을 때도 양성이 나왔을 때, 진짜 COVID-99에 걸렸을 확률은?</p>
<ul>
<li>베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있다.</li>
</ul>
</li>
</ul>
</li>
<li><p>조건부 확률은 유용한 통계적 해석을 제공하지만, 인과관계(Causality)를 추론할 때 함부로 사용해서는 안된다.</p>
<ul>
<li>인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다.</li>
<li>인과관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다. 제거하지 않았을 때는 가짜연관성(spurious correlation)이 나온다.</li>
</ul>
</li>
</ul>
</div><a class="article-more button is-small is-size-7" href="/bayesian-statistics/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-05T09:13:36.000Z" title="2021. 8. 5. 오후 6:13:36">2021-08-05</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:13:58.000Z" title="2021. 8. 22. 오후 6:13:58">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">7분안에 읽기 (약 979 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/statistics/">통계학 맛보기</a></h1><div class="content"><h2 id="모수란"><a href="#모수란" class="headerlink" title="모수란?"></a>모수란?</h2><ul>
<li>통계적 모델링은 적절한 가정 위에서 확률분포를 추정(inference)하는 것이 목표이며, 기계학습과 통계학이 공통적으로 추구하는 목표이다.</li>
<li>유한한 개수의 데이터만 관찰해서 모집단의 분포를 정확하게 알아낸다는 것은 불가능하다. 때문에 근사적으로 확률분포를 추정할 수밖에 없다.</li>
<li>데이터가 특정 확률분포를 따른다고 선험적(a priori)으로 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적 방법론이라고 한다.</li>
<li>특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면, 그것을 비모수(nonparametric) 방법론이라 부른다.<ul>
<li>기계학습의 많은 방법론은 비모수 방법론에 속한다.</li>
<li>모수가 무한히 많거나 모수가 데이터에 따라 바뀌는 것이지, 비모수방법론에는 모수가 없다는 것이 아니다.</li>
</ul>
</li>
</ul>
<h2 id="확률분포-가정하기-예제"><a href="#확률분포-가정하기-예제" class="headerlink" title="확률분포 가정하기(예제)"></a>확률분포 가정하기(예제)</h2><ul>
<li>우선 히스토그램을 통해 모양을 관찰한다.<ol>
<li>데이터가 2개의 값(0 또는 1)만 가지는 경우 -&gt; 베르누이분포</li>
<li>데이터가 n개의 이산적인 값을 가지는 경우 -&gt; 카테고리분포</li>
<li>데이터가 [0, 1]사이에서 값을 가지는 경우 -&gt; 베타분포</li>
<li>데이터가 0 이상의 값을 가지는 경우 -&gt; 감마분포, 로그정규분포 등</li>
<li>데이터가 R 전체에서 값을 가지는 경우 -&gt; 정규분포, 라플라스분포 등</li>
</ol>
</li>
</ul>
<h2 id="데이터로-모수-추정"><a href="#데이터로-모수-추정" class="headerlink" title="데이터로 모수 추정"></a>데이터로 모수 추정</h2><ul>
<li>데이터의 확률분포를 가정했다면, 모수를 추정해볼 수 있다.</li>
<li>정규분포의 모수는 평균과 분산으로, 이를 추정하는 통계량은 아래 식과 같다.<ul>
<li><img src="/image/image-20210805135419323.png" alt></li>
<li>표본분산을 구할 때 N이 아니라 N - 1 로 나누는 이유는 불편(unbiased) 추정량을 구하기 위해서다.</li>
</ul>
</li>
<li>통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 N이 커질수록 (데이터가 많아질수록) 정규분포를 따르게 된다.<ul>
<li>이것을 중심극한정리라고 부르며, 모집단의 분포가 정규분포를 따르지 않아도 성립한다.</li>
</ul>
</li>
</ul>
<h2 id="최대가능도-추정법"><a href="#최대가능도-추정법" class="headerlink" title="최대가능도 추정법"></a>최대가능도 추정법</h2><ul>
<li>표본평균이나 표본분산은 중요한 통계량이지만 확률분포마다 사용하는 모수가 다르므로 적절한 통계량이 달라지게 된다.</li>
<li>이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(maximum likelihood estimation, MLE)이다.</li>
<li>데이터집합 X가 독립적으로 추출되었을 경우엔 로그가능도를 최적화한다.<ul>
<li>로그가능도를 사용하는 이유<ol>
<li>로그가능도를 최적화하는 모수는 가능도를 최적화하는 MLE가 된다.</li>
<li>데이터의 숫자가 수억 단위가 된다면 컴퓨터의 정확도로는 가능도를 계산하는 것이 불가능해진다.</li>
<li>데이터가 독립일 경우, 로그를 사용하면 가능도의 곱셈을 로그가능도의 덧셈으로 바꿀 수 있기 때문에 컴퓨터로 연산이 가능해진다.</li>
<li>경사하강법으로 가능도를 최적화할 때 미분 연산을 사용하게 되는데, 로그가능도를 사용하면 연산량을 O(N^2)에서 선형시간으로 줄일 수 있다.</li>
<li>대개 손실함수의 경우 경사하강법을 사용하므로 음의 로그가능도(negative log-likelihood)를 최적화하게 된다.</li>
</ol>
</li>
</ul>
</li>
</ul>
</div><a class="article-more button is-small is-size-7" href="/statistics/#more">자세히 보기</a></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2021-08-05T09:13:07.000Z" title="2021. 8. 5. 오후 6:13:07">2021-08-05</time>&nbsp;게시 됨</span><span class="level-item"><time datetime="2021-08-22T09:13:24.000Z" title="2021. 8. 22. 오후 6:13:24">2021-08-22</time>&nbsp;업데이트 됨</span><span class="level-item"><a class="link-muted" href="/categories/Study/">Study</a><span> / </span><a class="link-muted" href="/categories/Study/Math-for-AI/">Math for AI</a></span><span class="level-item">5분안에 읽기 (약 726 단어)</span></div></div><h1 class="title is-3 is-size-4-mobile"><a class="link-muted" href="/probability-theory/">확률론 맛보기</a></h1><div class="content"><h2 id="딥러닝에서-확률론이-필요한-이유"><a href="#딥러닝에서-확률론이-필요한-이유" class="headerlink" title="딥러닝에서 확률론이 필요한 이유"></a>딥러닝에서 확률론이 필요한 이유</h2><ul>
<li>딥러닝은 확률론 기반의 기계학습 이론에 바탕을 둔다.</li>
<li>손실함수 등의 작동원리가 데이터 공간을 통계적으로 해석해서 유도하기 때문.</li>
<li>회귀분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습</li>
<li>분류문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습</li>
<li>분산 및 불확실성을 최소화하기 위해서는 측정 방법을 알아야한다.</li>
</ul>
<h2 id="확률분포"><a href="#확률분포" class="headerlink" title="확률분포"></a>확률분포</h2><ul>
<li>확률분포란 데이터 공간에서 데이터를 추출하는 분포이다.</li>
<li>데이터는 확률변수로 (x, y) ~ D 라고 표기한다.</li>
<li>결합분포 P(x, y)는 D를 모델링한다.<ul>
<li>D는 이론적으로 존재하는 확률분포이기 때문에 사전에 알 수 없다.</li>
</ul>
</li>
</ul>
<h2 id="확률변수"><a href="#확률변수" class="headerlink" title="확률변수"></a>확률변수</h2><h3 id="이산확률변수"><a href="#이산확률변수" class="headerlink" title="이산확률변수"></a>이산확률변수</h3><ul>
<li>이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다.</li>
</ul>
<h3 id="연속확률변수"><a href="#연속확률변수" class="headerlink" title="연속확률변수"></a>연속확률변수</h3><ul>
<li>연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도 위에서의 적분을 통해 모델링한다.</li>
<li>밀도함수는 누적확률분포의 변화율을 모델링한 것으로 확률로 해석해서는 안된다.</li>
</ul>
<h2 id="조건부확률과-기계학습"><a href="#조건부확률과-기계학습" class="headerlink" title="조건부확률과 기계학습"></a>조건부확률과 기계학습</h2></div><a class="article-more button is-small is-size-7" href="/probability-theory/#more">자세히 보기</a></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/categories/Study/">이전</a></div><div class="pagination-next"><a href="/categories/Study/page/3/">다음</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/categories/Study/">1</a></li><li><a class="pagination-link is-current" href="/categories/Study/page/2/">2</a></li><li><a class="pagination-link" href="/categories/Study/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/image/avatar.jpg" alt="YOHAI"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">YOHAI</p><p class="is-size-6 is-block">Your Own Humanistic AI</p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">포스트</p><a href="/archives"><p class="title">66</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">카테고리</p><a href="/categories"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">태그</p><a href="/tags"><p class="title">26</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/l-yohai" target="_blank" rel="external nofollow noopener noreferrer">팔로우</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Email" href="mailto:yohan9612@yonsei.ac.kr"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Github" href="https://github.com/l-yohai"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="external nofollow noopener noreferrer" title="Linkedin" href="https://www.linkedin.com/in/l-yohai/"><i class="fab fa-linkedin"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">카테고리</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/42-Life/"><span class="level-start"><span class="level-item">42 Life</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Boostcamp-AI-Tech/"><span class="level-start"><span class="level-item">Boostcamp AI Tech</span></span><span class="level-end"><span class="level-item tag">4</span></span></a><ul><li><a class="level is-mobile" href="/categories/Boostcamp-AI-Tech/Master-Class/"><span class="level-start"><span class="level-item">Master Class</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Competition/"><span class="level-start"><span class="level-item">Competition</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Diary/"><span class="level-start"><span class="level-item">Diary</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/Paper-Review/"><span class="level-start"><span class="level-item">Paper Review</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">10</span></span></a><ul><li><a class="level is-mobile" href="/categories/Programming/Problem-Solving/"><span class="level-start"><span class="level-item">Problem Solving</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Project/"><span class="level-start"><span class="level-item">Project</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Programming/Tips/"><span class="level-start"><span class="level-item">Tips</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Study/"><span class="level-start"><span class="level-item">Study</span></span><span class="level-end"><span class="level-item tag">30</span></span></a><ul><li><a class="level is-mobile" href="/categories/Study/AI/"><span class="level-start"><span class="level-item">AI</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Algorithm/"><span class="level-start"><span class="level-item">Algorithm</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/C/"><span class="level-start"><span class="level-item">C</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Math-for-AI/"><span class="level-start"><span class="level-item">Math for AI</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/NLP/"><span class="level-start"><span class="level-item">NLP</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul><li><a class="level is-mobile" href="/categories/Study/NLP/MRC/"><span class="level-start"><span class="level-item">MRC</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Study/Network/"><span class="level-start"><span class="level-item">Network</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Python/"><span class="level-start"><span class="level-item">Python</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/Study/Visualization/"><span class="level-start"><span class="level-item">Visualization</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/Yonsei/"><span class="level-start"><span class="level-item">Yonsei</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/fur-Musik/"><span class="level-start"><span class="level-item">für Musik</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">태그</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/42-Seoul/"><span class="tag">42 Seoul</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/42Seoul/"><span class="tag">42Seoul</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/42%EC%84%9C%EC%9A%B8/"><span class="tag">42서울</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/42%EC%84%9C%EC%9A%B8-%EB%B3%B8%EA%B3%BC%EC%A0%95/"><span class="tag">42서울 본과정</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/42%EC%84%9C%EC%9A%B8-%ED%9B%84%EA%B8%B0/"><span class="tag">42서울 후기</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/AI-%EB%8C%80%ED%9A%8C-%ED%98%91%EC%97%85-%ED%94%8C%EB%9E%98%EB%8B%9D%EA%B0%80%EC%9D%B4%EB%93%9C/"><span class="tag">AI, 대회, 협업, 플래닝가이드</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Franz-Liszt/"><span class="tag">Franz Liszt</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Kakao-I-Open-Builder/"><span class="tag">Kakao I Open Builder</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/La-Piscine/"><span class="tag">La Piscine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Papago-NMT-API/"><span class="tag">Papago NMT API</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Teachable-Machine/"><span class="tag">Teachable Machine</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Tech-Seminar/"><span class="tag">Tech Seminar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/VoyagerX/"><span class="tag">VoyagerX</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/WaveNet/"><span class="tag">WaveNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Word2Vec/"><span class="tag">Word2Vec</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EA%B3%B5%ED%86%B5%EA%B3%BC%EC%A0%95/"><span class="tag">공통과정</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%82%A8%EC%84%B8%EB%8F%99/"><span class="tag">남세동</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%94%A5%EB%9F%AC%EB%8B%9D/"><span class="tag">딥러닝</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EB%B3%B4%EC%9D%B4%EC%A0%80%EC%97%91%EC%8A%A4/"><span class="tag">보이저엑스</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%8B%9C%EA%B0%81%ED%99%94/"><span class="tag">시각화</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%97%B4%EC%A0%95/"><span class="tag">열정</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%B4%EB%84%88%EC%84%9C%ED%81%B4/"><span class="tag">이너서클</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%9D%B4%EB%85%B8%EB%B2%A0%EC%9D%B4%EC%85%98%EC%95%84%EC%B9%B4%EB%8D%B0%EB%AF%B8/"><span class="tag">이노베이션아카데미</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%B1%97%EB%B4%87/"><span class="tag">챗봇</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%B4%88%EC%A0%88%EA%B8%B0%EA%B5%90-%EC%97%B0%EC%8A%B5%EA%B3%A1/"><span class="tag">초절기교 연습곡</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%EC%B9%B4%EC%B9%B4%EC%98%A4%ED%86%A1/"><span class="tag">카카오톡</span><span class="tag">2</span></a></div></div></div></div></div><div class="column-right-shadow is-hidden-widescreen"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3"><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">최근 글</h3><article class="media"><figure class="media-left"><a class="image" href="/AI-Bookathon/"><img src="/image/3rd_ai_bookathon.jpg" alt="AI Bookathon 대상 후기"></a></figure><div class="media-content"><p class="date"><time datetime="2021-11-17T21:54:58.000Z">2021-11-18</time></p><p class="title"><a href="/AI-Bookathon/">AI Bookathon 대상 후기</a></p><p class="categories"><a href="/categories/Competition/">Competition</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2021-10-16T05:29:55.000Z">2021-10-16</time></p><p class="title"><a href="/PStage-MRC-7-Linking-MRC-and-Retrieval/">PStage MRC 7강 - Linking MRC and Retrieval</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/NLP/">NLP</a> / <a href="/categories/Study/NLP/MRC/">MRC</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2021-10-14T21:24:41.000Z">2021-10-15</time></p><p class="title"><a href="/PStage-MRC-6-Scaling-up-with-FAISS/">PStage MRC 6강 - Scaling up with FAISS</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/NLP/">NLP</a> / <a href="/categories/Study/NLP/MRC/">MRC</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2021-10-13T19:59:34.000Z">2021-10-14</time></p><p class="title"><a href="/PStage-MRC-4-5-Passage-Retrieval-Sparse-Embedding-Dense-Embedding/">PStage MRC 4-5강 - Passage Retrieval - Sparse Embedding, Dense Embedding</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/NLP/">NLP</a> / <a href="/categories/Study/NLP/MRC/">MRC</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2021-10-13T02:50:23.000Z">2021-10-13</time></p><p class="title"><a href="/PStage-MRC-3-Generation-based-MRC/">PStage MRC 3강 - Generation-based MRC</a></p><p class="categories"><a href="/categories/Study/">Study</a> / <a href="/categories/Study/NLP/">NLP</a> / <a href="/categories/Study/NLP/MRC/">MRC</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">아카이브</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2021/11/"><span class="level-start"><span class="level-item">11월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/10/"><span class="level-start"><span class="level-item">10월 2021</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/09/"><span class="level-start"><span class="level-item">9월 2021</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/08/"><span class="level-start"><span class="level-item">8월 2021</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/04/"><span class="level-start"><span class="level-item">4월 2021</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/12/"><span class="level-start"><span class="level-item">12월 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/09/"><span class="level-start"><span class="level-item">9월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/08/"><span class="level-start"><span class="level-item">8월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/06/"><span class="level-start"><span class="level-item">6월 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/05/"><span class="level-start"><span class="level-item">5월 2020</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/04/"><span class="level-start"><span class="level-item">4월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/03/"><span class="level-start"><span class="level-item">3월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/02/"><span class="level-start"><span class="level-item">2월 2020</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/image/logo.png" alt="YOHAI" height="28"></a><p class="is-size-7"><span>&copy; 2023 Yohan Lee</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="external nofollow noopener noreferrer">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="external nofollow noopener noreferrer" title="Download on GitHub" href="https://github.com/l-yohai"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("ko");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="맨 위로" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "이 웹 사이트는 귀하의 경험을 향상시키기 위해 Cookie를 사용합니다.",
          dismiss: "무시",
          allow: "허용",
          deny: "거부",
          link: "더 알아보기",
          policy: "Cookie 정책",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="입력 하세요..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"입력 하세요...","untitled":"(제목 없음)","posts":"포스트","pages":"페이지","categories":"카테고리","tags":"태그"});
        });</script></body></html>